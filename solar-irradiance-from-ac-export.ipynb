{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b4f091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Solar Irradiance From AC Export\n",
    "\n",
    "A Jupyter Notebook that does it's best to model and construct a historical solar irradiance time series from solar panel park's historical AC export data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb98ad",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5f3b9",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "# Standard Library Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd074bad",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 1.2 Configuration\n",
    "\n",
    "This project uses a two-step configuration process:\n",
    "\n",
    "1.  **Path Definition (`.env`):** This file defines the project's physical location (`PROJECT_ROOT`) and the name of the configuration file. This separation ensures the notebook is portable across different machines and environments.\n",
    "2.  **Parameter Definition (`config.yml`):** This file contains the physical and electrical parameters of your solar park(s), including sensitive information like GPS coordinates and detailed system specifications.\n",
    "\n",
    "**To get started:**\n",
    "\n",
    "1.  **Configure Paths:** Copy the template file `.env.example` to a new file named `.env`. Open `.env` and set the absolute path for the `PROJECT_ROOT` variable.\n",
    "2.  **Configure Parks:** Copy the example configuration file `config.example.yml` to `config.yml`. Open `config.yml` and replace the placeholder values with the details of your solar installation.\n",
    "\n",
    "The cell below loads the environment variables, resolves the final configuration path, and sets up the plotting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define paths using environment variables\n",
    "PROJECT_ROOT_STR = os.getenv(\"PROJECT_ROOT\")\n",
    "CONFIG_FILENAME = os.getenv(\"CONFIG_FILENAME\", \"config.yml\")  # Fallback to config.yml\n",
    "PRODUCTION_AND_PRICE_FILE_PATH = os.getenv(\n",
    "    \"PRODUCTION_AND_PRICE_FILE_PATH\",\n",
    "    \"/home/user/solar-irradiance-from-ac-export/production.csv\",\n",
    ")\n",
    "WEATHER_FILE_PATH = os.getenv(\n",
    "    \"WEATHER_FILE_PATH\", \"/home/user/solar-irradiance-from-ac-export/weather.csv\"\n",
    ")\n",
    "\n",
    "if not PROJECT_ROOT_STR:\n",
    "    # If PROJECT_ROOT is not set in .env, assume the current working directory\n",
    "    PROJECT_ROOT_STR = os.getcwd()\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è WARNING: PROJECT_ROOT not set in .env. Using current directory: {PROJECT_ROOT_STR}\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = Path(PROJECT_ROOT_STR)\n",
    "CONFIG_PATH = PROJECT_ROOT / CONFIG_FILENAME\n",
    "\n",
    "print(f\"Project Root defined as: {PROJECT_ROOT}\")\n",
    "print(f\"Configuration file path: {CONFIG_PATH}\")\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Extract park configurations\n",
    "    PARK_CONFIGS = config.get(\"parks\", {})\n",
    "\n",
    "    if not PARK_CONFIGS:\n",
    "        raise ValueError(\n",
    "            \"No parks defined under the 'parks' key in the configuration file.\"\n",
    "        )\n",
    "\n",
    "    # Create a list of park names for easy iteration later\n",
    "    PARK_NAMES = list(PARK_CONFIGS.keys())\n",
    "\n",
    "    # --- Load and Validate Target Park for Analysis ---\n",
    "    TARGET_PARK_NAME = os.getenv(\"TARGET_PARK_NAME\")\n",
    "\n",
    "    if not TARGET_PARK_NAME:\n",
    "        raise ValueError(\"TARGET_PARK_NAME is not set in the .env file. Please specify which park to analyze.\")\n",
    "\n",
    "    if TARGET_PARK_NAME not in PARK_NAMES:\n",
    "        raise ValueError(\n",
    "            f\"The target park '{TARGET_PARK_NAME}' defined in .env is not found in 'config.yml'.\\n\"\n",
    "            f\"Available parks in config: {PARK_NAMES}\"\n",
    "        )\n",
    "\n",
    "    print(f\"üéØ Analysis will be performed for target park: '{TARGET_PARK_NAME}'\")\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Configuration loaded successfully from '{CONFIG_PATH}' for {len(PARK_NAMES)} park(s): {', '.join(PARK_NAMES)}.\"\n",
    "    )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå CONFIGURATION ERROR: The '{CONFIG_PATH}' file was not found.\")\n",
    "    print(\n",
    "        \"Please check your .env file's PROJECT_ROOT setting, and ensure 'config.yml' exists at that location.\"\n",
    "    )\n",
    "    print(\n",
    "        \"If 'config.yml' is missing, copy 'config.example.yml' to 'config.yml' and fill in your park's details.\"\n",
    "    )\n",
    "except (yaml.YAMLError, ValueError) as e:\n",
    "    print(\n",
    "        f\"‚ùå CONFIGURATION ERROR: Could not parse '{CONFIG_PATH}'. Please check its format. Details: {e}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Plotting and Display Configuration ---\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# Set display options for better viewing in Jupyter\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "print(\"Plotting and display options set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522e0e5",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6183609",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Helper Function ---\n",
    "\n",
    "\n",
    "def load_park_specific_data(\n",
    "    file_path: str,\n",
    "    timestamp_col: str,\n",
    "    park_name_col: str,\n",
    "    required_data_cols: list[str],\n",
    "    target_park_name: str,\n",
    "    data_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads, validates, and filters data for a single specified park from a long-format CSV.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Absolute path to the CSV file.\n",
    "        timestamp_col (str): Name of the timestamp column.\n",
    "        park_name_col (str): Name of the park identifier column.\n",
    "        required_data_cols (list): List of required data column names.\n",
    "        target_park_name (str): The specific park to extract data for.\n",
    "        data_name (str): A descriptive name for the data (e.g., \"Production\").\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing only the data for the target park,\n",
    "                          with the park_name column removed. Returns an empty\n",
    "                          DataFrame on failure.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading {data_name} Data for '{target_park_name}' ---\")\n",
    "    print(f\"Attempting to load from: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load the full CSV\n",
    "        df = pd.read_csv(\n",
    "            file_path, parse_dates=[timestamp_col], index_col=timestamp_col\n",
    "        )\n",
    "\n",
    "        # 2. Basic Column Check\n",
    "        all_required_cols = required_data_cols + [park_name_col]\n",
    "        if not all(col in df.columns for col in all_required_cols):\n",
    "            missing = [col for col in all_required_cols if col not in df.columns]\n",
    "            raise ValueError(f\"Missing required columns in {data_name} CSV: {missing}\")\n",
    "\n",
    "        # 3. Data Cleaning and Validation\n",
    "        df.index = pd.to_datetime(df.index, utc=True)\n",
    "        df = df.dropna(subset=[park_name_col])\n",
    "        df[park_name_col] = df[park_name_col].astype(str)\n",
    "\n",
    "        # 4. Check if Target Park Exists in Data\n",
    "        if target_park_name not in df[park_name_col].unique():\n",
    "            raise ValueError(\n",
    "                f\"Target park '{target_park_name}' not found in the {data_name} file.\"\n",
    "            )\n",
    "\n",
    "        # 5. Filter for Target Park and Finalize\n",
    "        df_park = df[df[park_name_col] == target_park_name].copy()\n",
    "\n",
    "        # Drop the now-redundant park name column\n",
    "        df_park = df_park.drop(columns=[park_name_col])\n",
    "\n",
    "        df_park = df_park.sort_index()\n",
    "        print(f\"‚úÖ {data_name} data for '{target_park_name}' loaded successfully.\")\n",
    "        print(f\"   Shape of final DataFrame: {df_park.shape}\")\n",
    "        print(f\"   Time range: {df_park.index.min()} to {df_park.index.max()}\")\n",
    "        print(\"Sample:\")\n",
    "        print(df_park.sample(n=5))\n",
    "        return df_park\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå DATA ERROR: The {data_name} file was not found at: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå AN UNEXPECTED ERROR OCCURRED during {data_name} data loading: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"‚úÖ Helper function load_park_specific_data defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084d559",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.1 Hourly Production And Spot Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a38d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Production and Price Data ---\n",
    "\n",
    "# Define required column names for production data\n",
    "COL_TIMESTAMP = \"timestamp_utc\"\n",
    "COL_PARK_NAME = \"park_name\"\n",
    "PRODUCTION_DATA_COLS = [\"ac_export_kwh\", \"spot_price_eur_mwh\"]\n",
    "\n",
    "# Load the data for the target park using the helper function\n",
    "df_production = load_park_specific_data(\n",
    "    file_path=PRODUCTION_AND_PRICE_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=PRODUCTION_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Production & Price\",\n",
    ")\n",
    "assert isinstance(df_production.index, pd.DatetimeIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45339e8",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.2 Load Hourly Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Crop Weather Data ---\n",
    "\n",
    "# Define required column names for weather data\n",
    "WEATHER_DATA_COLS = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\", \"ghi_w_m2\"]\n",
    "\n",
    "# Load the weather data for the target park using the helper function\n",
    "df_weather = load_park_specific_data(\n",
    "    file_path=WEATHER_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=WEATHER_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Weather\",\n",
    ")\n",
    "assert isinstance(df_weather.index, pd.DatetimeIndex)\n",
    "\n",
    "# Post-processing: Crop the weather data to the production time range\n",
    "if not df_production.empty and not df_weather.empty:\n",
    "    start_time = df_production.index.min()\n",
    "    end_time = df_production.index.max()\n",
    "\n",
    "    original_rows = len(df_weather)\n",
    "    df_weather = df_weather.loc[start_time:end_time].copy()\n",
    "\n",
    "    print(f\"\\nWeather data cropped to production time range.\")\n",
    "    print(f\"   Original rows: {original_rows}, Cropped rows: {len(df_weather)}\")\n",
    "    print(f\"   New time range: {df_weather.index.min()} to {df_weather.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdd204",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 3. Data Upsampling and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f165ee8",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interpolation Helper Function ---\n",
    "\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def interpolate_by_gap_size(\n",
    "    data: pd.Series | pd.DataFrame,\n",
    "    max_gap_size: int = 1,\n",
    "    method: Literal[\n",
    "        \"linear\",\n",
    "        \"time\",\n",
    "        \"index\",\n",
    "        \"values\",\n",
    "        \"pad\",\n",
    "        \"nearest\",\n",
    "        \"zero\",\n",
    "        \"slinear\",\n",
    "        \"quadratic\",\n",
    "        \"cubic\",\n",
    "        \"barycentric\",\n",
    "        \"polynomial\",\n",
    "        \"spline\",\n",
    "        \"krogh\",\n",
    "        \"piecewise_polynomial\",\n",
    "        \"pchip\",\n",
    "        \"akima\",\n",
    "        \"cubicspline\",\n",
    "        \"from_derivatives\",\n",
    "    ] = \"linear\",\n",
    "    **kwargs\n",
    ") -> pd.Series | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpolates NaN values in a Series or DataFrame, but only for gaps\n",
    "    that are less than or equal to a specified maximum size.\n",
    "\n",
    "    For a DataFrame, the operation is applied column-wise.\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series | pd.DataFrame): Input data with potential NaNs.\n",
    "        max_gap_size (int): Max consecutive NaNs to interpolate. Gaps larger\n",
    "                            than this are ignored. Defaults to 1.\n",
    "        method (str): Interpolation technique (see pandas.Series.interpolate).\n",
    "                      Defaults to \"linear\".\n",
    "        **kwargs: Additional keyword arguments for the interpolate() method.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series | pd.DataFrame: New object with specified gaps filled.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, (pd.Series, pd.DataFrame)):\n",
    "        raise TypeError(\"Input `data` must be a pandas Series or DataFrame.\")\n",
    "    if not isinstance(max_gap_size, int) or max_gap_size <= 0:\n",
    "        raise ValueError(\"`max_gap_size` must be a positive integer.\")\n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        if data.empty or data.isna().sum() == 0:\n",
    "            return data.copy()\n",
    "\n",
    "        # Identify gaps by creating groups based on non-NaN values\n",
    "        grouper = data.notna().cumsum()\n",
    "        # Calculate the size of each NaN gap\n",
    "        gap_sizes = data.isna().groupby(grouper).transform(\"sum\")\n",
    "        # Create a boolean mask for NaNs that are part of small-enough gaps\n",
    "        mask_to_fill = data.isna() & (gap_sizes <= max_gap_size)\n",
    "\n",
    "        # Interpolate the entire series to get the fill values\n",
    "        fully_interpolated = data.interpolate(method=method, **kwargs) # type: ignore\n",
    "\n",
    "        # Apply the fill values only where the mask is True\n",
    "        return data.where(~mask_to_fill, fully_interpolated)\n",
    "\n",
    "    # If it's a DataFrame, apply this function to each column\n",
    "    return data.apply(\n",
    "        lambda col: interpolate_by_gap_size(\n",
    "            col, max_gap_size=max_gap_size, method=method, **kwargs\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper function interpolate_by_gap_size defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a52d2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.1 Upsample Production Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b71531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Production and Price Data to 30-Minute Frequency ---\n",
    "\n",
    "print(f\"--- Upsampling data for park: '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# 1. Determine the actual production period to avoid creating a massive index\n",
    "first_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.min()\n",
    "last_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.max()\n",
    "\n",
    "if pd.isna(first_prod_time) or pd.isna(last_prod_time):\n",
    "    print(\"‚ö†Ô∏è No valid production data (> 0 kWh) found. Cannot proceed.\")\n",
    "    df_production_30min = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"   - Production data range: {first_prod_time} to {last_prod_time}\")\n",
    "\n",
    "    # Crop the hourly data to the actual production period\n",
    "    df_prod_cropped = df_production.loc[first_prod_time:last_prod_time]\n",
    "\n",
    "    # Create a full 30-minute time range for this period\n",
    "    full_30min_range = pd.date_range(\n",
    "        start=first_prod_time, end=last_prod_time, freq=\"30min\", tz=\"UTC\"\n",
    "    )\n",
    "\n",
    "    # 2. Upsample spot price using forward-fill\n",
    "    # The price at HH:30 is the same as the price at HH:00\n",
    "    price_30min = (\n",
    "        df_prod_cropped[\"spot_price_eur_mwh\"]\n",
    "        .reindex(full_30min_range)\n",
    "        .ffill()\n",
    "        .to_frame()\n",
    "    )\n",
    "\n",
    "    # 3. Upsample production data from hourly kWh to 30-min average Power (W)\n",
    "    # Convert kWh (energy) to W (power) for the hourly interval\n",
    "    power_w_hourly = df_prod_cropped[\"ac_export_kwh\"] * 1000\n",
    "\n",
    "    # Shift the index 30 mins forward. The power calculated from the interval\n",
    "    # [HH:00, HH+1:00] is best represented at the midpoint, HH:30.\n",
    "    power_w_hourly.index = power_w_hourly.index + pd.Timedelta(minutes=30)\n",
    "\n",
    "    # Reindex to the full 30-min range. This introduces NaNs at HH:00.\n",
    "    # Use interpolate_by_gap_size(max_gap_size=1) to linearly fill only the\n",
    "    # points at HH:00, which are surrounded by two HH:30 points.\n",
    "    power_w_30min = interpolate_by_gap_size(\n",
    "        power_w_hourly.reindex(full_30min_range), max_gap_size=1, method=\"linear\"\n",
    "    ).to_frame(\"ac_export_w\") # type: ignore\n",
    "\n",
    "    # 4. Combine the upsampled series into a single DataFrame\n",
    "    df_production_30min = price_30min.join(power_w_30min)\n",
    "\n",
    "    print(\"\\n‚úÖ Production and price data upsampled to 30-minute frequency.\")\n",
    "    print(f\"   - Shape of new DataFrame: {df_production_30min.shape}\")\n",
    "    print(\n",
    "        f\"   - New time range: {df_production_30min.index.min()} to {df_production_30min.index.max()}\"\n",
    "    )\n",
    "    print(\"   - Sample data:\")\n",
    "    print(df_production_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e916ff",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.2 Upsample Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Weather Data to 30-Minute Frequency ---\n",
    "\n",
    "print(\"--- Upsampling weather data ---\")\n",
    "\n",
    "# 1. Use the index from the upsampled production data for perfect alignment\n",
    "target_30min_range = df_production_30min.index\n",
    "print(\n",
    "    f\"   - Aligning to target time range: {target_30min_range.min()} to {target_30min_range.max()}\"\n",
    ")\n",
    "\n",
    "# 2. Handle standard weather variables with linear interpolation\n",
    "standard_weather_cols = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\"]\n",
    "df_weather_standard_30min = interpolate_by_gap_size(\n",
    "    df_weather[standard_weather_cols].reindex(target_30min_range),\n",
    "    max_gap_size=1,\n",
    "    method=\"linear\",\n",
    ")\n",
    "\n",
    "# 3. Handle GHI with a time shift before interpolation\n",
    "# The hourly GHI value at HH:00 represents the average over the interval [HH-1:00, HH:00].\n",
    "# We shift the timestamp back 30 mins to place this value at the interval's midpoint (HH-1:30).\n",
    "ghi_hourly = df_weather[[\"ghi_w_m2\"]].copy()\n",
    "assert isinstance(ghi_hourly.index, pd.DatetimeIndex)\n",
    "ghi_hourly.index = ghi_hourly.index - pd.Timedelta(minutes=30)\n",
    "\n",
    "# Reindex and interpolate. This will correctly fill the values at the top of the hour (HH:00).\n",
    "df_ghi_30min = interpolate_by_gap_size(\n",
    "    ghi_hourly.reindex(target_30min_range), max_gap_size=1, method=\"linear\"\n",
    ")\n",
    "\n",
    "# 4. Combine all upsampled weather series\n",
    "df_weather_30min = df_weather_standard_30min.join(df_ghi_30min)\n",
    "\n",
    "print(\"\\n‚úÖ Weather data upsampled to 30-minute frequency.\")\n",
    "print(f\"   - Shape of new DataFrame: {df_weather_30min.shape}\")\n",
    "print(\"   - Sample data:\")\n",
    "print(df_weather_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856009b2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.3 Model PVLIB Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fe609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Solar Geometry and Clear-Sky Irradiance with PVLIB ---\n",
    "\n",
    "import pvlib\n",
    "\n",
    "print(f\"--- Modeling PVLIB features for '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# 1. Extract location parameters from the main config\n",
    "park_params = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "location = pvlib.location.Location(\n",
    "    latitude=park_params[\"location\"][\"latitude\"],\n",
    "    longitude=park_params[\"location\"][\"longitude\"],\n",
    "    tz=\"UTC\",\n",
    ")\n",
    "\n",
    "# 2. Prepare inputs for pvlib calculations\n",
    "times = df_weather_30min.index\n",
    "pressure_pa = df_weather_30min[\"pressure_hpa\"] * 100  # Convert hPa to Pa\n",
    "temperature_c = df_weather_30min[\"temp_air_c\"]\n",
    "\n",
    "# 3. Perform pvlib calculations\n",
    "print(\"   - Calculating solar position...\")\n",
    "solar_position = location.get_solarposition(\n",
    "    times, pressure=pressure_pa, temperature=temperature_c  # type: ignore\n",
    ")\n",
    "assert isinstance(solar_position, pd.DataFrame)\n",
    "\n",
    "print(\"   - Calculating airmass...\")\n",
    "airmass_relative = pvlib.atmosphere.get_relative_airmass(\n",
    "    zenith=solar_position[\"apparent_zenith\"]\n",
    ")\n",
    "airmass_absolute = pvlib.atmosphere.get_absolute_airmass(\n",
    "    airmass_relative=airmass_relative, pressure=pressure_pa  # type: ignore\n",
    ")\n",
    "\n",
    "print(\"   - Calculating extraterrestrial radiation...\")\n",
    "dni_extra = pvlib.irradiance.get_extra_radiation(times)\n",
    "\n",
    "print(\"   - Calculating clear-sky irradiance (Ineichen model)...\")\n",
    "clearsky_irrad = location.get_clearsky(\n",
    "    times,\n",
    "    model=\"ineichen\",\n",
    "    solar_position=solar_position,\n",
    "    airmass_absolute=airmass_absolute,\n",
    "    dni_extra=dni_extra,\n",
    ")\n",
    "assert isinstance(clearsky_irrad, pd.DataFrame)\n",
    "clearsky_irrad = clearsky_irrad.rename(\n",
    "    columns={\n",
    "        \"ghi\": \"ghi_clearsky_w_m2\",\n",
    "        \"dni\": \"dni_clearsky_w_m2\",\n",
    "        \"dhi\": \"dhi_clearsky_w_m2\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"   - Adding 'is_day' flag...\")\n",
    "is_day = solar_position[\"apparent_zenith\"] < 90.0\n",
    "\n",
    "# 4. Assemble the final pvlib DataFrame\n",
    "df_pvlib_30min = pd.concat(\n",
    "    [\n",
    "        solar_position[[\"apparent_zenith\", \"zenith\", \"azimuth\"]],\n",
    "        pd.Series(airmass_relative, name=\"airmass_relative\"),\n",
    "        pd.Series(airmass_absolute, name=\"airmass_absolute\"),\n",
    "        pd.Series(dni_extra, name=\"dni_extra_w_m2\"),\n",
    "        clearsky_irrad,\n",
    "        pd.Series(is_day, name=\"is_day\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ PVLIB features modeled successfully.\")\n",
    "print(f\"   - Shape of new DataFrame: {df_pvlib_30min.shape}\")\n",
    "print(\"   - Sample data:\")\n",
    "print(df_pvlib_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4a5aa",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.4 Merge DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge All 30-Minute DataFrames ---\n",
    "\n",
    "print(\"--- Merging all 30-minute data sources ---\")\n",
    "\n",
    "# Join the three dataframes on their common DatetimeIndex\n",
    "# This preserves all rows and fills with NaNs where data is missing in one source\n",
    "df_30min = df_production_30min.join([df_weather_30min, df_pvlib_30min])\n",
    "\n",
    "print(\"\\n‚úÖ All data sources successfully merged into a single DataFrame.\")\n",
    "print(f\"   - Final shape: {df_30min.shape}\")\n",
    "print(f\"   - Final time range: {df_30min.index.min()} to {df_30min.index.max()}\")\n",
    "print(\"   - Sample of merged data:\")\n",
    "print(df_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d3e84",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 4. Anomaly Flagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d961c",
   "metadata": {},
   "source": [
    "### 4.1 Flag Clipping and Curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Flag Clipping and Curtailment Events ---\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "print(f\"--- Flagging anomalies for '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Fetch park-specific parameters from the config\n",
    "park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "export_limit_w = park_config[\"export_limit_kw\"] * 1000\n",
    "pdc0_w = park_config[\"system\"][\"pdc0\"]\n",
    "\n",
    "# Define thresholds for flagging logic\n",
    "# Flag as clipped if power is within this percentage of the export limit\n",
    "CLIPPING_TOLERANCE_PCT = 0.01\n",
    "# Flag as curtailed if power is below this percentage of DC capacity during negative prices\n",
    "CURTAILMENT_POWER_THRESHOLD_PCT = 0.01\n",
    "\n",
    "clipping_threshold_w = export_limit_w * (1 - CLIPPING_TOLERANCE_PCT)\n",
    "curtailment_threshold_w = pdc0_w * CURTAILMENT_POWER_THRESHOLD_PCT\n",
    "\n",
    "print(f\"   - Clipping threshold: >= {clipping_threshold_w:.2f} W\")\n",
    "print(\n",
    "    f\"   - Curtailment threshold: < {curtailment_threshold_w:.2f} W (during negative prices)\"\n",
    ")\n",
    "\n",
    "# Initialize flag columns to False\n",
    "df_30min[\"is_clipped\"] = False\n",
    "df_30min[\"is_curtailed\"] = False\n",
    "\n",
    "# --- Clipping Detection Logic ---\n",
    "print(\"\\n1. Applying Clipping Detection Logic...\")\n",
    "\n",
    "# Step 1: Directly flag HH:30 points at or near the export limit\n",
    "is_hh30 = df_30min.index.minute == 30\n",
    "direct_clip_mask = is_hh30 & (df_30min[\"ac_export_w\"] >= clipping_threshold_w)\n",
    "df_30min.loc[direct_clip_mask, \"is_clipped\"] = True\n",
    "print(f\"   - Found {direct_clip_mask.sum()} HH:30 points with direct clipping.\")\n",
    "\n",
    "# Step 2: Propagate flag to adjacent HH:30 points (Temporal Contamination)\n",
    "clipped_at_hh30 = df_30min[\"is_clipped\"][is_hh30]\n",
    "neighbor_clip_mask = clipped_at_hh30.shift(\n",
    "    1, fill_value=False\n",
    ") | clipped_at_hh30.shift(-1, fill_value=False)\n",
    "\n",
    "# --- FIX ---\n",
    "# The original code incorrectly used `neighbor_clip_mask.index`, which flagged all HH:30 points.\n",
    "# The corrected code uses boolean alignment to flag only the HH:30 points where the neighbor_clip_mask is True.\n",
    "df_30min.loc[is_hh30, \"is_clipped\"] |= neighbor_clip_mask\n",
    "print(\n",
    "    f\"   - Propagated clipping flag to {neighbor_clip_mask.sum()} neighboring HH:30 points.\"\n",
    ")\n",
    "\n",
    "# Step 3: Propagate flag to interpolated HH:00 points (Interpolation Contamination)\n",
    "is_hh00 = df_30min.index.minute == 0\n",
    "interpolated_clip_mask = df_30min[\"is_clipped\"].shift(\n",
    "    1, fill_value=False\n",
    ") | df_30min[\"is_clipped\"].shift(-1, fill_value=False)\n",
    "df_30min.loc[is_hh00, \"is_clipped\"] |= interpolated_clip_mask[is_hh00]\n",
    "print(f\"   - Propagated clipping flag to interpolated HH:00 points.\")\n",
    "\n",
    "# --- Curtailment Detection Logic ---\n",
    "print(\"\\n2. Applying Curtailment Detection Logic...\")\n",
    "\n",
    "# Step 1: Directly flag HH:30 points with negative prices and low production during the day\n",
    "# The 'is_day' check prevents flagging nighttime hours where production is naturally zero.\n",
    "direct_curtail_mask = (\n",
    "    is_hh30\n",
    "    & df_30min[\"is_day\"]\n",
    "    & (df_30min[\"spot_price_eur_mwh\"] < 0)\n",
    "    & (df_30min[\"ac_export_w\"] < curtailment_threshold_w)\n",
    ")\n",
    "df_30min.loc[direct_curtail_mask, \"is_curtailed\"] = True\n",
    "print(\n",
    "    f\"   - Found {direct_curtail_mask.sum()} HH:30 points with direct curtailment.\"\n",
    ")\n",
    "\n",
    "# Step 2: Propagate flag to interpolated HH:00 points\n",
    "interpolated_curtail_mask = df_30min[\"is_curtailed\"].shift(\n",
    "    1, fill_value=False\n",
    ") | df_30min[\"is_curtailed\"].shift(-1, fill_value=False)\n",
    "df_30min.loc[is_hh00, \"is_curtailed\"] |= interpolated_curtail_mask[is_hh00]\n",
    "print(f\"   - Propagated curtailment flag to interpolated HH:00 points.\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_clipped = df_30min[\"is_clipped\"].sum()\n",
    "total_curtailed = df_30min[\"is_curtailed\"].sum()\n",
    "print(\"\\n‚úÖ Anomaly flagging complete.\")\n",
    "print(\n",
    "    f\"   - Total points flagged as clipped: {total_clipped} ({total_clipped / len(df_30min):.2%})\"\n",
    ")\n",
    "print(\n",
    "    f\"   - Total points flagged as curtailed: {total_curtailed} ({total_curtailed / len(df_30min):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aee645",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 4.2 Flag Inter-Row Shading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In cell \"4.2 Flag Inter-Row Shading\"\n",
    "\n",
    "### 4.2 Flag Inter-Row Shading\n",
    "\n",
    "# --- Calculate and Flag Inter-Row Shading for Fixed-Tilt Arrays ---\n",
    "# This step models the geometry of the solar park to identify periods where\n",
    "# one row of panels casts a shadow on the row behind it. The flagging logic\n",
    "# is conservative to account for the upsampling process:\n",
    "# 1. Interval Contamination: The power value at HH:30 represents the energy\n",
    "#    from the [HH:00, HH+1:00] interval. If shading occurs at either HH:00\n",
    "#    or HH:30, the entire interval is considered contaminated, and the\n",
    "#    HH:30 point is flagged.\n",
    "# 2. Interpolation Contamination: The power value at HH:00 is interpolated\n",
    "#    from its two HH:30 neighbors. If either neighbor is flagged, the\n",
    "#    interpolated point is also flagged.\n",
    "\n",
    "import pvlib\n",
    "\n",
    "print(\"Attempting to flag inter-row shading periods with contamination logic...\")\n",
    "\n",
    "# Retrieve the specific configuration for the target park\n",
    "park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "mount_config = park_config.get(\"mounting\")\n",
    "\n",
    "# Initialize flag column to False before applying logic\n",
    "df_30min[\"is_shaded\"] = False\n",
    "\n",
    "# Check if the necessary geometric parameters are defined in config.yml\n",
    "if mount_config and mount_config.get(\"gcr\"):\n",
    "    assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "    try:\n",
    "        gcr = mount_config[\"gcr\"]\n",
    "        if not (0 < gcr < 1):\n",
    "            raise ValueError(f\"GCR must be between 0 and 1, but got {gcr}\")\n",
    "\n",
    "        # --- Step 0: Calculate the raw, point-in-time shading fraction ---\n",
    "        shaded_fraction = pvlib.shading.shaded_fraction1d(\n",
    "            solar_zenith=df_30min[\"zenith\"],\n",
    "            solar_azimuth=df_30min[\"azimuth\"],\n",
    "            axis_azimuth=mount_config[\"axis_azimuth\"],\n",
    "            shaded_row_rotation=mount_config[\"surface_tilt\"],\n",
    "            collector_width=1.0,\n",
    "            pitch=1.0 / gcr,\n",
    "            axis_tilt=0,\n",
    "            surface_to_axis_offset=0,\n",
    "            cross_axis_slope=0,\n",
    "        )\n",
    "\n",
    "        # Create a temporary mask for when shading geometrically occurs during the day\n",
    "        point_in_time_shade_mask = (shaded_fraction > 0) & df_30min[\"is_day\"]\n",
    "\n",
    "        # --- Step 1: Apply Interval Contamination to HH:30 points ---\n",
    "        print(\"1. Applying Interval Contamination Logic...\")\n",
    "        is_hh30 = df_30min.index.minute == 30\n",
    "\n",
    "        # An HH:30 point is shaded if the geometric shading occurs at its own\n",
    "        # timestamp OR at the start of its interval (the preceding HH:00 point).\n",
    "        interval_shade_mask = point_in_time_shade_mask | point_in_time_shade_mask.shift(\n",
    "            1, fill_value=False\n",
    "        )\n",
    "\n",
    "        # Apply this logic only to the HH:30 points, which represent the hourly intervals.\n",
    "        direct_shade_mask = is_hh30 & interval_shade_mask\n",
    "        df_30min.loc[direct_shade_mask, \"is_shaded\"] = True\n",
    "        print(\n",
    "            f\"   - Found {direct_shade_mask.sum()} HH:30 points with direct or interval shading.\"\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Apply Interpolation Contamination to HH:00 points ---\n",
    "        print(\"2. Applying Interpolation Contamination Logic...\")\n",
    "        is_hh00 = df_30min.index.minute == 0\n",
    "\n",
    "        # An HH:00 point is shaded if either of its adjacent HH:30 neighbors is shaded.\n",
    "        interpolated_shade_mask = df_30min[\"is_shaded\"].shift(\n",
    "            1, fill_value=False\n",
    "        ) | df_30min[\"is_shaded\"].shift(-1, fill_value=False)\n",
    "        df_30min.loc[is_hh00, \"is_shaded\"] |= interpolated_shade_mask[is_hh00]\n",
    "        print(f\"   - Propagated shading flag to interpolated HH:00 points.\")\n",
    "\n",
    "    except (KeyError, ValueError) as e:\n",
    "        print(\n",
    "            f\"‚ùå SHADING ERROR: Could not calculate shading due to missing or invalid config. Details: {e}\"\n",
    "        )\n",
    "        # The 'is_shaded' column is already False, so no further action needed.\n",
    "else:\n",
    "    print(\n",
    "        \"‚ÑπÔ∏è NOTE: Mounting 'gcr' not found in config.yml. Skipping shading calculation.\"\n",
    "    )\n",
    "    # The 'is_shaded' column is already False, so no further action needed.\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_shaded = df_30min[\"is_shaded\"].sum()\n",
    "daytime_points = df_30min[\"is_day\"].sum()\n",
    "pct_shaded = (total_shaded / daytime_points) * 100 if daytime_points > 0 else 0\n",
    "print(\"\\n‚úÖ Shading flagging complete.\")\n",
    "print(\n",
    "    f\"   - Total points flagged as shaded: {total_shaded} ({pct_shaded:.2f}% of daytime).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e78923",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 4.3 Visually Verify Flags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visually Verify Anomaly Flags with Interactive Plots ---\n",
    "\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def create_flag_verification_plot(\n",
    "    df_day: pd.DataFrame,\n",
    "    flag_col: str,\n",
    "    title: str,\n",
    "    flag_color: str,\n",
    "    normal_color: str = \"royalblue\",\n",
    ") -> go.Figure:\n",
    "    \"\"\"Creates an interactive bar plot to visualize a specific anomaly flag for a single day.\"\"\"\n",
    "\n",
    "    df_plot = df_day.copy()\n",
    "    df_plot[\"color\"] = df_plot[flag_col].map({True: flag_color, False: normal_color})\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_plot.index,\n",
    "            y=df_plot[\"ac_export_w\"],\n",
    "            marker_color=df_plot[\"color\"],\n",
    "            customdata=df_plot[flag_col],\n",
    "            hovertemplate=\"<b>Time</b>: %{x|%H:%M}<br><b>Power</b>: %{y:.0f} W<br><b>Flagged</b>: %{customdata}<extra></extra>\",\n",
    "            # Set period to 30 minutes (in milliseconds) and align bars to the start of the period\n",
    "            xperiod=30 * 60 * 1000,\n",
    "            xperiodalignment=\"middle\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        xaxis_title=\"Time [UTC]\",\n",
    "        yaxis_title=\"AC Export [W]\",\n",
    "        bargap=0.05,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "\n",
    "# --- Plot 1: Clipping Verification ---\n",
    "clipped_days = df_30min[df_30min[\"is_clipped\"]].index.normalize().unique()  # type: ignore\n",
    "\n",
    "if not clipped_days.empty:\n",
    "    random_clipped_day = random.choice(clipped_days)\n",
    "    df_plot_clip = df_30min[df_30min.index.date == random_clipped_day.date()]\n",
    "\n",
    "    fig_clip = create_flag_verification_plot(\n",
    "        df_day=df_plot_clip,\n",
    "        flag_col=\"is_clipped\",\n",
    "        title=f\"Clipping Verification for {random_clipped_day.strftime('%Y-%m-%d')}\",\n",
    "        flag_color=\"crimson\",\n",
    "    )\n",
    "    fig_clip.show()\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No days with clipping were found to plot.\")\n",
    "\n",
    "# --- Plot 2: Curtailment Verification ---\n",
    "curtailed_days = df_30min[df_30min[\"is_curtailed\"]].index.normalize().unique()  # type: ignore\n",
    "\n",
    "if not curtailed_days.empty:\n",
    "    random_curtailed_day = random.choice(curtailed_days)\n",
    "    df_plot_curtail = df_30min[df_30min.index.date == random_curtailed_day.date()]\n",
    "\n",
    "    fig_curtail = create_flag_verification_plot(\n",
    "        df_day=df_plot_curtail,\n",
    "        flag_col=\"is_curtailed\",\n",
    "        title=f\"Curtailment Verification for {random_curtailed_day.strftime('%Y-%m-%d')}\",\n",
    "        flag_color=\"crimson\",\n",
    "    )\n",
    "    fig_curtail.show()\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No days with curtailment were found to plot.\")\n",
    "\n",
    "# --- Plot 3: Shading Verification ---\n",
    "if \"is_shaded\" in df_30min.columns:\n",
    "    shaded_days = df_30min[df_30min[\"is_shaded\"]].index.normalize().unique()  # type: ignore\n",
    "\n",
    "    if not shaded_days.empty:\n",
    "        random_shaded_day = random.choice(shaded_days)\n",
    "        df_plot_shade = df_30min[df_30min.index.date == random_shaded_day.date()]\n",
    "\n",
    "        fig_shade = create_flag_verification_plot(\n",
    "            df_day=df_plot_shade,\n",
    "            flag_col=\"is_shaded\",\n",
    "            title=f\"Shading Verification for {random_shaded_day.strftime('%Y-%m-%d')}\",\n",
    "            flag_color=\"crimson\",\n",
    "        )\n",
    "        fig_shade.show()\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No days with inter-row shading were found to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6817726",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 5. AC Export -> Plane of Array (POA) Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c471d",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff33105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "# This section is now split into two core functions:\n",
    "# 1. `calculate_effective_pdc`: Determines the system's DC capacity at any given\n",
    "#    moment, accounting for initial degradation and user-defined adjustments for\n",
    "#    known issues or upgrades.\n",
    "# 2. `estimate_poa_and_temp_cell`: The iterative solver that, given an effective\n",
    "#    DC capacity, reverses the PV power chain to estimate POA irradiance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pvlib\n",
    "\n",
    "\n",
    "def calculate_effective_pdc(\n",
    "    pdc0: float,\n",
    "    commissioning_date: pd.Timestamp,\n",
    "    degradation_rate: float,\n",
    "    current_timestamp: pd.Timestamp,\n",
    "    adjustments: list[dict] | None = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the effective DC capacity at a given timestamp, accounting for\n",
    "    annual degradation and any specified capacity adjustments.\n",
    "\n",
    "    Args:\n",
    "        pdc0: Nameplate DC power at STC (W).\n",
    "        commissioning_date: The date the system began operation.\n",
    "        degradation_rate: Annual degradation rate (e.g., 0.005 for 0.5%).\n",
    "        current_timestamp: The timestamp for which to calculate the capacity.\n",
    "        adjustments: A pre-processed and sorted list of dictionaries, each\n",
    "                     defining a capacity adjustment period.\n",
    "\n",
    "    Returns:\n",
    "        The effective DC capacity in Watts, or np.nan if the period is excluded.\n",
    "    \"\"\"\n",
    "    # 1. Calculate base degradation\n",
    "    time_delta = current_timestamp - commissioning_date\n",
    "    years_passed = max(0, time_delta.total_seconds() / (365.25 * 24 * 3600))\n",
    "    pdc_degraded = pdc0 * (1 - degradation_rate) ** years_passed\n",
    "\n",
    "    if not adjustments:\n",
    "        return pdc_degraded\n",
    "\n",
    "    # 2. Find and apply the most recent applicable adjustment\n",
    "    # The 'adjustments' list is sorted by start_date descending.\n",
    "    for adj in adjustments:\n",
    "        if current_timestamp >= adj[\"start_date\"]:\n",
    "            # This is the most recent adjustment rule. Check if it's active.\n",
    "            if current_timestamp <= adj[\"end_date\"]:\n",
    "                adj_type = adj[\"adjustment_type\"]\n",
    "\n",
    "                if adj_type == \"exclude\":\n",
    "                    return np.nan  # Exclude this period from analysis\n",
    "\n",
    "                value = adj[\"value\"]\n",
    "                if adj_type == \"percentage\":\n",
    "                    pdc_degraded *= 1 + value\n",
    "                elif adj_type == \"absolute_w\":\n",
    "                    pdc_degraded += value\n",
    "            # Whether the rule was active or expired, it was the most recent one,\n",
    "            # so we can stop searching.\n",
    "            break\n",
    "\n",
    "    return max(0, pdc_degraded)\n",
    "\n",
    "\n",
    "def estimate_poa_and_temp_cell(\n",
    "    p_ac: float,\n",
    "    temp_air: float,\n",
    "    wind_speed: float,\n",
    "    pdc_effective: float,\n",
    "    gamma_pmp: float,\n",
    "    inverter_efficiency: float,\n",
    "    temp_model_params: dict[str, float],\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Iteratively estimates POA irradiance and cell temperature from AC power,\n",
    "    using a pre-calculated effective DC capacity.\n",
    "\n",
    "    Args:\n",
    "        p_ac: AC power output in Watts.\n",
    "        temp_air: Ambient air temperature in Celsius.\n",
    "        wind_speed: Wind speed in m/s.\n",
    "        pdc_effective: Effective DC power of the system at the given timestamp (W).\n",
    "        gamma_pmp: Power temperature coefficient (e.g., -0.004 for -0.4%/¬∞C).\n",
    "        inverter_efficiency: Nominal inverter efficiency (e.g., 0.985).\n",
    "        temp_model_params: Parameters for the SAPM cell temperature model.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - Estimated POA irradiance (W/m^2).\n",
    "        - Estimated cell temperature (¬∞C).\n",
    "        - Final temperature difference upon convergence (¬∞C).\n",
    "        - Number of iterations performed.\n",
    "    \"\"\"\n",
    "    if any(pd.isna(val) for val in [p_ac, temp_air, wind_speed, pdc_effective]):\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    if p_ac <= 0 or pdc_effective <= 0:\n",
    "        return 0.0, temp_air, 0.0, 0\n",
    "\n",
    "    # Constants and initial guess\n",
    "    TEMP_REF = 25.0\n",
    "    IRRAD_REF = 1000.0\n",
    "    MAX_ITER = 10\n",
    "    TOLERANCE = 0.1\n",
    "    temp_cell_guess = temp_air + 20.0\n",
    "    p_dc = p_ac / inverter_efficiency\n",
    "\n",
    "    irrad_estimate = np.nan\n",
    "    temp_cell_new = np.nan\n",
    "    temp_diff = np.nan\n",
    "\n",
    "    for i in range(1, MAX_ITER + 1):\n",
    "        temp_factor = 1 + gamma_pmp * (temp_cell_guess - TEMP_REF)\n",
    "        if temp_factor <= 0:\n",
    "            return 0.0, temp_air, np.nan, i\n",
    "\n",
    "        irrad_estimate = (p_dc / (pdc_effective * temp_factor)) * IRRAD_REF\n",
    "        irrad_estimate = max(0, irrad_estimate)\n",
    "\n",
    "        temp_cell_new = pvlib.temperature.sapm_cell(\n",
    "            poa_global=irrad_estimate,\n",
    "            temp_air=temp_air,\n",
    "            wind_speed=wind_speed,\n",
    "            **temp_model_params,\n",
    "        )\n",
    "        temp_diff = abs(temp_cell_new - temp_cell_guess)\n",
    "\n",
    "        if temp_diff < TOLERANCE:\n",
    "            return irrad_estimate, temp_cell_new, temp_diff, i\n",
    "\n",
    "        temp_cell_guess = temp_cell_new\n",
    "\n",
    "    return irrad_estimate, temp_cell_new, temp_diff, MAX_ITER\n",
    "\n",
    "\n",
    "print(\n",
    "    \"‚úÖ Helper functions calculate_effective_pdc and estimate_poa_and_temp_cell defined.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b4c65",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 5.1 Main Processing and Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing and Data Integration ---\n",
    "\n",
    "# --- Retrieve Park-Specific Configuration ---\n",
    "try:\n",
    "    park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "    system_config = park_config[\"system\"]\n",
    "    temp_model_config = park_config[\"temperature_model\"]\n",
    "\n",
    "    pdc0: float = system_config[\"pdc0\"]\n",
    "    gamma_pmp: float = system_config[\"gamma_pmp\"]\n",
    "    inverter_efficiency: float = system_config[\"inverter_efficiency\"]\n",
    "    degradation_rate: float = system_config[\"degradation_rate\"]\n",
    "    commissioning_date_str: str = system_config[\"commissioning_date\"]\n",
    "    commissioning_ts: pd.Timestamp = pd.to_datetime(commissioning_date_str, utc=True)\n",
    "\n",
    "    # --- Load and Pre-process DC Capacity Adjustments ---\n",
    "    dc_adjustments_raw = park_config.get(\"dc_capacity_adjustments\", [])\n",
    "    dc_adjustments_processed = []\n",
    "    if dc_adjustments_raw:\n",
    "        print(\n",
    "            f\"\\nüîß Found {len(dc_adjustments_raw)} DC capacity adjustment period(s). Pre-processing...\"\n",
    "        )\n",
    "        for adj in dc_adjustments_raw:\n",
    "            try:\n",
    "                adj_type = adj[\"adjustment_type\"]\n",
    "                processed_adj = {\n",
    "                    \"start_date\": pd.to_datetime(adj[\"start_date\"], utc=True),\n",
    "                    \"end_date\": (\n",
    "                        pd.to_datetime(adj.get(\"end_date\"), utc=True)\n",
    "                        if adj.get(\"end_date\")\n",
    "                        else pd.Timestamp.max.tz_localize(\"UTC\")\n",
    "                    ),\n",
    "                    \"adjustment_type\": adj_type,\n",
    "                    # Value is not required for the 'exclude' type\n",
    "                    \"value\": float(adj[\"value\"]) if adj_type != \"exclude\" else None,\n",
    "                }\n",
    "                dc_adjustments_processed.append(processed_adj)\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(\n",
    "                    f\"‚ö†Ô∏è WARNING: Skipping invalid DC capacity adjustment due to missing/invalid key: {adj}. Error: {e}\"\n",
    "                )\n",
    "\n",
    "        # Sort by start_date descending to easily find the latest applicable adjustment\n",
    "        dc_adjustments_processed.sort(key=lambda x: x[\"start_date\"], reverse=True)\n",
    "        print(\"‚úÖ Adjustments processed.\")\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è No DC capacity adjustments defined for this park.\")\n",
    "\n",
    "    print(f\"\\n‚öôÔ∏è Using parameters for '{TARGET_PARK_NAME}':\")\n",
    "    print(f\"  - Commissioning Date: {commissioning_ts.date()}\")\n",
    "    print(f\"  - Nameplate DC Power (pdc0): {pdc0 / 1e3:,.1f} kW\")\n",
    "    print(f\"  - Annual Degradation: {degradation_rate:.1%}\")\n",
    "    print(f\"  - Temp. Coefficient (gamma_pmp): {gamma_pmp * 100:.3f} %/¬∞C\")\n",
    "    print(f\"  - Inverter Efficiency: {inverter_efficiency:.1%}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\n",
    "        f\"‚ùå CONFIGURATION ERROR: Missing key {e} for park '{TARGET_PARK_NAME}' in config.yml.\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# --- Define Temperature Model from Config ---\n",
    "try:\n",
    "    model_type: str = temp_model_config[\"model_type\"]\n",
    "    model_name: str = temp_model_config[\"model_name\"]\n",
    "    temp_model_parameters = pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS[model_type][\n",
    "        model_name\n",
    "    ]\n",
    "    print(f\"\\nüå°Ô∏è Using {model_type.upper()} temperature model: '{model_name}'\")\n",
    "except KeyError:\n",
    "    print(\n",
    "        f\"‚ùå CONFIGURATION ERROR: Invalid temperature model '{model_type}/{model_name}' specified in config.yml.\"  # type: ignore\n",
    "    )\n",
    "    print(\n",
    "        \"Please check available models in pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS.\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# --- Prepare DataFrame for Re-running ---\n",
    "# Drop columns from previous runs to avoid conflicts.\n",
    "cols_to_drop = [\n",
    "    \"pdc_effective_w\",\n",
    "    \"poa_estimated_w_m2\",\n",
    "    \"temp_cell_estimated_c\",\n",
    "    \"temp_convergence_diff\",\n",
    "    \"iterations\",\n",
    "]\n",
    "df_30min = df_30min.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# --- Run POA Estimation ---\n",
    "estimation_mask = (\n",
    "    (df_30min[\"ac_export_w\"] > 0)\n",
    "    & (~df_30min[\"is_clipped\"])\n",
    "    & (~df_30min[\"is_curtailed\"])\n",
    "    & (df_30min[\"is_day\"])\n",
    ")\n",
    "df_analysis = df_30min.loc[estimation_mask].copy()\n",
    "\n",
    "print(f\"\\nüî¨ Calculating effective DC capacity for {len(df_analysis):,} data points...\")\n",
    "\n",
    "# 1. Calculate effective DC capacity for each timestamp\n",
    "df_analysis[\"pdc_effective_w\"] = df_analysis.index.to_series().apply(\n",
    "    lambda ts: calculate_effective_pdc(\n",
    "        pdc0=pdc0,\n",
    "        commissioning_date=commissioning_ts,\n",
    "        degradation_rate=degradation_rate,\n",
    "        current_timestamp=ts,\n",
    "        adjustments=dc_adjustments_processed,\n",
    "    )\n",
    ")\n",
    "print(\"‚úÖ Effective DC capacity calculated.\")\n",
    "\n",
    "print(f\"üî¨ Running POA estimation...\")\n",
    "# 2. Run the iterative estimation using the calculated effective DC capacity\n",
    "results = df_analysis.apply(\n",
    "    lambda row: estimate_poa_and_temp_cell(\n",
    "        p_ac=row[\"ac_export_w\"],\n",
    "        temp_air=row[\"temp_air_c\"],\n",
    "        wind_speed=row[\"wind_speed_m_s\"],\n",
    "        pdc_effective=row[\"pdc_effective_w\"],\n",
    "        gamma_pmp=gamma_pmp,\n",
    "        inverter_efficiency=inverter_efficiency,\n",
    "        temp_model_params=temp_model_parameters,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "if not results.empty:\n",
    "    results_df = pd.DataFrame(\n",
    "        results.tolist(),\n",
    "        index=df_analysis.index,\n",
    "        columns=[\n",
    "            \"poa_estimated_w_m2\",\n",
    "            \"temp_cell_estimated_c\",\n",
    "            \"temp_convergence_diff\",\n",
    "            \"iterations\",\n",
    "        ],\n",
    "    )\n",
    "    # Join both the effective pdc and the estimation results back to the main df\n",
    "    df_30min = df_30min.join(df_analysis[[\"pdc_effective_w\"]])\n",
    "    df_30min = df_30min.join(results_df)\n",
    "\n",
    "print(\"‚úÖ Estimation complete.\")\n",
    "\n",
    "# --- Final Data Cleanup ---\n",
    "print(\"\\nüßπ Cleaning up and filling non-estimated periods...\")\n",
    "night_mask = ~df_30min[\"is_day\"]\n",
    "df_30min.loc[night_mask, \"poa_estimated_w_m2\"] = 0.0\n",
    "df_30min.loc[night_mask, \"temp_cell_estimated_c\"] = df_30min.loc[\n",
    "    night_mask, \"temp_air_c\"\n",
    "]\n",
    "df_30min[\"iterations\"] = df_30min[\"iterations\"].fillna(0).astype(int)\n",
    "print(\n",
    "    \"‚úÖ Cleanup complete. Clipped/curtailed daytime periods remain as NaN for irradiance and temperature.\"\n",
    ")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\n",
    "    \"\\nüìä Sample of dataframe with new estimated POA and effective DC capacity columns:\"\n",
    ")\n",
    "display(df_30min.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f091f1a",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 6. Modeling GHI from Estimated POA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897b751",
   "metadata": {},
   "source": [
    "### 6.1 GHI Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0340f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GHI Modeling ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pvlib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"--- Starting GHI Modeling from Estimated POA ---\")\n",
    "\n",
    "# --- 1. Preparation ---\n",
    "GHI_MODEL_COLS = [\n",
    "    \"ghi_modeled_w_m2\",\n",
    "    \"dni_modeled_w_m2\",\n",
    "    \"dhi_modeled_w_m2\",\n",
    "    \"kt_modeled\",\n",
    "    \"ghi_model_converged\",\n",
    "    \"ghi_model_iterations\",\n",
    "]\n",
    "IRRADIANCE_COLS = GHI_MODEL_COLS[:4]\n",
    "\n",
    "df_30min.drop(\n",
    "    columns=[col for col in GHI_MODEL_COLS if col in df_30min.columns],\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "\n",
    "df_30min[\"ghi_model_converged\"] = pd.Series(\n",
    "    pd.NA, index=df_30min.index, dtype=\"boolean\"\n",
    ")\n",
    "df_30min[\"ghi_model_iterations\"] = np.nan\n",
    "for col in IRRADIANCE_COLS:\n",
    "    df_30min[col] = np.nan\n",
    "\n",
    "location_params = PARK_CONFIGS[TARGET_PARK_NAME][\"location\"]\n",
    "mounting_params = PARK_CONFIGS[TARGET_PARK_NAME][\"mounting\"]\n",
    "\n",
    "# --- 2. GHI Reverse Transposition with Progress Bar ---\n",
    "print(\"Step 1: Calculating GHI for daytime points with valid POA...\")\n",
    "\n",
    "# A more robust mask: only process daytime points with a positive POA estimate.\n",
    "# The > 0 condition implicitly handles both NaNs and zeros.\n",
    "calc_mask = df_30min[\"is_day\"] & (df_30min[\"poa_estimated_w_m2\"] > 0)\n",
    "df_to_process = df_30min.loc[calc_mask]\n",
    "assert isinstance(df_to_process.index, pd.DatetimeIndex)\n",
    "\n",
    "print(f\"   - Found {len(df_to_process)} points to process.\")\n",
    "\n",
    "if not df_to_process.empty:\n",
    "    monthly_groups = df_to_process.groupby(\n",
    "        [df_to_process.index.year, df_to_process.index.month]\n",
    "    )\n",
    "    results_list = []\n",
    "\n",
    "    pbar = tqdm(monthly_groups, desc=\"Modeling GHI (monthly chunks)\")\n",
    "    for (year, month), group in pbar:\n",
    "        pbar.set_postfix_str(f\"{year}-{month:02d}\")\n",
    "\n",
    "        ghi_results_chunk = pvlib.irradiance.ghi_from_poa_driesse_2023(\n",
    "            surface_tilt=mounting_params[\"surface_tilt\"],\n",
    "            surface_azimuth=mounting_params[\"surface_azimuth\"],\n",
    "            solar_zenith=group[\"zenith\"],\n",
    "            solar_azimuth=group[\"azimuth\"],\n",
    "            poa_global=group[\"poa_estimated_w_m2\"],\n",
    "            dni_extra=group[\"dni_extra_w_m2\"],\n",
    "            albedo=location_params[\"albedo\"],\n",
    "            full_output=True,\n",
    "        )\n",
    "\n",
    "        results_list.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"ghi_modeled_w_m2\": ghi_results_chunk[0],\n",
    "                    \"ghi_model_converged\": ghi_results_chunk[1],\n",
    "                    \"ghi_model_iterations\": ghi_results_chunk[2],\n",
    "                },\n",
    "                index=group.index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if results_list:\n",
    "        all_results_df = pd.concat(results_list)\n",
    "        df_30min.update(all_results_df)\n",
    "\n",
    "# --- 3. GHI Decomposition (ERBS Model) ---\n",
    "# Decompose only where GHI was successfully modeled and converged.\n",
    "decomp_mask = df_30min[\"ghi_modeled_w_m2\"].notna() & (\n",
    "    df_30min[\"ghi_model_converged\"] == True\n",
    ")\n",
    "print(f\"Step 2: Decomposing GHI for {decomp_mask.sum()} valid points...\")\n",
    "\n",
    "if decomp_mask.any():\n",
    "    decomposed = pvlib.irradiance.erbs_driesse(\n",
    "        ghi=df_30min.loc[decomp_mask, \"ghi_modeled_w_m2\"],\n",
    "        zenith=df_30min.loc[decomp_mask, \"zenith\"],\n",
    "        datetime_or_doy=df_30min.loc[decomp_mask].index,\n",
    "    ).rename( # type: ignore\n",
    "        columns={\n",
    "            \"dni\": \"dni_modeled_w_m2\",\n",
    "            \"dhi\": \"dhi_modeled_w_m2\",\n",
    "            \"kt\": \"kt_modeled\",\n",
    "        }\n",
    "    )\n",
    "    df_30min.update(decomposed)\n",
    "\n",
    "# --- 4. Data Cleaning and Finalization ---\n",
    "print(\"Step 3: Cleaning and finalizing modeled irradiance data...\")\n",
    "\n",
    "failed_convergence_mask = df_30min[\"ghi_model_converged\"] == False\n",
    "df_30min.loc[failed_convergence_mask, IRRADIANCE_COLS] = np.nan\n",
    "print(\n",
    "    f\"   - Invalidated {failed_convergence_mask.sum()} points due to GHI model non-convergence.\"\n",
    ")\n",
    "\n",
    "unrealistic_dni_mask = df_30min[\"dni_modeled_w_m2\"] > (\n",
    "    df_30min[\"dni_clearsky_w_m2\"] * 1.05\n",
    ")\n",
    "df_30min.loc[unrealistic_dni_mask, IRRADIANCE_COLS] = np.nan\n",
    "print(\n",
    "    f\"   - Invalidated {unrealistic_dni_mask.sum()} points exceeding the clear-sky DNI limit.\"\n",
    ")\n",
    "\n",
    "night_mask = ~df_30min[\"is_day\"]\n",
    "df_30min.loc[night_mask, IRRADIANCE_COLS] = df_30min.loc[\n",
    "    night_mask, IRRADIANCE_COLS\n",
    "].fillna(0)\n",
    "df_30min[\"ghi_model_converged\"] = df_30min[\"ghi_model_converged\"].fillna(False)\n",
    "df_30min.loc[night_mask, \"ghi_model_iterations\"] = df_30min.loc[\n",
    "    night_mask, \"ghi_model_iterations\"\n",
    "].fillna(0)\n",
    "\n",
    "for col in IRRADIANCE_COLS:\n",
    "    if col in df_30min.columns:\n",
    "        df_30min[col] = df_30min[col].clip(lower=0)\n",
    "\n",
    "print(\"\\n‚úÖ GHI Modeling and Decomposition complete.\")\n",
    "\n",
    "display_cols = [\n",
    "    \"poa_estimated_w_m2\",\n",
    "    \"ghi_modeled_w_m2\",\n",
    "    \"dni_modeled_w_m2\",\n",
    "    \"dhi_modeled_w_m2\",\n",
    "    \"kt_modeled\",\n",
    "    \"ghi_clearsky_w_m2\",\n",
    "    \"dni_clearsky_w_m2\",\n",
    "    \"ghi_model_converged\",\n",
    "    \"ghi_model_iterations\",\n",
    "]\n",
    "display(df_30min[display_cols].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e45220",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 6.2 Visual Verification of GHI Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual Verification of GHI Model ---\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "\n",
    "# --- User Configuration ---\n",
    "# Set to a tuple like (2023, 8) (Year, Month) to select a specific month.\n",
    "# Set to None to automatically select a random month from the available data.\n",
    "selected_month: tuple[int, int] | None = None\n",
    "# --------------------------\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "\n",
    "# 1. Determine the target month\n",
    "if selected_month is None:\n",
    "    # Get unique (year, month) pairs from the index\n",
    "    all_months = list(\n",
    "        df_30min.index.to_series().apply(lambda dt: (dt.year, dt.month)).unique()\n",
    "    )\n",
    "\n",
    "    if not all_months:\n",
    "        raise ValueError(\"Dataframe is empty or index is invalid.\")\n",
    "\n",
    "    year, month = random.choice(all_months)\n",
    "    print(f\"No month selected, randomly choosing: Year {year}, Month {month}\")\n",
    "else:\n",
    "    year, month = selected_month\n",
    "    if not 1 <= month <= 12:\n",
    "        raise ValueError(f\"Invalid month number: {month}. Must be between 1 and 12.\")\n",
    "\n",
    "    # Check if the year is in the available data range\n",
    "    if df_30min.index.empty:\n",
    "        raise ValueError(\"Dataframe is empty.\")\n",
    "    min_year, max_year = df_30min.index.year.min(), df_30min.index.year.max()\n",
    "    if not min_year <= year <= max_year:\n",
    "        raise ValueError(\n",
    "            f\"Invalid year: {year}. Data available for years {min_year}-{max_year}.\"\n",
    "        )\n",
    "    print(f\"Plotting user-selected month: Year {year}, Month {month}\")\n",
    "\n",
    "# 2. Create a cell-specific dataframe slice for the selected month\n",
    "# Using partial string indexing is a concise and effective way to slice a DatetimeIndex.\n",
    "month_str = f\"{year}-{month:02d}\"\n",
    "df_month_ghi_verify = df_30min.loc[month_str]\n",
    "\n",
    "if df_month_ghi_verify.empty:\n",
    "    raise ValueError(\n",
    "        f\"No data available for the selected month: {month_str}. \"\n",
    "        f\"Data range is from {df_30min.index.min()} to {df_30min.index.max()}\"\n",
    "    )\n",
    "\n",
    "# --- Plotting ---\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.1,\n",
    "    subplot_titles=(\"AC Power Export\", \"GHI: Modeled vs. Reference\"),\n",
    ")\n",
    "\n",
    "# Row 1: AC Export\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_month_ghi_verify.index,\n",
    "        y=df_month_ghi_verify[\"ac_export_w\"],\n",
    "        name=\"AC Export\",\n",
    "        mode=\"lines\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Row 2: GHI Comparison\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_month_ghi_verify.index,\n",
    "        y=df_month_ghi_verify[\"ghi_w_m2\"],\n",
    "        name=\"Reference GHI\",\n",
    "        mode=\"lines\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_month_ghi_verify.index,\n",
    "        y=df_month_ghi_verify[\"ghi_modeled_w_m2\"],\n",
    "        name=\"Modeled GHI\",\n",
    "        mode=\"lines\",\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"rgba(255, 255, 255, 0.15)\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# --- Layout and Final Touches ---\n",
    "fig.update_layout(\n",
    "    title_text=f\"GHI Model Verification: Month {month}, {year}\",\n",
    "    height=600,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Power (W)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Irradiance (W/m¬≤)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Time (UTC)\", row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253f88f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 6.3 Diagnostic: Daily GHI Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2187606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostic: Daily GHI Comparison ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Starting Daily GHI Comparison ---\")\n",
    "\n",
    "# --- 1. Preparation and Synchronization ---\n",
    "# Select relevant columns, including 'is_day' for accurate point counting.\n",
    "df_comp = df_30min[[\"ghi_w_m2\", \"ghi_modeled_w_m2\", \"is_day\"]].copy()\n",
    "\n",
    "# Synchronize NaNs: if one GHI column has a NaN, set the other to NaN as well.\n",
    "initial_valid_points = (\n",
    "    df_comp[[\"ghi_w_m2\", \"ghi_modeled_w_m2\"]].notna().all(axis=1).sum()\n",
    ")\n",
    "sync_mask = df_comp[\"ghi_w_m2\"].isna() | df_comp[\"ghi_modeled_w_m2\"].isna()\n",
    "df_comp.loc[sync_mask, [\"ghi_w_m2\", \"ghi_modeled_w_m2\"]] = np.nan\n",
    "final_valid_points = df_comp[[\"ghi_w_m2\", \"ghi_modeled_w_m2\"]].notna().all(axis=1).sum()\n",
    "print(\n",
    "    f\"Step 1: Synchronized NaNs. Valid concurrent points for comparison: {final_valid_points} (down from {initial_valid_points}).\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2. Integrate to Daily Energy (Wh/m¬≤) ---\n",
    "def daily_energy_wh(series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Integrates a power series (in Watts) over a day to get energy (in Watt-hours).\n",
    "    Uses the trapezoidal rule for accuracy.\n",
    "    \"\"\"\n",
    "    series = series.dropna()\n",
    "    assert isinstance(series.index, pd.DatetimeIndex)\n",
    "    if len(series) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    time_in_hours = series.index.hour + series.index.minute / 60.0\n",
    "    # Use the modern `trapezoid` function instead of the deprecated `trapz`.\n",
    "    return np.trapezoid(y=series.values, x=time_in_hours.values)  # type: ignore\n",
    "\n",
    "\n",
    "print(\"Step 2: Aggregating 30-min power (W/m¬≤) to daily energy (Wh/m¬≤)...\")\n",
    "# Apply the function to the GHI columns. `resample` creates a continuous daily index.\n",
    "df_daily_ghi_comparison = (\n",
    "    df_comp[[\"ghi_w_m2\", \"ghi_modeled_w_m2\"]].resample(\"D\").apply(daily_energy_wh)  # type: ignore\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_daily_ghi_comparison.rename(\n",
    "    columns={\n",
    "        \"ghi_w_m2\": \"ghi_ref_wh_m2\",\n",
    "        \"ghi_modeled_w_m2\": \"ghi_modeled_wh_m2\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# --- 3. Add Daytime Point Count ---\n",
    "print(\"Step 3: Counting valid daytime data points per day...\")\n",
    "# Filter for daytime points within the synchronized dataframe\n",
    "daytime_points = df_comp.loc[df_comp[\"is_day\"]]\n",
    "# Resample and count non-NaN values for each day\n",
    "daily_point_counts = daytime_points[\"ghi_w_m2\"].resample(\"D\").count()\n",
    "df_daily_ghi_comparison[\"daytime_point_count\"] = daily_point_counts\n",
    "\n",
    "\n",
    "# --- 4. Calculate Deltas ---\n",
    "print(\"Step 4: Calculating daily absolute and relative deltas...\")\n",
    "df_daily_ghi_comparison[\"delta_abs_wh_m2\"] = (\n",
    "    df_daily_ghi_comparison[\"ghi_modeled_wh_m2\"]\n",
    "    - df_daily_ghi_comparison[\"ghi_ref_wh_m2\"]\n",
    ")\n",
    "\n",
    "df_daily_ghi_comparison[\"delta_rel_pct\"] = (\n",
    "    df_daily_ghi_comparison[\"delta_abs_wh_m2\"]\n",
    "    / df_daily_ghi_comparison[\"ghi_ref_wh_m2\"]\n",
    ") * 100\n",
    "df_daily_ghi_comparison.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# --- 5. Finalize and Display ---\n",
    "print(\n",
    "    f\"\\n‚úÖ Daily GHI comparison complete. Generated {len(df_daily_ghi_comparison)} daily summaries.\"\n",
    ")\n",
    "\n",
    "# Display a sample and descriptive statistics of the resulting dataframe\n",
    "display(df_daily_ghi_comparison.sample(5))\n",
    "print(\"\\nDescriptive Statistics for Daily GHI Comparison:\")\n",
    "# Describe only the numeric columns, focusing on days with valid data.\n",
    "display(df_daily_ghi_comparison.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3b1ad",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 6.4 Visualization: Daily GHI Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization: Daily GHI Comparison ---\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Define the year you want to inspect.\n",
    "# It defaults to the most recent full year available in the data.\n",
    "assert isinstance(df_daily_ghi_comparison.index, pd.DatetimeIndex)\n",
    "# YEAR_TO_PLOT = df_daily_ghi_comparison.index.year.max()\n",
    "YEAR_TO_PLOT = 2020\n",
    "\n",
    "print(f\"--- Generating Daily GHI Comparison Plot for {YEAR_TO_PLOT} ---\")\n",
    "\n",
    "# Filter the dataframe for the selected year\n",
    "df_daily_ghi_plot = df_daily_ghi_comparison[df_daily_ghi_comparison.index.year == YEAR_TO_PLOT]\n",
    "\n",
    "# --- 2. Create the Figure with Subplots ---\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    subplot_titles=(\n",
    "        f\"Daily GHI: Modeled vs. Reference ({YEAR_TO_PLOT})\",\n",
    "        f\"Relative Delta ({YEAR_TO_PLOT})\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- 3. Add Traces ---\n",
    "\n",
    "# Define a hover template for rich, consistent tooltips\n",
    "hovertemplate = (\n",
    "    \"<b>%{x|%Y-%m-%d}</b><br>\"\n",
    "    + \"Value: %{y:,.0f}<br>\"\n",
    "    + \"Points: %{customdata[0]}<extra></extra>\"\n",
    ")\n",
    "\n",
    "# TOP PLOT: Daily Energy Comparison\n",
    "# Add the reference trace first (this is the baseline for the fill)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_daily_ghi_plot.index,\n",
    "        y=df_daily_ghi_plot[\"ghi_ref_wh_m2\"],\n",
    "        name=\"Reference GHI (Wh/m¬≤)\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"royalblue\", width=1.5),\n",
    "        customdata=df_daily_ghi_plot[[\"daytime_point_count\"]],\n",
    "        hovertemplate=hovertemplate,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Add the modeled trace with a fill to the reference trace\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_daily_ghi_plot.index,\n",
    "        y=df_daily_ghi_plot[\"ghi_modeled_wh_m2\"],\n",
    "        name=\"Modeled GHI (Wh/m¬≤)\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"orange\", width=1.5),\n",
    "        fill=\"tonexty\",  # This fills the area to the previously added trace\n",
    "        fillcolor=\"rgba(255,165,0,0.2)\",\n",
    "        customdata=df_daily_ghi_plot[[\"daytime_point_count\"]],\n",
    "        hovertemplate=hovertemplate,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# BOTTOM PLOT: Relative Delta\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_daily_ghi_plot.index,\n",
    "        y=df_daily_ghi_plot[\"delta_rel_pct\"],\n",
    "        name=\"Relative Delta (%)\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"firebrick\", width=1.5),\n",
    "        customdata=df_daily_ghi_plot[[\"daytime_point_count\"]],\n",
    "        hovertemplate=\"<b>%{x|%Y-%m-%d}</b><br>\"\n",
    "        + \"Delta: %{y:.2f}%<br>\"\n",
    "        + \"Points: %{customdata[0]}<extra></extra>\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Add a zero-line to the delta plot for easy reference\n",
    "fig.add_hline(\n",
    "    y=0,\n",
    "    line_width=1,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"gray\",\n",
    "    row=2, # type: ignore\n",
    "    col=1, # type: ignore\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Update Layout and Finalize ---\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    xaxis_showticklabels=True,\n",
    "    xaxis2_showticklabels=True,\n",
    ")\n",
    "\n",
    "# Update y-axis titles\n",
    "fig.update_yaxes(title_text=\"Daily Energy (Wh/m¬≤)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Relative Delta (%)\", row=2, col=1)\n",
    "\n",
    "# Ensure the x-axis range slider is not visible for a cleaner look\n",
    "fig.update_xaxes(rangeslider_visible=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea3f80",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 7. Advanced GHI Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7d7cc",
   "metadata": {},
   "source": [
    "### 7.1 Golden DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Golden DataFrame Construction ---\n",
    "\n",
    "# --- Imports and Constants ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- Configuration for Golden Day Selection ---\n",
    "\n",
    "# Defines the tolerance for how far the parabola's vertex can be from solar noon.\n",
    "# A value of 60 minutes is a reasonable starting point.\n",
    "VERTEX_NOON_TOLERANCE_MINUTES = 60\n",
    "\n",
    "# Defines the quantile for R¬≤ scores to be considered \"golden\".\n",
    "# 0.85 means we select the top 15% of days with the best parabolic fits.\n",
    "R2_QUANTILE_THRESHOLD = 0.85\n",
    "\n",
    "# Defines the minimum number of valid data points required within a day\n",
    "# to attempt a parabolic fit. This prevents fitting on sparse data.\n",
    "# A value of 7 corresponds to at least 3.5 hours of valid data.\n",
    "MIN_DATAPOINTS_FOR_FIT = 7\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "def get_parabolic_fit_details(\n",
    "    day_df: pd.DataFrame, column_name: str, solar_noon_ts: pd.Timestamp\n",
    ") -> dict[str, float | pd.Timestamp] | None:\n",
    "    \"\"\"\n",
    "    Fits a parabola to a given data series for a full day, validates it, and returns fit details.\n",
    "\n",
    "    Validation criteria:\n",
    "    1. The series must contain at least MIN_DATAPOINTS_FOR_FIT non-null data points.\n",
    "    2. The fitted parabola must curve downwards (coefficient 'a' < 0).\n",
    "    3. The parabola's vertex must be temporally close to the actual solar noon.\n",
    "\n",
    "    Args:\n",
    "        day_df: DataFrame containing the time series data for a single day.\n",
    "        column_name: The name of the column to fit the parabola to.\n",
    "        solar_noon_ts: The timestamp of the solar noon for that day.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with fit details (r2, a, vertex_time) or None if validation fails.\n",
    "    \"\"\"\n",
    "    series_to_fit = day_df[column_name].dropna()\n",
    "    if len(series_to_fit) < MIN_DATAPOINTS_FOR_FIT:\n",
    "        return None\n",
    "\n",
    "    time_seconds = (series_to_fit.index - series_to_fit.index.min()).total_seconds()\n",
    "    values = series_to_fit.to_numpy()\n",
    "\n",
    "    try:\n",
    "        coeffs = np.polyfit(time_seconds, values, 2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return None\n",
    "\n",
    "    a, b, c = coeffs\n",
    "    if a >= 0:\n",
    "        return None\n",
    "\n",
    "    vertex_x_seconds = -b / (2 * a)\n",
    "    vertex_timestamp = series_to_fit.index.min() + pd.Timedelta(\n",
    "        seconds=vertex_x_seconds\n",
    "    )\n",
    "    time_diff_minutes = abs((vertex_timestamp - solar_noon_ts).total_seconds()) / 60\n",
    "    if time_diff_minutes > VERTEX_NOON_TOLERANCE_MINUTES:\n",
    "        return None\n",
    "\n",
    "    values_predicted = np.polyval(coeffs, time_seconds)\n",
    "    r2 = r2_score(values, values_predicted)\n",
    "    return {\"r2\": r2, \"a\": a, \"vertex_time\": vertex_timestamp}\n",
    "\n",
    "\n",
    "# --- Main Analysis ---\n",
    "\n",
    "# --- Step 1: Prepare a temporary DataFrame for clear-sky day detection ---\n",
    "print(\"Step 1: Preparing data for clear-sky day detection...\")\n",
    "df_fit_analysis = df_30min.copy()\n",
    "assert isinstance(df_fit_analysis.index, pd.DatetimeIndex)\n",
    "\n",
    "# Invalidate AC export data points that are not suitable for parabolic fitting.\n",
    "# This includes flagged data and daytime zero-export, which would distort the fit.\n",
    "# Reference GHI is unaffected as it's an independent measurement.\n",
    "fit_invalidation_mask = (\n",
    "    df_fit_analysis[\"is_clipped\"]\n",
    "    | df_fit_analysis[\"is_curtailed\"]\n",
    "    | df_fit_analysis[\"is_shaded\"]\n",
    "    | ((df_fit_analysis[\"ac_export_w\"] <= 0) & df_fit_analysis[\"is_day\"])\n",
    ")\n",
    "df_fit_analysis.loc[fit_invalidation_mask, \"ac_export_w\"] = np.nan\n",
    "print(\n",
    "    f\"   - Invalidated {fit_invalidation_mask.sum()} AC export data points for the fitting process.\"\n",
    ")\n",
    "\n",
    "# --- Step 2: Identify clear-sky days using parabolic fitting on full-day data ---\n",
    "print(\"\\nStep 2: Identifying clear-sky days via parabolic fitting...\")\n",
    "df_fit_analysis[\"date_only\"] = df_fit_analysis.index.date\n",
    "\n",
    "# Identify solar noon for each day, which is still needed for vertex validation\n",
    "solar_noon_times = (\n",
    "    df_fit_analysis.loc[df_fit_analysis[\"is_day\"]]\n",
    "    .groupby(\"date_only\")[\"zenith\"]\n",
    "    .idxmin()\n",
    ")\n",
    "solar_noon_map = {date: ts for date, ts in solar_noon_times.items()}\n",
    "df_fit_analysis[\"solar_noon\"] = df_fit_analysis[\"date_only\"].map(solar_noon_map)\n",
    "\n",
    "# Isolate all valid daylight data for the fitting process\n",
    "df_daylight_for_fit = df_fit_analysis[\n",
    "    df_fit_analysis[\"is_day\"] & df_fit_analysis[\"solar_noon\"].notna()\n",
    "].copy()\n",
    "\n",
    "# Calculate and validate parabolic fits for AC export and reference GHI\n",
    "print(\"   - Calculating and validating parabolic fits for AC export and GHI...\")\n",
    "fit_details_ac = (\n",
    "    df_daylight_for_fit.groupby(\"date_only\")\n",
    "    .apply(\n",
    "        lambda group_df: get_parabolic_fit_details(\n",
    "            group_df, \"ac_export_w\", group_df[\"solar_noon\"].iloc[0]  # type: ignore\n",
    "        ),\n",
    "        include_groups=False,  # type: ignore\n",
    "    )\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "fit_details_ghi = (\n",
    "    df_daylight_for_fit.groupby(\"date_only\")\n",
    "    .apply(\n",
    "        lambda group_df: get_parabolic_fit_details(\n",
    "            group_df, \"ghi_w_m2\", group_df[\"solar_noon\"].iloc[0]  # type: ignore\n",
    "        ),\n",
    "        include_groups=False,  # type: ignore\n",
    "    )\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Combine fit results\n",
    "df_ac_fits = pd.DataFrame(\n",
    "    fit_details_ac.tolist(), index=fit_details_ac.index\n",
    ").add_prefix(\"ac_\")\n",
    "df_ghi_fits = pd.DataFrame(\n",
    "    fit_details_ghi.tolist(), index=fit_details_ghi.index\n",
    ").add_prefix(\"ghi_\")\n",
    "window_fit_scores = df_ac_fits.join(df_ghi_fits, how=\"inner\")\n",
    "\n",
    "# Determine R-squared thresholds and identify golden days\n",
    "if window_fit_scores.empty:\n",
    "    raise ValueError(\n",
    "        \"No valid days found after parabolic fit validation. Try adjusting parameters or check data quality.\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    \"   - Determining R-squared thresholds and selecting clear-sky ('golden') days...\"\n",
    ")\n",
    "r2_ac_threshold = window_fit_scores[\"ac_r2\"].quantile(R2_QUANTILE_THRESHOLD)\n",
    "r2_ghi_threshold = window_fit_scores[\"ghi_r2\"].quantile(R2_QUANTILE_THRESHOLD)\n",
    "\n",
    "top_percent = int(np.round((1 - R2_QUANTILE_THRESHOLD) * 100))\n",
    "print(\n",
    "    f\"     - Using AC Power R¬≤ threshold: {r2_ac_threshold:.4f} (Top {top_percent}% of valid fits)\"\n",
    ")\n",
    "print(\n",
    "    f\"     - Using Reference GHI R¬≤ threshold: {r2_ghi_threshold:.4f} (Top {top_percent}% of valid fits)\"\n",
    ")\n",
    "\n",
    "golden_dates = window_fit_scores[\n",
    "    (window_fit_scores[\"ac_r2\"] > r2_ac_threshold)\n",
    "    & (window_fit_scores[\"ghi_r2\"] > r2_ghi_threshold)\n",
    "].index\n",
    "\n",
    "# --- Step 3: Construct the Final \"Golden Dataset\" ---\n",
    "print(\"\\nStep 3: Constructing the final 'golden' DataFrame...\")\n",
    "\n",
    "# Start with the original, unmodified data\n",
    "df_golden = df_30min.copy()\n",
    "\n",
    "# Define conditions for invalidating data points\n",
    "is_not_golden_day = ~df_golden.index.to_series().dt.date.isin(golden_dates)\n",
    "is_contaminated = (\n",
    "    df_golden[\"is_clipped\"] | df_golden[\"is_curtailed\"] | df_golden[\"is_shaded\"]\n",
    ")\n",
    "is_night = ~df_golden[\"is_day\"]\n",
    "\n",
    "# Invalidation mask for model-derived columns, which are sensitive to contamination\n",
    "model_invalidation_mask = is_not_golden_day | is_contaminated | is_night\n",
    "# Invalidation mask for reference GHI, which is only filtered by day quality and time\n",
    "reference_invalidation_mask = is_not_golden_day | is_night\n",
    "\n",
    "# Define the columns for the final dataset\n",
    "columns_model_derived = [\"ghi_modeled_w_m2\", \"temp_cell_estimated_c\"]\n",
    "column_reference = \"ghi_w_m2\"\n",
    "final_columns = columns_model_derived + [column_reference]\n",
    "\n",
    "# Apply the respective masks to nullify invalid data\n",
    "df_golden.loc[model_invalidation_mask, columns_model_derived] = np.nan\n",
    "df_golden.loc[reference_invalidation_mask, column_reference] = np.nan\n",
    "\n",
    "# Create the final DataFrame with only the essential columns for validation\n",
    "df_golden = df_golden[final_columns]\n",
    "\n",
    "print(\n",
    "    f\"\\nConstructed 'golden' dataset with {int(df_golden[columns_model_derived].notna().all(axis=1).sum())} fully valid modeled data points\"\n",
    "    f\" from {len(golden_dates)} clear-sky days.\"\n",
    ")\n",
    "print(\"The 'df_golden' DataFrame is now ready for advanced validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3f585",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 7.2 Visual Comparison on Golden Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d034fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual Comparison on Golden Days ---\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "if df_golden.empty:\n",
    "    print(\"The 'golden' dataset is empty. Cannot generate the time series plot.\")\n",
    "else:\n",
    "    # Reindex for gapped plotting\n",
    "    full_range_index = pd.date_range(\n",
    "        start=df_golden.index.min(), end=df_golden.index.max(), freq=\"30min\"\n",
    "    )\n",
    "    df_plot = df_golden.reindex(full_range_index)\n",
    "\n",
    "    # --- Create the plot using graph_objects ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add Modeled GHI trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.index,\n",
    "            y=df_plot[\"ghi_modeled_w_m2\"],\n",
    "            name=\"Modeled GHI\",\n",
    "            hovertemplate=\"<b>%{x}</b><br>GHI: %{y:.2f} W/m¬≤\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add Reference GHI trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.index,\n",
    "            y=df_plot[\"ghi_w_m2\"],\n",
    "            name=\"Reference GHI\",\n",
    "            hovertemplate=\"<b>%{x}</b><br>GHI: %{y:.2f} W/m¬≤\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Update layout and axis titles ---\n",
    "    fig.update_layout(\n",
    "        title=f\"Modeled vs. Reference GHI on Golden Days for {TARGET_PARK_NAME}\",\n",
    "        legend_title_text=\"GHI Source\",\n",
    "        yaxis_title=\"GHI (W/m¬≤)\",\n",
    "        xaxis_title=\"Date / Time (UTC)\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f421638",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 7.3 Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e25f5",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pcolors\n",
    "import statsmodels.api as sm\n",
    "import colorsys\n",
    "from statsmodels.regression.linear_model import RegressionResultsWrapper\n",
    "\n",
    "\n",
    "def plot_regression_scatter(\n",
    "    df_reg: pd.DataFrame,\n",
    "    x_col: str,\n",
    "    y_col: str,\n",
    "    ols_model: RegressionResultsWrapper,\n",
    "    title: str,\n",
    "    x_label: str,\n",
    "    y_label: str,\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Generates a standardized regression scatter plot with a Year-Quarter color scheme.\n",
    "    \"\"\"\n",
    "    # --- Color Preparation ---\n",
    "    df_plot = df_reg.copy()\n",
    "    assert isinstance(df_plot.index, pd.DatetimeIndex)\n",
    "    df_plot[\"Timestamp\"] = df_plot.index\n",
    "    df_plot[\"Year\"] = df_plot.index.year\n",
    "    df_plot[\"Quarter\"] = df_plot.index.quarter\n",
    "\n",
    "    unique_years = sorted(df_plot[\"Year\"].unique())\n",
    "    base_colors_hex = pcolors.qualitative.Plotly\n",
    "    year_colors = {\n",
    "        year: base_colors_hex[i % len(base_colors_hex)]\n",
    "        for i, year in enumerate(unique_years)\n",
    "    }\n",
    "    quarter_lightness = {1: 0.80, 2: 0.65, 3: 0.50, 4: 0.35}\n",
    "\n",
    "    color_map = {}\n",
    "    for year, hex_color in year_colors.items():\n",
    "        color_map[year] = {}\n",
    "        rgb_float = tuple(c / 255.0 for c in pcolors.hex_to_rgb(hex_color))\n",
    "        h, l, s = colorsys.rgb_to_hls(*rgb_float)\n",
    "        for quarter, lightness_mod in quarter_lightness.items():\n",
    "            new_rgb_float = colorsys.hls_to_rgb(h, lightness_mod, s)\n",
    "            new_rgb_int = tuple(int(c * 255) for c in new_rgb_float)\n",
    "            color_map[year][quarter] = f\"rgb{new_rgb_int}\"\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter traces\n",
    "    for (year, quarter), group in df_plot.groupby([\"Year\", \"Quarter\"]):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=group[x_col],\n",
    "                y=group[y_col],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=color_map[year][quarter], opacity=0.6, size=5),\n",
    "                name=f\"{year}-Q{quarter}\",\n",
    "                customdata=group[[\"ghi_modeled_w_m2\", \"ghi_w_m2\", \"Timestamp\"]],\n",
    "                hovertemplate=(\n",
    "                    \"<b>%{customdata[2]|%Y-%m-%d %H:%M}</b><br><br>\"\n",
    "                    f\"{x_label}: %{{x:.1f}}<br>\"\n",
    "                    f\"{y_label}: %{{y:.2f}}<br>\"\n",
    "                    \"Modeled GHI: %{customdata[0]:.1f}<br>\"\n",
    "                    \"Reference GHI: %{customdata[1]:.1f}\"\n",
    "                    \"<extra></extra>\"\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add OLS trendline\n",
    "    x_range = [df_plot[x_col].min(), df_plot[x_col].max()]\n",
    "    y_range = [ols_model.params[\"const\"] + ols_model.params[x_col] * x for x in x_range]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=y_range,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"red\", width=2),\n",
    "            name=\"OLS Trendline\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Layout ---\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=x_label,\n",
    "        yaxis_title=y_label,\n",
    "        legend_title=\"Year-Quarter\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "print(\"Helper function plot_regression_scatter defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9459238",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 7.3.1 Regression for Absolute Delta\n",
    "\n",
    "Here we analyze the absolute error `(Modeled - Reference)` in W/m¬≤ to identify any temperature-dependent additive bias in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regression for Absolute Delta ---\n",
    "\n",
    "# --- 0. Configuration ---\n",
    "# Define the minimum estimated cell temperature (¬∞C) required for data points\n",
    "# to be included in the regression analysis. This helps filter out cold\n",
    "# periods (potential snow cover) that skew results.\n",
    "MIN_CELL_TEMP_FOR_REGRESSION_C = 5.0\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "# Filter data to exclude low cell temperatures\n",
    "df_abs_reg = df_golden[\n",
    "    df_golden[\"temp_cell_estimated_c\"] >= MIN_CELL_TEMP_FOR_REGRESSION_C\n",
    "].copy()\n",
    "df_abs_reg[\"delta_ghi_w_m2\"] = df_abs_reg[\"ghi_modeled_w_m2\"] - df_abs_reg[\"ghi_w_m2\"]\n",
    "\n",
    "# --- 2. OLS Regression ---\n",
    "X = sm.add_constant(df_abs_reg[\"temp_cell_estimated_c\"])\n",
    "y = df_abs_reg[\"delta_ghi_w_m2\"]\n",
    "ols_model_abs = sm.OLS(y, X, missing=\"drop\").fit()\n",
    "\n",
    "# --- 3. Plotting ---\n",
    "fig_abs = plot_regression_scatter(\n",
    "    df_reg=df_abs_reg,\n",
    "    x_col=\"temp_cell_estimated_c\",\n",
    "    y_col=\"delta_ghi_w_m2\",\n",
    "    ols_model=ols_model_abs,\n",
    "    title=(\n",
    "        f\"Absolute Model Error vs. Cell Temperature for {TARGET_PARK_NAME}\\n\"\n",
    "        f\"(Filtered: Cell Temp >= {MIN_CELL_TEMP_FOR_REGRESSION_C}¬∞C)\"\n",
    "    ),\n",
    "    x_label=\"Estimated Cell Temperature (¬∞C)\",\n",
    "    y_label=\"Delta (W/m¬≤)\",\n",
    ")\n",
    "fig_abs.show()\n",
    "\n",
    "# --- 4. Summary ---\n",
    "print(\"\\n--- OLS Regression Results (Absolute Delta) ---\")\n",
    "print(ols_model_abs.summary())\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c461ee",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 7.3.2 Regression for Relative Delta\n",
    "\n",
    "Next, we analyze the relative error as a percentage. This helps identify multiplicative biases that scale with irradiance. We filter out low-irradiance data to avoid numerical instability where small absolute errors can lead to huge relative errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66811c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regression for Relative Delta ---\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "# Filter out low irradiance values and low cell temperatures (using the threshold defined in 7.3.1)\n",
    "df_rel_reg = df_golden[\n",
    "    (df_golden[\"ghi_w_m2\"] > 100)\n",
    "    & (df_golden[\"temp_cell_estimated_c\"] >= MIN_CELL_TEMP_FOR_REGRESSION_C)\n",
    "].copy()\n",
    "\n",
    "# Calculate relative delta as a percentage\n",
    "df_rel_reg[\"delta_ghi_relative\"] = (\n",
    "    (df_rel_reg[\"ghi_modeled_w_m2\"] - df_rel_reg[\"ghi_w_m2\"])\n",
    "    / df_rel_reg[\"ghi_w_m2\"]\n",
    "    * 100\n",
    ")\n",
    "\n",
    "# --- 2. OLS Regression ---\n",
    "X = sm.add_constant(df_rel_reg[\"temp_cell_estimated_c\"])\n",
    "y = df_rel_reg[\"delta_ghi_relative\"]\n",
    "ols_model_rel = sm.OLS(y, X, missing=\"drop\").fit()\n",
    "\n",
    "# --- 3. Plotting ---\n",
    "fig_rel = plot_regression_scatter(\n",
    "    df_reg=df_rel_reg,\n",
    "    x_col=\"temp_cell_estimated_c\",\n",
    "    y_col=\"delta_ghi_relative\",\n",
    "    ols_model=ols_model_rel,\n",
    "    title=(\n",
    "        f\"Relative Model Error vs. Cell Temperature for {TARGET_PARK_NAME}\\n\"\n",
    "        f\"(Filtered: GHI > 100 W/m¬≤ and Cell Temp >= {MIN_CELL_TEMP_FOR_REGRESSION_C}¬∞C)\"\n",
    "    ),\n",
    "    x_label=\"Estimated Cell Temperature (¬∞C)\",\n",
    "    y_label=\"Relative Delta (%)\",\n",
    ")\n",
    "fig_rel.show()\n",
    "\n",
    "# --- 4. Summary ---\n",
    "print(\"\\n--- OLS Regression Results (Relative Delta) ---\")\n",
    "print(ols_model_rel.summary())\n",
    "print(\"---------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
