{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b4f091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Solar Irradiance From AC Export\n",
    "\n",
    "A Jupyter Notebook that does it's best to model and construct a historical solar irradiance time series from solar panel park's historical AC export data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb98ad",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5f3b9",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "# Standard Library Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"✅ Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd074bad",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 1.2 Configuration\n",
    "\n",
    "This project uses a two-step configuration process:\n",
    "\n",
    "1.  **Path Definition (`.env`):** This file defines the project's physical location (`PROJECT_ROOT`) and the name of the configuration file. This separation ensures the notebook is portable across different machines and environments.\n",
    "2.  **Parameter Definition (`config.yml`):** This file contains the physical and electrical parameters of your solar park(s), including sensitive information like GPS coordinates and detailed system specifications.\n",
    "\n",
    "**To get started:**\n",
    "\n",
    "1.  **Configure Paths:** Copy the template file `.env.example` to a new file named `.env`. Open `.env` and set the absolute path for the `PROJECT_ROOT` variable.\n",
    "2.  **Configure Parks:** Copy the example configuration file `config.example.yml` to `config.yml`. Open `config.yml` and replace the placeholder values with the details of your solar installation.\n",
    "\n",
    "The cell below loads the environment variables, resolves the final configuration path, and sets up the plotting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define paths using environment variables\n",
    "PROJECT_ROOT_STR = os.getenv(\"PROJECT_ROOT\")\n",
    "CONFIG_FILENAME = os.getenv(\"CONFIG_FILENAME\", \"config.yml\")  # Fallback to config.yml\n",
    "PRODUCTION_AND_PRICE_FILE_PATH = os.getenv(\n",
    "    \"PRODUCTION_AND_PRICE_FILE_PATH\",\n",
    "    \"/home/user/solar-irradiance-from-ac-export/production.csv\",\n",
    ")\n",
    "WEATHER_FILE_PATH = os.getenv(\n",
    "    \"WEATHER_FILE_PATH\", \"/home/user/solar-irradiance-from-ac-export/weather.csv\"\n",
    ")\n",
    "OUTPUT_DIR_STR = os.getenv(\"OUTPUT_DIR\")\n",
    "\n",
    "if not PROJECT_ROOT_STR:\n",
    "    # If PROJECT_ROOT is not set in .env, assume the current working directory\n",
    "    PROJECT_ROOT_STR = os.getcwd()\n",
    "    print(\n",
    "        f\"⚠️ WARNING: PROJECT_ROOT not set in .env. Using current directory: {PROJECT_ROOT_STR}\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = Path(PROJECT_ROOT_STR)\n",
    "CONFIG_PATH = PROJECT_ROOT / CONFIG_FILENAME\n",
    "\n",
    "if not OUTPUT_DIR_STR:\n",
    "    # If OUTPUT_DIR is not set, default to a subdirectory within the project root\n",
    "    OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "    print(\n",
    "        f\"⚠️ WARNING: OUTPUT_DIR not set in .env. Using default directory: {OUTPUT_DIR}\"\n",
    "    )\n",
    "else:\n",
    "    OUTPUT_DIR = Path(OUTPUT_DIR_STR)\n",
    "\n",
    "print(f\"Project Root defined as: {PROJECT_ROOT}\")\n",
    "print(f\"Configuration file path: {CONFIG_PATH}\")\n",
    "print(f\"Output directory defined as: {OUTPUT_DIR}\")\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Extract park configurations\n",
    "    PARK_CONFIGS = config.get(\"parks\", {})\n",
    "\n",
    "    if not PARK_CONFIGS:\n",
    "        raise ValueError(\n",
    "            \"No parks defined under the 'parks' key in the configuration file.\"\n",
    "        )\n",
    "\n",
    "    # Create a list of park names for easy iteration later\n",
    "    PARK_NAMES = list(PARK_CONFIGS.keys())\n",
    "\n",
    "    # --- Load and Validate Target Park for Analysis ---\n",
    "    TARGET_PARK_NAME = os.getenv(\"TARGET_PARK_NAME\")\n",
    "\n",
    "    if not TARGET_PARK_NAME:\n",
    "        raise ValueError(\"TARGET_PARK_NAME is not set in the .env file. Please specify which park to analyze.\")\n",
    "\n",
    "    if TARGET_PARK_NAME not in PARK_NAMES:\n",
    "        raise ValueError(\n",
    "            f\"The target park '{TARGET_PARK_NAME}' defined in .env is not found in 'config.yml'.\\n\"\n",
    "            f\"Available parks in config: {PARK_NAMES}\"\n",
    "        )\n",
    "\n",
    "    print(f\"🎯 Analysis will be performed for target park: '{TARGET_PARK_NAME}'\")\n",
    "\n",
    "    print(\n",
    "        f\"✅ Configuration loaded successfully from '{CONFIG_PATH}' for {len(PARK_NAMES)} park(s): {', '.join(PARK_NAMES)}.\"\n",
    "    )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ CONFIGURATION ERROR: The '{CONFIG_PATH}' file was not found.\")\n",
    "    print(\n",
    "        \"Please check your .env file's PROJECT_ROOT setting, and ensure 'config.yml' exists at that location.\"\n",
    "    )\n",
    "    print(\n",
    "        \"If 'config.yml' is missing, copy 'config.example.yml' to 'config.yml' and fill in your park's details.\"\n",
    "    )\n",
    "except (yaml.YAMLError, ValueError) as e:\n",
    "    print(\n",
    "        f\"❌ CONFIGURATION ERROR: Could not parse '{CONFIG_PATH}'. Please check its format. Details: {e}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Plotting and Display Configuration ---\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# Set display options for better viewing in Jupyter\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "print(\"Plotting and display options set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522e0e5",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6183609",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Helper Function ---\n",
    "\n",
    "\n",
    "def load_park_specific_data(\n",
    "    file_path: str,\n",
    "    timestamp_col: str,\n",
    "    park_name_col: str,\n",
    "    required_data_cols: list[str],\n",
    "    target_park_name: str,\n",
    "    data_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads, validates, and filters data for a single specified park from a long-format CSV.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Absolute path to the CSV file.\n",
    "        timestamp_col (str): Name of the timestamp column.\n",
    "        park_name_col (str): Name of the park identifier column.\n",
    "        required_data_cols (list): List of required data column names.\n",
    "        target_park_name (str): The specific park to extract data for.\n",
    "        data_name (str): A descriptive name for the data (e.g., \"Production\").\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing only the data for the target park,\n",
    "                          with the park_name column removed. Returns an empty\n",
    "                          DataFrame on failure.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading {data_name} Data for '{target_park_name}' ---\")\n",
    "    print(f\"Attempting to load from: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load the full CSV\n",
    "        df = pd.read_csv(\n",
    "            file_path, parse_dates=[timestamp_col], index_col=timestamp_col\n",
    "        )\n",
    "\n",
    "        # 2. Basic Column Check\n",
    "        all_required_cols = required_data_cols + [park_name_col]\n",
    "        if not all(col in df.columns for col in all_required_cols):\n",
    "            missing = [col for col in all_required_cols if col not in df.columns]\n",
    "            raise ValueError(f\"Missing required columns in {data_name} CSV: {missing}\")\n",
    "\n",
    "        # 3. Data Cleaning and Validation\n",
    "        df.index = pd.to_datetime(df.index, utc=True)\n",
    "        df = df.dropna(subset=[park_name_col])\n",
    "        df[park_name_col] = df[park_name_col].astype(str)\n",
    "\n",
    "        # 4. Check if Target Park Exists in Data\n",
    "        if target_park_name not in df[park_name_col].unique():\n",
    "            raise ValueError(\n",
    "                f\"Target park '{target_park_name}' not found in the {data_name} file.\"\n",
    "            )\n",
    "\n",
    "        # 5. Filter for Target Park and Finalize\n",
    "        df_park = df[df[park_name_col] == target_park_name].copy()\n",
    "\n",
    "        # Drop the now-redundant park name column\n",
    "        df_park = df_park.drop(columns=[park_name_col])\n",
    "\n",
    "        df_park = df_park.sort_index()\n",
    "        print(f\"✅ {data_name} data for '{target_park_name}' loaded successfully.\")\n",
    "        print(f\"   Shape of final DataFrame: {df_park.shape}\")\n",
    "        print(f\"   Time range: {df_park.index.min()} to {df_park.index.max()}\")\n",
    "        print(\"Sample:\")\n",
    "        print(df_park.sample(n=5))\n",
    "        return df_park\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ DATA ERROR: The {data_name} file was not found at: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ AN UNEXPECTED ERROR OCCURRED during {data_name} data loading: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"✅ Helper function load_park_specific_data defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084d559",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.1 Hourly Production And Spot Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a38d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Production and Price Data ---\n",
    "\n",
    "# Define required column names for production data\n",
    "COL_TIMESTAMP = \"timestamp_utc\"\n",
    "COL_PARK_NAME = \"park_name\"\n",
    "PRODUCTION_DATA_COLS = [\"ac_export_kwh\", \"spot_price_eur_mwh\"]\n",
    "\n",
    "# Load the data for the target park using the helper function\n",
    "df_production = load_park_specific_data(\n",
    "    file_path=PRODUCTION_AND_PRICE_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=PRODUCTION_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Production & Price\",\n",
    ")\n",
    "assert isinstance(df_production.index, pd.DatetimeIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45339e8",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.2 Load Hourly Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Crop Weather Data ---\n",
    "\n",
    "# Define required column names for weather data\n",
    "WEATHER_DATA_COLS = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\", \"ghi_w_m2\"]\n",
    "\n",
    "# Load the weather data for the target park using the helper function\n",
    "df_weather = load_park_specific_data(\n",
    "    file_path=WEATHER_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=WEATHER_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Weather\",\n",
    ")\n",
    "assert isinstance(df_weather.index, pd.DatetimeIndex)\n",
    "\n",
    "# Post-processing: Crop the weather data to the production time range\n",
    "if not df_production.empty and not df_weather.empty:\n",
    "    start_time = df_production.index.min()\n",
    "    end_time = df_production.index.max()\n",
    "\n",
    "    original_rows = len(df_weather)\n",
    "    df_weather = df_weather.loc[start_time:end_time].copy()\n",
    "\n",
    "    print(f\"\\nWeather data cropped to production time range.\")\n",
    "    print(f\"   Original rows: {original_rows}, Cropped rows: {len(df_weather)}\")\n",
    "    print(f\"   New time range: {df_weather.index.min()} to {df_weather.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdd204",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 3. Data Upsampling and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f165ee8",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interpolation Helper Function ---\n",
    "\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def interpolate_by_gap_size(\n",
    "    data: pd.Series | pd.DataFrame,\n",
    "    max_gap_size: int = 1,\n",
    "    method: Literal[\n",
    "        \"linear\",\n",
    "        \"time\",\n",
    "        \"index\",\n",
    "        \"values\",\n",
    "        \"pad\",\n",
    "        \"nearest\",\n",
    "        \"zero\",\n",
    "        \"slinear\",\n",
    "        \"quadratic\",\n",
    "        \"cubic\",\n",
    "        \"barycentric\",\n",
    "        \"polynomial\",\n",
    "        \"spline\",\n",
    "        \"krogh\",\n",
    "        \"piecewise_polynomial\",\n",
    "        \"pchip\",\n",
    "        \"akima\",\n",
    "        \"cubicspline\",\n",
    "        \"from_derivatives\",\n",
    "    ] = \"linear\",\n",
    "    **kwargs\n",
    ") -> pd.Series | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpolates NaN values in a Series or DataFrame, but only for gaps\n",
    "    that are less than or equal to a specified maximum size.\n",
    "\n",
    "    For a DataFrame, the operation is applied column-wise.\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series | pd.DataFrame): Input data with potential NaNs.\n",
    "        max_gap_size (int): Max consecutive NaNs to interpolate. Gaps larger\n",
    "                            than this are ignored. Defaults to 1.\n",
    "        method (str): Interpolation technique (see pandas.Series.interpolate).\n",
    "                      Defaults to \"linear\".\n",
    "        **kwargs: Additional keyword arguments for the interpolate() method.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series | pd.DataFrame: New object with specified gaps filled.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, (pd.Series, pd.DataFrame)):\n",
    "        raise TypeError(\"Input `data` must be a pandas Series or DataFrame.\")\n",
    "    if not isinstance(max_gap_size, int) or max_gap_size <= 0:\n",
    "        raise ValueError(\"`max_gap_size` must be a positive integer.\")\n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        if data.empty or data.isna().sum() == 0:\n",
    "            return data.copy()\n",
    "\n",
    "        # Identify gaps by creating groups based on non-NaN values\n",
    "        grouper = data.notna().cumsum()\n",
    "        # Calculate the size of each NaN gap\n",
    "        gap_sizes = data.isna().groupby(grouper).transform(\"sum\")\n",
    "        # Create a boolean mask for NaNs that are part of small-enough gaps\n",
    "        mask_to_fill = data.isna() & (gap_sizes <= max_gap_size)\n",
    "\n",
    "        # Interpolate the entire series to get the fill values\n",
    "        fully_interpolated = data.interpolate(method=method, **kwargs) # type: ignore\n",
    "\n",
    "        # Apply the fill values only where the mask is True\n",
    "        return data.where(~mask_to_fill, fully_interpolated)\n",
    "\n",
    "    # If it's a DataFrame, apply this function to each column\n",
    "    return data.apply(\n",
    "        lambda col: interpolate_by_gap_size(\n",
    "            col, max_gap_size=max_gap_size, method=method, **kwargs\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"✅ Helper function interpolate_by_gap_size defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a52d2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.1 Upsample Production Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b71531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Production and Price Data to 30-Minute Frequency ---\n",
    "\n",
    "print(f\"--- Upsampling data for park: '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# 1. Determine the actual production period to avoid creating a massive index\n",
    "first_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.min()\n",
    "last_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.max()\n",
    "\n",
    "if pd.isna(first_prod_time) or pd.isna(last_prod_time):\n",
    "    print(\"⚠️ No valid production data (> 0 kWh) found. Cannot proceed.\")\n",
    "    df_production_30min = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"   - Production data range: {first_prod_time} to {last_prod_time}\")\n",
    "\n",
    "    # Crop the hourly data to the actual production period\n",
    "    df_prod_cropped = df_production.loc[first_prod_time:last_prod_time]\n",
    "\n",
    "    # Create a full 30-minute time range for this period\n",
    "    full_30min_range = pd.date_range(\n",
    "        start=first_prod_time, end=last_prod_time, freq=\"30min\", tz=\"UTC\"\n",
    "    )\n",
    "\n",
    "    # 2. Upsample spot price using forward-fill\n",
    "    # The price at HH:30 is the same as the price at HH:00\n",
    "    price_30min = (\n",
    "        df_prod_cropped[\"spot_price_eur_mwh\"]\n",
    "        .reindex(full_30min_range)\n",
    "        .ffill()\n",
    "        .to_frame()\n",
    "    )\n",
    "\n",
    "    # 3. Upsample production data from hourly kWh to 30-min average Power (W)\n",
    "    # Convert kWh (energy) to W (power) for the hourly interval\n",
    "    power_w_hourly = df_prod_cropped[\"ac_export_kwh\"] * 1000\n",
    "\n",
    "    # Shift the index 30 mins forward. The power calculated from the interval\n",
    "    # [HH:00, HH+1:00] is best represented at the midpoint, HH:30.\n",
    "    power_w_hourly.index = power_w_hourly.index + pd.Timedelta(minutes=30)\n",
    "\n",
    "    # Reindex to the full 30-min range. This introduces NaNs at HH:00.\n",
    "    # Use interpolate_by_gap_size(max_gap_size=1) to linearly fill only the\n",
    "    # points at HH:00, which are surrounded by two HH:30 points.\n",
    "    power_w_30min = interpolate_by_gap_size(\n",
    "        power_w_hourly.reindex(full_30min_range), max_gap_size=1, method=\"linear\"\n",
    "    ).to_frame(\"ac_export_w\") # type: ignore\n",
    "\n",
    "    # 4. Combine the upsampled series into a single DataFrame\n",
    "    df_production_30min = price_30min.join(power_w_30min)\n",
    "\n",
    "    print(\"\\n✅ Production and price data upsampled to 30-minute frequency.\")\n",
    "    print(f\"   - Shape of new DataFrame: {df_production_30min.shape}\")\n",
    "    print(\n",
    "        f\"   - New time range: {df_production_30min.index.min()} to {df_production_30min.index.max()}\"\n",
    "    )\n",
    "    print(\"   - Sample data:\")\n",
    "    print(df_production_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e916ff",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.2 Upsample Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Weather Data to 30-Minute Frequency ---\n",
    "\n",
    "print(\"--- Upsampling weather data ---\")\n",
    "\n",
    "# 1. Use the index from the upsampled production data for perfect alignment\n",
    "target_30min_range = df_production_30min.index\n",
    "print(\n",
    "    f\"   - Aligning to target time range: {target_30min_range.min()} to {target_30min_range.max()}\"\n",
    ")\n",
    "\n",
    "# 2. Handle standard weather variables with linear interpolation\n",
    "standard_weather_cols = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\"]\n",
    "df_weather_standard_30min = interpolate_by_gap_size(\n",
    "    df_weather[standard_weather_cols].reindex(target_30min_range),\n",
    "    max_gap_size=1,\n",
    "    method=\"linear\",\n",
    ")\n",
    "\n",
    "# 3. Handle GHI with a time shift before interpolation\n",
    "# The hourly GHI value at HH:00 represents the average over the interval [HH-1:00, HH:00].\n",
    "# We shift the timestamp back 30 mins to place this value at the interval's midpoint (HH-1:30).\n",
    "ghi_hourly = df_weather[[\"ghi_w_m2\"]].copy()\n",
    "assert isinstance(ghi_hourly.index, pd.DatetimeIndex)\n",
    "ghi_hourly.index = ghi_hourly.index - pd.Timedelta(minutes=30)\n",
    "\n",
    "# Reindex and interpolate. This will correctly fill the values at the top of the hour (HH:00).\n",
    "df_ghi_30min = interpolate_by_gap_size(\n",
    "    ghi_hourly.reindex(target_30min_range), max_gap_size=1, method=\"linear\"\n",
    ")\n",
    "\n",
    "# 4. Combine all upsampled weather series\n",
    "df_weather_30min = df_weather_standard_30min.join(df_ghi_30min)\n",
    "\n",
    "print(\"\\n✅ Weather data upsampled to 30-minute frequency.\")\n",
    "print(f\"   - Shape of new DataFrame: {df_weather_30min.shape}\")\n",
    "print(\"   - Sample data:\")\n",
    "print(df_weather_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856009b2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.3 Model PVLIB Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fe609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Solar Geometry and Clear-Sky Irradiance with PVLIB ---\n",
    "\n",
    "import pvlib\n",
    "\n",
    "print(f\"--- Modeling PVLIB features for '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# 1. Extract location parameters from the main config\n",
    "park_params = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "location = pvlib.location.Location(\n",
    "    latitude=park_params[\"location\"][\"latitude\"],\n",
    "    longitude=park_params[\"location\"][\"longitude\"],\n",
    "    tz=\"UTC\",\n",
    ")\n",
    "\n",
    "# 2. Prepare inputs for pvlib calculations\n",
    "times = df_weather_30min.index\n",
    "pressure_pa = df_weather_30min[\"pressure_hpa\"] * 100  # Convert hPa to Pa\n",
    "temperature_c = df_weather_30min[\"temp_air_c\"]\n",
    "\n",
    "# 3. Perform pvlib calculations\n",
    "print(\"   - Calculating solar position...\")\n",
    "solar_position = location.get_solarposition(\n",
    "    times, pressure=pressure_pa, temperature=temperature_c  # type: ignore\n",
    ")\n",
    "assert isinstance(solar_position, pd.DataFrame)\n",
    "\n",
    "# Fill any NaN in apparent_zenith by falling back to the geometric zenith.\n",
    "# This occurs when atmospheric data (pressure, temp) is missing, which is needed\n",
    "# for refraction correction. This fallback prevents NaN propagation.\n",
    "solar_position[\"apparent_zenith\"] = solar_position[\"apparent_zenith\"].fillna(\n",
    "    solar_position[\"zenith\"]\n",
    ")\n",
    "\n",
    "print(\"   - Calculating airmass...\")\n",
    "airmass_relative = pvlib.atmosphere.get_relative_airmass(\n",
    "    zenith=solar_position[\"apparent_zenith\"]\n",
    ")\n",
    "assert isinstance(airmass_relative, pd.Series)\n",
    "airmass_absolute = pvlib.atmosphere.get_absolute_airmass(\n",
    "    airmass_relative=airmass_relative, pressure=pressure_pa  # type: ignore\n",
    ")\n",
    "assert isinstance(airmass_absolute, pd.Series)\n",
    "\n",
    "# For timestamps with missing pressure data, airmass_absolute will be NaN.\n",
    "# In these cases, fall back to using airmass_relative. This is equivalent to\n",
    "# assuming standard sea-level pressure, a reasonable approximation to avoid\n",
    "# data loss in the clear-sky model, especially over long data gaps.\n",
    "airmass_absolute = airmass_absolute.fillna(airmass_relative)\n",
    "\n",
    "print(\"   - Calculating extraterrestrial radiation...\")\n",
    "dni_extra = pvlib.irradiance.get_extra_radiation(times)\n",
    "\n",
    "print(\"   - Calculating clear-sky irradiance (Ineichen model)...\")\n",
    "clearsky_irrad = location.get_clearsky(\n",
    "    times,\n",
    "    model=\"ineichen\",\n",
    "    solar_position=solar_position,\n",
    "    airmass_absolute=airmass_absolute,\n",
    "    dni_extra=dni_extra,\n",
    ")\n",
    "assert isinstance(clearsky_irrad, pd.DataFrame)\n",
    "clearsky_irrad = clearsky_irrad.rename(\n",
    "    columns={\n",
    "        \"ghi\": \"ghi_clearsky_w_m2\",\n",
    "        \"dni\": \"dni_clearsky_w_m2\",\n",
    "        \"dhi\": \"dhi_clearsky_w_m2\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"   - Adding 'is_day' flag...\")\n",
    "is_day = solar_position[\"apparent_zenith\"] < 90.0\n",
    "\n",
    "# 4. Assemble the final pvlib DataFrame\n",
    "df_pvlib_30min = pd.concat(\n",
    "    [\n",
    "        solar_position[[\"apparent_zenith\", \"zenith\", \"azimuth\"]],\n",
    "        pd.Series(airmass_relative, name=\"airmass_relative\"),\n",
    "        pd.Series(airmass_absolute, name=\"airmass_absolute\"),\n",
    "        pd.Series(dni_extra, name=\"dni_extra_w_m2\"),\n",
    "        clearsky_irrad,\n",
    "        pd.Series(is_day, name=\"is_day\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(\"\\n✅ PVLIB features modeled successfully.\")\n",
    "print(f\"   - Shape of new DataFrame: {df_pvlib_30min.shape}\")\n",
    "print(\"   - Sample data:\")\n",
    "print(df_pvlib_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4a5aa",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.4 Merge DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge All 30-Minute DataFrames ---\n",
    "\n",
    "print(\"--- Merging all 30-minute data sources ---\")\n",
    "\n",
    "# Join the three dataframes on their common DatetimeIndex\n",
    "# This preserves all rows and fills with NaNs where data is missing in one source\n",
    "df_30min = df_production_30min.join([df_weather_30min, df_pvlib_30min])\n",
    "\n",
    "print(\"\\n✅ All data sources successfully merged into a single DataFrame.\")\n",
    "print(f\"   - Final shape: {df_30min.shape}\")\n",
    "print(f\"   - Final time range: {df_30min.index.min()} to {df_30min.index.max()}\")\n",
    "print(\"   - Sample of merged data:\")\n",
    "print(df_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d3e84",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 4. Anomaly Flagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d961c",
   "metadata": {},
   "source": [
    "### 4.1 Flag Clipping and Curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Flag Clipping and Curtailment Events ---\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "print(f\"--- Flagging anomalies for '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Fetch park-specific parameters from the config\n",
    "park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "export_limit_w = park_config[\"export_limit_kw\"] * 1000\n",
    "pdc0_w = park_config[\"system\"][\"pdc0\"]\n",
    "\n",
    "# Define thresholds for flagging logic\n",
    "# Flag as clipped if power is within this percentage of the export limit\n",
    "CLIPPING_TOLERANCE_PCT = 0.01\n",
    "# Flag as curtailed if power is below this percentage of DC capacity during negative prices\n",
    "CURTAILMENT_POWER_THRESHOLD_PCT = 0.01\n",
    "\n",
    "clipping_threshold_w = export_limit_w * (1 - CLIPPING_TOLERANCE_PCT)\n",
    "curtailment_threshold_w = pdc0_w * CURTAILMENT_POWER_THRESHOLD_PCT\n",
    "\n",
    "print(f\"   - Clipping threshold: >= {clipping_threshold_w:.2f} W\")\n",
    "print(\n",
    "    f\"   - Curtailment threshold: < {curtailment_threshold_w:.2f} W (during negative prices)\"\n",
    ")\n",
    "\n",
    "# Initialize flag columns to False\n",
    "df_30min[\"is_clipped\"] = False\n",
    "df_30min[\"is_curtailed\"] = False\n",
    "\n",
    "# --- Clipping Detection Logic ---\n",
    "print(\"\\n1. Applying Clipping Detection Logic...\")\n",
    "\n",
    "# Step 1: Directly flag HH:30 points at or near the export limit\n",
    "is_hh30 = df_30min.index.minute == 30\n",
    "direct_clip_mask = is_hh30 & (df_30min[\"ac_export_w\"] >= clipping_threshold_w)\n",
    "df_30min.loc[direct_clip_mask, \"is_clipped\"] = True\n",
    "print(f\"   - Found {direct_clip_mask.sum()} HH:30 points with direct clipping.\")\n",
    "\n",
    "# Step 2: Propagate flag to adjacent HH:30 points (Temporal Contamination)\n",
    "clipped_at_hh30 = df_30min[\"is_clipped\"][is_hh30]\n",
    "neighbor_clip_mask = clipped_at_hh30.shift(\n",
    "    1, fill_value=False\n",
    ") | clipped_at_hh30.shift(-1, fill_value=False)\n",
    "\n",
    "# --- FIX ---\n",
    "# The original code incorrectly used `neighbor_clip_mask.index`, which flagged all HH:30 points.\n",
    "# The corrected code uses boolean alignment to flag only the HH:30 points where the neighbor_clip_mask is True.\n",
    "df_30min.loc[is_hh30, \"is_clipped\"] |= neighbor_clip_mask\n",
    "print(\n",
    "    f\"   - Propagated clipping flag to {neighbor_clip_mask.sum()} neighboring HH:30 points.\"\n",
    ")\n",
    "\n",
    "# Step 3: Propagate flag to interpolated HH:00 points (Interpolation Contamination)\n",
    "is_hh00 = df_30min.index.minute == 0\n",
    "interpolated_clip_mask = df_30min[\"is_clipped\"].shift(\n",
    "    1, fill_value=False\n",
    ") | df_30min[\"is_clipped\"].shift(-1, fill_value=False)\n",
    "df_30min.loc[is_hh00, \"is_clipped\"] |= interpolated_clip_mask[is_hh00]\n",
    "print(f\"   - Propagated clipping flag to interpolated HH:00 points.\")\n",
    "\n",
    "# --- Curtailment Detection Logic ---\n",
    "print(\"\\n2. Applying Curtailment Detection Logic...\")\n",
    "\n",
    "# Step 1: Directly flag HH:30 points with negative prices and low production during the day\n",
    "# The 'is_day' check prevents flagging nighttime hours where production is naturally zero.\n",
    "direct_curtail_mask = (\n",
    "    is_hh30\n",
    "    & df_30min[\"is_day\"]\n",
    "    & (df_30min[\"spot_price_eur_mwh\"] < 0)\n",
    "    & (df_30min[\"ac_export_w\"] < curtailment_threshold_w)\n",
    ")\n",
    "df_30min.loc[direct_curtail_mask, \"is_curtailed\"] = True\n",
    "print(\n",
    "    f\"   - Found {direct_curtail_mask.sum()} HH:30 points with direct curtailment.\"\n",
    ")\n",
    "\n",
    "# Step 2: Propagate flag to interpolated HH:00 points\n",
    "interpolated_curtail_mask = df_30min[\"is_curtailed\"].shift(\n",
    "    1, fill_value=False\n",
    ") | df_30min[\"is_curtailed\"].shift(-1, fill_value=False)\n",
    "df_30min.loc[is_hh00, \"is_curtailed\"] |= interpolated_curtail_mask[is_hh00]\n",
    "print(f\"   - Propagated curtailment flag to interpolated HH:00 points.\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_clipped = df_30min[\"is_clipped\"].sum()\n",
    "total_curtailed = df_30min[\"is_curtailed\"].sum()\n",
    "print(\"\\n✅ Anomaly flagging complete.\")\n",
    "print(\n",
    "    f\"   - Total points flagged as clipped: {total_clipped} ({total_clipped / len(df_30min):.2%})\"\n",
    ")\n",
    "print(\n",
    "    f\"   - Total points flagged as curtailed: {total_curtailed} ({total_curtailed / len(df_30min):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aee645",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 4.2 Flag Inter-Row Shading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In cell \"4.2 Flag Inter-Row Shading\"\n",
    "\n",
    "### 4.2 Flag Inter-Row Shading\n",
    "\n",
    "# --- Calculate and Flag Inter-Row Shading for Fixed-Tilt Arrays ---\n",
    "# This step models the geometry of the solar park to identify periods where\n",
    "# one row of panels casts a shadow on the row behind it. The flagging logic\n",
    "# is conservative to account for the upsampling process:\n",
    "# 1. Interval Contamination: The power value at HH:30 represents the energy\n",
    "#    from the [HH:00, HH+1:00] interval. If shading occurs at either HH:00\n",
    "#    or HH:30, the entire interval is considered contaminated, and the\n",
    "#    HH:30 point is flagged.\n",
    "# 2. Interpolation Contamination: The power value at HH:00 is interpolated\n",
    "#    from its two HH:30 neighbors. If either neighbor is flagged, the\n",
    "#    interpolated point is also flagged.\n",
    "\n",
    "import pvlib\n",
    "\n",
    "print(\"Attempting to flag inter-row shading periods with contamination logic...\")\n",
    "\n",
    "# Retrieve the specific configuration for the target park\n",
    "park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "mount_config = park_config.get(\"mounting\")\n",
    "\n",
    "# Initialize flag column to False before applying logic\n",
    "df_30min[\"is_shaded\"] = False\n",
    "\n",
    "# Check if the necessary geometric parameters are defined in config.yml\n",
    "if mount_config and mount_config.get(\"gcr\"):\n",
    "    assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "    try:\n",
    "        gcr = mount_config[\"gcr\"]\n",
    "        if not (0 < gcr < 1):\n",
    "            raise ValueError(f\"GCR must be between 0 and 1, but got {gcr}\")\n",
    "\n",
    "        # --- Step 0: Calculate the raw, point-in-time shading fraction ---\n",
    "        shaded_fraction = pvlib.shading.shaded_fraction1d(\n",
    "            solar_zenith=df_30min[\"zenith\"],\n",
    "            solar_azimuth=df_30min[\"azimuth\"],\n",
    "            axis_azimuth=mount_config[\"axis_azimuth\"],\n",
    "            shaded_row_rotation=mount_config[\"surface_tilt\"],\n",
    "            collector_width=1.0,\n",
    "            pitch=1.0 / gcr,\n",
    "            axis_tilt=0,\n",
    "            surface_to_axis_offset=0,\n",
    "            cross_axis_slope=0,\n",
    "        )\n",
    "\n",
    "        # Create a temporary mask for when shading geometrically occurs during the day\n",
    "        point_in_time_shade_mask = (shaded_fraction > 0) & df_30min[\"is_day\"]\n",
    "\n",
    "        # --- Step 1: Apply Interval Contamination to HH:30 points ---\n",
    "        print(\"1. Applying Interval Contamination Logic...\")\n",
    "        is_hh30 = df_30min.index.minute == 30\n",
    "\n",
    "        # An HH:30 point is shaded if the geometric shading occurs at its own\n",
    "        # timestamp OR at the start of its interval (the preceding HH:00 point).\n",
    "        interval_shade_mask = point_in_time_shade_mask | point_in_time_shade_mask.shift(\n",
    "            1, fill_value=False\n",
    "        )\n",
    "\n",
    "        # Apply this logic only to the HH:30 points, which represent the hourly intervals.\n",
    "        direct_shade_mask = is_hh30 & interval_shade_mask\n",
    "        df_30min.loc[direct_shade_mask, \"is_shaded\"] = True\n",
    "        print(\n",
    "            f\"   - Found {direct_shade_mask.sum()} HH:30 points with direct or interval shading.\"\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Apply Interpolation Contamination to HH:00 points ---\n",
    "        print(\"2. Applying Interpolation Contamination Logic...\")\n",
    "        is_hh00 = df_30min.index.minute == 0\n",
    "\n",
    "        # An HH:00 point is shaded if either of its adjacent HH:30 neighbors is shaded.\n",
    "        interpolated_shade_mask = df_30min[\"is_shaded\"].shift(\n",
    "            1, fill_value=False\n",
    "        ) | df_30min[\"is_shaded\"].shift(-1, fill_value=False)\n",
    "        df_30min.loc[is_hh00, \"is_shaded\"] |= interpolated_shade_mask[is_hh00]\n",
    "        print(f\"   - Propagated shading flag to interpolated HH:00 points.\")\n",
    "\n",
    "    except (KeyError, ValueError) as e:\n",
    "        print(\n",
    "            f\"❌ SHADING ERROR: Could not calculate shading due to missing or invalid config. Details: {e}\"\n",
    "        )\n",
    "        # The 'is_shaded' column is already False, so no further action needed.\n",
    "else:\n",
    "    print(\n",
    "        \"ℹ️ NOTE: Mounting 'gcr' not found in config.yml. Skipping shading calculation.\"\n",
    "    )\n",
    "    # The 'is_shaded' column is already False, so no further action needed.\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_shaded = df_30min[\"is_shaded\"].sum()\n",
    "daytime_points = df_30min[\"is_day\"].sum()\n",
    "pct_shaded = (total_shaded / daytime_points) * 100 if daytime_points > 0 else 0\n",
    "print(\"\\n✅ Shading flagging complete.\")\n",
    "print(\n",
    "    f\"   - Total points flagged as shaded: {total_shaded} ({pct_shaded:.2f}% of daytime).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e78923",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 4.3 Visually Verify Flags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visually Verify Anomaly Flags with Interactive Plots ---\n",
    "\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def create_flag_verification_plot(\n",
    "    df_day: pd.DataFrame,\n",
    "    flag_col: str,\n",
    "    title: str,\n",
    "    flag_color: str,\n",
    "    normal_color: str = \"royalblue\",\n",
    ") -> go.Figure:\n",
    "    \"\"\"Creates an interactive bar plot to visualize a specific anomaly flag for a single day.\"\"\"\n",
    "\n",
    "    df_plot = df_day.copy()\n",
    "    df_plot[\"color\"] = df_plot[flag_col].map({True: flag_color, False: normal_color})\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_plot.index,\n",
    "            y=df_plot[\"ac_export_w\"],\n",
    "            marker_color=df_plot[\"color\"],\n",
    "            customdata=df_plot[flag_col],\n",
    "            hovertemplate=\"<b>Time</b>: %{x|%H:%M}<br><b>Power</b>: %{y:.0f} W<br><b>Flagged</b>: %{customdata}<extra></extra>\",\n",
    "            # Set period to 30 minutes (in milliseconds) and align bars to the start of the period\n",
    "            xperiod=30 * 60 * 1000,\n",
    "            xperiodalignment=\"middle\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        xaxis_title=\"Time [UTC]\",\n",
    "        yaxis_title=\"AC Export [W]\",\n",
    "        bargap=0.05,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "\n",
    "# --- Plot 1: Clipping Verification ---\n",
    "clipped_days = df_30min[df_30min[\"is_clipped\"]].index.normalize().unique()  # type: ignore\n",
    "\n",
    "if not clipped_days.empty:\n",
    "    random_clipped_day = random.choice(clipped_days)\n",
    "    df_plot_clip = df_30min[df_30min.index.date == random_clipped_day.date()]\n",
    "\n",
    "    fig_clip = create_flag_verification_plot(\n",
    "        df_day=df_plot_clip,\n",
    "        flag_col=\"is_clipped\",\n",
    "        title=f\"Clipping Verification for {random_clipped_day.strftime('%Y-%m-%d')}\",\n",
    "        flag_color=\"crimson\",\n",
    "    )\n",
    "    fig_clip.show()\n",
    "else:\n",
    "    print(\"ℹ️ No days with clipping were found to plot.\")\n",
    "\n",
    "# --- Plot 2: Curtailment Verification ---\n",
    "curtailed_days = df_30min[df_30min[\"is_curtailed\"]].index.normalize().unique()  # type: ignore\n",
    "\n",
    "if not curtailed_days.empty:\n",
    "    random_curtailed_day = random.choice(curtailed_days)\n",
    "    df_plot_curtail = df_30min[df_30min.index.date == random_curtailed_day.date()]\n",
    "\n",
    "    fig_curtail = create_flag_verification_plot(\n",
    "        df_day=df_plot_curtail,\n",
    "        flag_col=\"is_curtailed\",\n",
    "        title=f\"Curtailment Verification for {random_curtailed_day.strftime('%Y-%m-%d')}\",\n",
    "        flag_color=\"crimson\",\n",
    "    )\n",
    "    fig_curtail.show()\n",
    "else:\n",
    "    print(\"ℹ️ No days with curtailment were found to plot.\")\n",
    "\n",
    "# --- Plot 3: Shading Verification ---\n",
    "if \"is_shaded\" in df_30min.columns:\n",
    "    shaded_days = df_30min[df_30min[\"is_shaded\"]].index.normalize().unique()  # type: ignore\n",
    "\n",
    "    if not shaded_days.empty:\n",
    "        random_shaded_day = random.choice(shaded_days)\n",
    "        df_plot_shade = df_30min[df_30min.index.date == random_shaded_day.date()]\n",
    "\n",
    "        fig_shade = create_flag_verification_plot(\n",
    "            df_day=df_plot_shade,\n",
    "            flag_col=\"is_shaded\",\n",
    "            title=f\"Shading Verification for {random_shaded_day.strftime('%Y-%m-%d')}\",\n",
    "            flag_color=\"crimson\",\n",
    "        )\n",
    "        fig_shade.show()\n",
    "    else:\n",
    "        print(\"ℹ️ No days with inter-row shading were found to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6817726",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 5. AC Export -> Plane of Array (POA) Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c471d",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff33105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "# This section is now split into two core functions:\n",
    "# 1. `calculate_effective_pdc`: Determines the system's DC capacity at any given\n",
    "#    moment, accounting for initial degradation and user-defined adjustments for\n",
    "#    known issues or upgrades.\n",
    "# 2. `estimate_poa_and_temp_cell`: The iterative solver that, given an effective\n",
    "#    DC capacity, reverses the PV power chain to estimate POA irradiance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pvlib\n",
    "\n",
    "\n",
    "def calculate_effective_pdc(\n",
    "    pdc0: float,\n",
    "    commissioning_date: pd.Timestamp,\n",
    "    degradation_rate: float,\n",
    "    current_timestamp: pd.Timestamp,\n",
    "    adjustments: list[dict] | None = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the effective DC capacity at a given timestamp, accounting for\n",
    "    annual degradation and any specified capacity adjustments.\n",
    "\n",
    "    Args:\n",
    "        pdc0: Nameplate DC power at STC (W).\n",
    "        commissioning_date: The date the system began operation.\n",
    "        degradation_rate: Annual degradation rate (e.g., 0.005 for 0.5%).\n",
    "        current_timestamp: The timestamp for which to calculate the capacity.\n",
    "        adjustments: A pre-processed and sorted list of dictionaries, each\n",
    "                     defining a capacity adjustment period.\n",
    "\n",
    "    Returns:\n",
    "        The effective DC capacity in Watts, or np.nan if the period is excluded.\n",
    "    \"\"\"\n",
    "    # 1. Calculate base degradation\n",
    "    time_delta = current_timestamp - commissioning_date\n",
    "    years_passed = max(0, time_delta.total_seconds() / (365.25 * 24 * 3600))\n",
    "    pdc_degraded = pdc0 * (1 - degradation_rate) ** years_passed\n",
    "\n",
    "    if not adjustments:\n",
    "        return pdc_degraded\n",
    "\n",
    "    # 2. Find and apply the most recent applicable adjustment\n",
    "    # The 'adjustments' list is sorted by start_date descending.\n",
    "    for adj in adjustments:\n",
    "        if current_timestamp >= adj[\"start_date\"]:\n",
    "            # This is the most recent adjustment rule. Check if it's active.\n",
    "            if current_timestamp <= adj[\"end_date\"]:\n",
    "                adj_type = adj[\"adjustment_type\"]\n",
    "\n",
    "                if adj_type == \"exclude\":\n",
    "                    return np.nan  # Exclude this period from analysis\n",
    "\n",
    "                value = adj[\"value\"]\n",
    "                if adj_type == \"percentage\":\n",
    "                    pdc_degraded *= 1 + value\n",
    "                elif adj_type == \"absolute_w\":\n",
    "                    pdc_degraded += value\n",
    "            # Whether the rule was active or expired, it was the most recent one,\n",
    "            # so we can stop searching.\n",
    "            break\n",
    "\n",
    "    return max(0, pdc_degraded)\n",
    "\n",
    "\n",
    "def estimate_poa_and_temp_cell(\n",
    "    p_ac: float,\n",
    "    temp_air: float,\n",
    "    wind_speed: float,\n",
    "    pdc_effective: float,\n",
    "    gamma_pmp: float,\n",
    "    inverter_efficiency: float,\n",
    "    dc_derate_factor: float,\n",
    "    temp_model_params: dict[str, float],\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Iteratively estimates POA irradiance and cell temperature from AC power,\n",
    "    using a pre-calculated effective DC capacity.\n",
    "\n",
    "    Args:\n",
    "        p_ac: AC power output in Watts.\n",
    "        temp_air: Ambient air temperature in Celsius.\n",
    "        wind_speed: Wind speed in m/s.\n",
    "        pdc_effective: Effective DC power of the system at the given timestamp (W).\n",
    "        gamma_pmp: Power temperature coefficient (e.g., -0.004 for -0.4%/°C).\n",
    "        inverter_efficiency: Nominal inverter efficiency (e.g., 0.985).\n",
    "        dc_derate_factor: A factor to account for unmodeled DC losses (e.g., 0.97).\n",
    "        temp_model_params: Parameters for the SAPM cell temperature model.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - Estimated POA irradiance (W/m^2).\n",
    "        - Estimated cell temperature (°C).\n",
    "        - Final temperature difference upon convergence (°C).\n",
    "        - Number of iterations performed.\n",
    "    \"\"\"\n",
    "    if any(pd.isna(val) for val in [p_ac, temp_air, wind_speed, pdc_effective]):\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    if p_ac <= 0 or pdc_effective <= 0:\n",
    "        return 0.0, temp_air, 0.0, 0\n",
    "\n",
    "    # Constants and initial guess\n",
    "    TEMP_REF = 25.0\n",
    "    IRRAD_REF = 1000.0\n",
    "    MAX_ITER = 10\n",
    "    TOLERANCE = 0.1\n",
    "    temp_cell_guess = temp_air + 20.0\n",
    "    p_dc = p_ac / inverter_efficiency\n",
    "\n",
    "    irrad_estimate = np.nan\n",
    "    temp_cell_new = np.nan\n",
    "    temp_diff = np.nan\n",
    "\n",
    "    for i in range(1, MAX_ITER + 1):\n",
    "        temp_factor = 1 + gamma_pmp * (temp_cell_guess - TEMP_REF)\n",
    "        if temp_factor <= 0:\n",
    "            return 0.0, temp_air, np.nan, i\n",
    "\n",
    "        irrad_estimate = (\n",
    "            p_dc / (pdc_effective * dc_derate_factor * temp_factor)\n",
    "        ) * IRRAD_REF\n",
    "        irrad_estimate = max(0, irrad_estimate)\n",
    "\n",
    "        temp_cell_new = pvlib.temperature.sapm_cell(\n",
    "            poa_global=irrad_estimate,\n",
    "            temp_air=temp_air,\n",
    "            wind_speed=wind_speed,\n",
    "            **temp_model_params,\n",
    "        )\n",
    "        temp_diff = abs(temp_cell_new - temp_cell_guess)\n",
    "\n",
    "        if temp_diff < TOLERANCE:\n",
    "            return irrad_estimate, temp_cell_new, temp_diff, i\n",
    "\n",
    "        temp_cell_guess = temp_cell_new\n",
    "\n",
    "    return irrad_estimate, temp_cell_new, temp_diff, MAX_ITER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b4c65",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 5.1 Main Processing and Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing and Data Integration ---\n",
    "\n",
    "# --- Retrieve Park-Specific Configuration ---\n",
    "try:\n",
    "    park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "    system_config = park_config[\"system\"]\n",
    "    temp_model_config = park_config[\"temperature_model\"]\n",
    "\n",
    "    pdc0: float = system_config[\"pdc0\"]\n",
    "    gamma_pmp: float = system_config[\"gamma_pmp\"]\n",
    "    inverter_efficiency: float = system_config[\"inverter_efficiency\"]\n",
    "    dc_derate_factor: float = system_config.get(\"dc_derate_factor\", 1.0)\n",
    "    degradation_rate: float = system_config[\"degradation_rate\"]\n",
    "    commissioning_date_str: str = system_config[\"commissioning_date\"]\n",
    "    commissioning_ts: pd.Timestamp = pd.to_datetime(commissioning_date_str, utc=True)\n",
    "\n",
    "    # --- Load and Pre-process DC Capacity Adjustments ---\n",
    "    dc_adjustments_raw = park_config.get(\"dc_capacity_adjustments\", [])\n",
    "    dc_adjustments_processed = []\n",
    "    if dc_adjustments_raw:\n",
    "        print(\n",
    "            f\"\\n🔧 Found {len(dc_adjustments_raw)} DC capacity adjustment period(s). Pre-processing...\"\n",
    "        )\n",
    "        for adj in dc_adjustments_raw:\n",
    "            try:\n",
    "                adj_type = adj[\"adjustment_type\"]\n",
    "                processed_adj = {\n",
    "                    \"start_date\": pd.to_datetime(adj[\"start_date\"], utc=True),\n",
    "                    \"end_date\": (\n",
    "                        pd.to_datetime(adj.get(\"end_date\"), utc=True)\n",
    "                        if adj.get(\"end_date\")\n",
    "                        else pd.Timestamp.max.tz_localize(\"UTC\")\n",
    "                    ),\n",
    "                    \"adjustment_type\": adj_type,\n",
    "                    # Value is not required for the 'exclude' type\n",
    "                    \"value\": float(adj[\"value\"]) if adj_type != \"exclude\" else None,\n",
    "                }\n",
    "                dc_adjustments_processed.append(processed_adj)\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(\n",
    "                    f\"⚠️ WARNING: Skipping invalid DC capacity adjustment due to missing/invalid key: {adj}. Error: {e}\"\n",
    "                )\n",
    "\n",
    "        # Sort by start_date descending to easily find the latest applicable adjustment\n",
    "        dc_adjustments_processed.sort(key=lambda x: x[\"start_date\"], reverse=True)\n",
    "        print(\"✅ Adjustments processed.\")\n",
    "    else:\n",
    "        print(\"\\nℹ️ No DC capacity adjustments defined for this park.\")\n",
    "\n",
    "    print(f\"\\n⚙️ Using parameters for '{TARGET_PARK_NAME}':\")\n",
    "    print(f\"  - Commissioning Date: {commissioning_ts.date()}\")\n",
    "    print(f\"  - Nameplate DC Power (pdc0): {pdc0 / 1e3:,.1f} kW\")\n",
    "    print(f\"  - Annual Degradation: {degradation_rate:.1%}\")\n",
    "    print(f\"  - Temp. Coefficient (gamma_pmp): {gamma_pmp * 100:.3f} %/°C\")\n",
    "    print(f\"  - Inverter Efficiency: {inverter_efficiency:.1%}\")\n",
    "    print(f\"  - DC Derate Factor: {dc_derate_factor:.2%}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\n",
    "        f\"❌ CONFIGURATION ERROR: Missing key {e} for park '{TARGET_PARK_NAME}' in config.yml.\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# --- Define Temperature Model from Config ---\n",
    "try:\n",
    "    model_type: str = temp_model_config[\"model_type\"]\n",
    "    model_name: str = temp_model_config[\"model_name\"]\n",
    "    temp_model_parameters = pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS[model_type][\n",
    "        model_name\n",
    "    ]\n",
    "    print(f\"\\n🌡️ Using {model_type.upper()} temperature model: '{model_name}'\")\n",
    "except KeyError:\n",
    "    print(\n",
    "        f\"❌ CONFIGURATION ERROR: Invalid temperature model '{model_type}/{model_name}' specified in config.yml.\"  # type: ignore\n",
    "    )\n",
    "    print(\n",
    "        \"Please check available models in pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS.\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# --- Prepare DataFrame for Re-running ---\n",
    "# Drop columns from previous runs to avoid conflicts.\n",
    "cols_to_drop = [\n",
    "    \"pdc_effective_w\",\n",
    "    \"poa_estimated_w_m2\",\n",
    "    \"temp_cell_estimated_c\",\n",
    "    \"temp_convergence_diff\",\n",
    "    \"iterations\",\n",
    "]\n",
    "df_30min = df_30min.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# --- Run POA Estimation ---\n",
    "estimation_mask = (\n",
    "    (df_30min[\"ac_export_w\"] > 0)\n",
    "    & (~df_30min[\"is_clipped\"])\n",
    "    & (~df_30min[\"is_curtailed\"])\n",
    "    & (~df_30min[\"is_shaded\"])\n",
    "    & (df_30min[\"is_day\"])\n",
    ")\n",
    "df_analysis = df_30min.loc[estimation_mask].copy()\n",
    "\n",
    "print(f\"\\n🔬 Calculating effective DC capacity for {len(df_analysis):,} data points...\")\n",
    "\n",
    "# 1. Calculate effective DC capacity for each timestamp\n",
    "df_analysis[\"pdc_effective_w\"] = df_analysis.index.to_series().apply(\n",
    "    lambda ts: calculate_effective_pdc(\n",
    "        pdc0=pdc0,\n",
    "        commissioning_date=commissioning_ts,\n",
    "        degradation_rate=degradation_rate,\n",
    "        current_timestamp=ts,\n",
    "        adjustments=dc_adjustments_processed,\n",
    "    )\n",
    ")\n",
    "\n",
    "# FIXME: This is a temporary hacky solution. By setting ac_export_w to NaN for periods\n",
    "# where pdc_effective_w is NaN (i.e., excluded periods from config), we are avoiding\n",
    "# downstream issues, particularly in cell 7.1 (Golden DataFrame Construction).\n",
    "# A more robust solution should be implemented to handle these excluded periods\n",
    "# more gracefully throughout the entire notebook.\n",
    "pdc_is_nan_mask = df_analysis[\"pdc_effective_w\"].isna()\n",
    "if pdc_is_nan_mask.any():\n",
    "    print(\n",
    "        f\"ℹ️ Nullifying 'ac_export_w' for {pdc_is_nan_mask.sum()} entries due to excluded DC capacity periods.\"\n",
    "    )\n",
    "    # Update both the main DataFrame and the analysis slice to ensure consistency\n",
    "    df_30min.loc[df_analysis.index[pdc_is_nan_mask], \"ac_export_w\"] = float(\"nan\")\n",
    "    df_analysis.loc[pdc_is_nan_mask, \"ac_export_w\"] = float(\"nan\")\n",
    "\n",
    "print(\"✅ Effective DC capacity calculated.\")\n",
    "\n",
    "print(f\"🔬 Running POA estimation...\")\n",
    "# 2. Run the iterative estimation using the calculated effective DC capacity\n",
    "results = df_analysis.apply(\n",
    "    lambda row: estimate_poa_and_temp_cell(\n",
    "        p_ac=row[\"ac_export_w\"],\n",
    "        temp_air=row[\"temp_air_c\"],\n",
    "        wind_speed=row[\"wind_speed_m_s\"],\n",
    "        pdc_effective=row[\"pdc_effective_w\"],\n",
    "        gamma_pmp=gamma_pmp,\n",
    "        inverter_efficiency=inverter_efficiency,\n",
    "        dc_derate_factor=dc_derate_factor,\n",
    "        temp_model_params=temp_model_parameters,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "if not results.empty:\n",
    "    results_df = pd.DataFrame(\n",
    "        results.tolist(),\n",
    "        index=df_analysis.index,\n",
    "        columns=[\n",
    "            \"poa_estimated_w_m2\",\n",
    "            \"temp_cell_estimated_c\",\n",
    "            \"temp_convergence_diff\",\n",
    "            \"iterations\",\n",
    "        ],\n",
    "    )\n",
    "    # Join both the effective pdc and the estimation results back to the main df\n",
    "    df_30min = df_30min.join(df_analysis[[\"pdc_effective_w\"]])\n",
    "    df_30min = df_30min.join(results_df)\n",
    "\n",
    "print(\"✅ Estimation complete.\")\n",
    "\n",
    "# --- Final Data Cleanup ---\n",
    "print(\"\\n🧹 Cleaning up and filling non-estimated periods...\")\n",
    "night_mask = ~df_30min[\"is_day\"]\n",
    "df_30min.loc[night_mask, \"poa_estimated_w_m2\"] = 0.0\n",
    "df_30min.loc[night_mask, \"temp_cell_estimated_c\"] = df_30min.loc[\n",
    "    night_mask, \"temp_air_c\"\n",
    "]\n",
    "df_30min[\"iterations\"] = df_30min[\"iterations\"].fillna(0).astype(int)\n",
    "print(\n",
    "    \"✅ Cleanup complete. Clipped/curtailed daytime periods remain as NaN for irradiance and temperature.\"\n",
    ")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\n",
    "    \"\\n📊 Sample of dataframe with new estimated POA and effective DC capacity columns:\"\n",
    ")\n",
    "display(df_30min.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f091f1a",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 6. Modeling GHI from Estimated POA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897b751",
   "metadata": {},
   "source": [
    "### 6.1 GHI Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0340f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GHI Modeling ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pvlib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"--- Starting GHI Modeling from Estimated POA ---\")\n",
    "\n",
    "# --- 1. Preparation ---\n",
    "GHI_MODEL_COLS = [\n",
    "    \"ghi_modeled_w_m2\",\n",
    "    \"dni_modeled_w_m2\",\n",
    "    \"dhi_modeled_w_m2\",\n",
    "    \"kt_modeled\",\n",
    "    \"ghi_model_converged\",\n",
    "    \"ghi_model_iterations\",\n",
    "]\n",
    "IRRADIANCE_COLS = GHI_MODEL_COLS[:4]\n",
    "\n",
    "df_30min.drop(\n",
    "    columns=[col for col in GHI_MODEL_COLS if col in df_30min.columns],\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "\n",
    "df_30min[\"ghi_model_converged\"] = pd.Series(\n",
    "    pd.NA, index=df_30min.index, dtype=\"boolean\"\n",
    ")\n",
    "df_30min[\"ghi_model_iterations\"] = np.nan\n",
    "for col in IRRADIANCE_COLS:\n",
    "    df_30min[col] = np.nan\n",
    "\n",
    "location_params = PARK_CONFIGS[TARGET_PARK_NAME][\"location\"]\n",
    "mounting_params = PARK_CONFIGS[TARGET_PARK_NAME][\"mounting\"]\n",
    "\n",
    "# --- 2. GHI Reverse Transposition with Progress Bar ---\n",
    "print(\"Step 1: Calculating GHI for daytime points with valid POA...\")\n",
    "\n",
    "# A more robust mask: only process daytime points with a positive POA estimate.\n",
    "# The > 0 condition implicitly handles both NaNs and zeros.\n",
    "calc_mask = df_30min[\"is_day\"] & (df_30min[\"poa_estimated_w_m2\"] > 0)\n",
    "df_to_process = df_30min.loc[calc_mask]\n",
    "assert isinstance(df_to_process.index, pd.DatetimeIndex)\n",
    "\n",
    "print(f\"   - Found {len(df_to_process)} points to process.\")\n",
    "\n",
    "if not df_to_process.empty:\n",
    "    monthly_groups = df_to_process.groupby(\n",
    "        [df_to_process.index.year, df_to_process.index.month]\n",
    "    )\n",
    "    results_list = []\n",
    "\n",
    "    pbar = tqdm(monthly_groups, desc=\"Modeling GHI (monthly chunks)\")\n",
    "    for (year, month), group in pbar:\n",
    "        pbar.set_postfix_str(f\"{year}-{month:02d}\")\n",
    "\n",
    "        ghi_results_chunk = pvlib.irradiance.ghi_from_poa_driesse_2023(\n",
    "            surface_tilt=mounting_params[\"surface_tilt\"],\n",
    "            surface_azimuth=mounting_params[\"surface_azimuth\"],\n",
    "            solar_zenith=group[\"zenith\"],\n",
    "            solar_azimuth=group[\"azimuth\"],\n",
    "            poa_global=group[\"poa_estimated_w_m2\"],\n",
    "            dni_extra=group[\"dni_extra_w_m2\"],\n",
    "            albedo=location_params[\"albedo\"],\n",
    "            full_output=True,\n",
    "        )\n",
    "\n",
    "        results_list.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"ghi_modeled_w_m2\": ghi_results_chunk[0],\n",
    "                    \"ghi_model_converged\": ghi_results_chunk[1],\n",
    "                    \"ghi_model_iterations\": ghi_results_chunk[2],\n",
    "                },\n",
    "                index=group.index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if results_list:\n",
    "        all_results_df = pd.concat(results_list)\n",
    "        df_30min.update(all_results_df)\n",
    "\n",
    "# --- 3. GHI Decomposition (ERBS Model) ---\n",
    "# Decompose only where GHI was successfully modeled and converged.\n",
    "decomp_mask = df_30min[\"ghi_modeled_w_m2\"].notna() & (\n",
    "    df_30min[\"ghi_model_converged\"] == True\n",
    ")\n",
    "print(f\"Step 2: Decomposing GHI for {decomp_mask.sum()} valid points...\")\n",
    "\n",
    "if decomp_mask.any():\n",
    "    decomposed = pvlib.irradiance.erbs_driesse(\n",
    "        ghi=df_30min.loc[decomp_mask, \"ghi_modeled_w_m2\"],\n",
    "        zenith=df_30min.loc[decomp_mask, \"zenith\"],\n",
    "        datetime_or_doy=df_30min.loc[decomp_mask].index,\n",
    "    ).rename( # type: ignore\n",
    "        columns={\n",
    "            \"dni\": \"dni_modeled_w_m2\",\n",
    "            \"dhi\": \"dhi_modeled_w_m2\",\n",
    "            \"kt\": \"kt_modeled\",\n",
    "        }\n",
    "    )\n",
    "    df_30min.update(decomposed)\n",
    "\n",
    "# --- 4. Data Cleaning and Finalization ---\n",
    "print(\"Step 3: Cleaning and finalizing modeled irradiance data...\")\n",
    "\n",
    "failed_convergence_mask = df_30min[\"ghi_model_converged\"] == False\n",
    "df_30min.loc[failed_convergence_mask, IRRADIANCE_COLS] = np.nan\n",
    "print(\n",
    "    f\"   - Invalidated {failed_convergence_mask.sum()} points due to GHI model non-convergence.\"\n",
    ")\n",
    "\n",
    "unrealistic_dni_mask = df_30min[\"dni_modeled_w_m2\"] > (\n",
    "    df_30min[\"dni_clearsky_w_m2\"] * 1.05\n",
    ")\n",
    "df_30min.loc[unrealistic_dni_mask, IRRADIANCE_COLS] = np.nan\n",
    "print(\n",
    "    f\"   - Invalidated {unrealistic_dni_mask.sum()} points exceeding the clear-sky DNI limit.\"\n",
    ")\n",
    "\n",
    "night_mask = ~df_30min[\"is_day\"]\n",
    "df_30min.loc[night_mask, IRRADIANCE_COLS] = df_30min.loc[\n",
    "    night_mask, IRRADIANCE_COLS\n",
    "].fillna(0)\n",
    "df_30min[\"ghi_model_converged\"] = df_30min[\"ghi_model_converged\"].fillna(False)\n",
    "df_30min.loc[night_mask, \"ghi_model_iterations\"] = df_30min.loc[\n",
    "    night_mask, \"ghi_model_iterations\"\n",
    "].fillna(0)\n",
    "\n",
    "for col in IRRADIANCE_COLS:\n",
    "    if col in df_30min.columns:\n",
    "        df_30min[col] = df_30min[col].clip(lower=0)\n",
    "\n",
    "print(\"\\n✅ GHI Modeling and Decomposition complete.\")\n",
    "\n",
    "display_cols = [\n",
    "    \"poa_estimated_w_m2\",\n",
    "    \"ghi_modeled_w_m2\",\n",
    "    \"dni_modeled_w_m2\",\n",
    "    \"dhi_modeled_w_m2\",\n",
    "    \"kt_modeled\",\n",
    "    \"ghi_clearsky_w_m2\",\n",
    "    \"dni_clearsky_w_m2\",\n",
    "    \"ghi_model_converged\",\n",
    "    \"ghi_model_iterations\",\n",
    "]\n",
    "display(df_30min[display_cols].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e45220",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 6.2 Visual Verification of GHI Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual Verification of GHI Model ---\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "\n",
    "# --- User Configuration ---\n",
    "# Set to a tuple like (2023, 8) (Year, Month) to select a specific month.\n",
    "# Set to None to automatically select a random month from the available data.\n",
    "# selected_month: tuple[int, int] | None = None\n",
    "selected_month: tuple[int, int] | None = (2022, 6)\n",
    "# --------------------------\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "\n",
    "# 1. Determine the target month\n",
    "if selected_month is None:\n",
    "    # Get unique (year, month) pairs from the index\n",
    "    all_months = list(\n",
    "        df_30min.index.to_series().apply(lambda dt: (dt.year, dt.month)).unique()\n",
    "    )\n",
    "\n",
    "    if not all_months:\n",
    "        raise ValueError(\"Dataframe is empty or index is invalid.\")\n",
    "\n",
    "    year, month = random.choice(all_months)\n",
    "    print(f\"No month selected, randomly choosing: Year {year}, Month {month}\")\n",
    "else:\n",
    "    year, month = selected_month\n",
    "    if not 1 <= month <= 12:\n",
    "        raise ValueError(f\"Invalid month number: {month}. Must be between 1 and 12.\")\n",
    "\n",
    "    # Check if the year is in the available data range\n",
    "    if df_30min.index.empty:\n",
    "        raise ValueError(\"Dataframe is empty.\")\n",
    "    min_year, max_year = df_30min.index.year.min(), df_30min.index.year.max()\n",
    "    if not min_year <= year <= max_year:\n",
    "        raise ValueError(\n",
    "            f\"Invalid year: {year}. Data available for years {min_year}-{max_year}.\"\n",
    "        )\n",
    "    print(f\"Plotting user-selected month: Year {year}, Month {month}\")\n",
    "\n",
    "# 2. Create a cell-specific dataframe slice for the selected month\n",
    "# Using partial string indexing is a concise and effective way to slice a DatetimeIndex.\n",
    "month_str = f\"{year}-{month:02d}\"\n",
    "df_month_ghi_verify = df_30min.loc[month_str]\n",
    "\n",
    "if df_month_ghi_verify.empty:\n",
    "    raise ValueError(\n",
    "        f\"No data available for the selected month: {month_str}. \"\n",
    "        f\"Data range is from {df_30min.index.min()} to {df_30min.index.max()}\"\n",
    "    )\n",
    "\n",
    "# --- Plotting ---\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.1,\n",
    "    subplot_titles=(\"AC Power Export\", \"GHI: Modeled vs. Reference\"),\n",
    ")\n",
    "\n",
    "# Row 1: AC Export\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_month_ghi_verify.index,\n",
    "        y=df_month_ghi_verify[\"ac_export_w\"],\n",
    "        name=\"AC Export\",\n",
    "        mode=\"lines\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Row 2: GHI Comparison\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_month_ghi_verify.index,\n",
    "        y=df_month_ghi_verify[\"ghi_w_m2\"],\n",
    "        name=\"Reference GHI\",\n",
    "        mode=\"lines\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_month_ghi_verify.index,\n",
    "        y=df_month_ghi_verify[\"ghi_modeled_w_m2\"],\n",
    "        name=\"Modeled GHI\",\n",
    "        mode=\"lines\",\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"rgba(255, 255, 255, 0.15)\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# --- Layout and Final Touches ---\n",
    "fig.update_layout(\n",
    "    title_text=f\"GHI Model Verification: Month {month}, {year}\",\n",
    "    height=600,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Power (W)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Irradiance (W/m²)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Time (UTC)\", row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253f88f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 6.3 Diagnostic: Daily Energy and Snow Cover Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2187606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostic: Daily Energy and Snow Cover Analysis ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0. Configuration for Snow Flagging ---\n",
    "# Threshold for mean daily air temperature below which snow cover is considered possible.\n",
    "POTENTIAL_SNOW_TEMP_THRESHOLD_C = 1.5  # degrees Celsius\n",
    "\n",
    "# Threshold for the relative energy deficit. If modeled energy is this much lower than\n",
    "# reference energy (e.g., -70%), it's a strong indicator of an anomaly like snow.\n",
    "ENERGY_DEFICIT_THRESHOLD_PCT = -70.0  # percent\n",
    "\n",
    "print(\"--- Starting Daily Energy and Snow Cover Analysis ---\")\n",
    "\n",
    "# --- 1. Preparation and Synchronization ---\n",
    "# Select relevant columns for comparison and diagnostics.\n",
    "df_comp = df_30min[\n",
    "    [\n",
    "        \"ghi_w_m2\",\n",
    "        \"ghi_modeled_w_m2\",\n",
    "        \"temp_air_c\",\n",
    "        \"is_day\",\n",
    "        \"is_clipped\",\n",
    "        \"is_curtailed\",\n",
    "        \"is_shaded\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# Synchronize NaNs: ensure we only compare timestamps where both series are valid.\n",
    "# Also, nullify data points flagged for anomalies (clipping, curtailment, shading)\n",
    "# as they are not suitable for GHI comparison.\n",
    "anomaly_mask = (\n",
    "    df_comp[\"ghi_w_m2\"].isna()\n",
    "    | df_comp[\"ghi_modeled_w_m2\"].isna()\n",
    "    | df_comp[\"is_clipped\"]\n",
    "    | df_comp[\"is_curtailed\"]\n",
    "    | df_comp[\"is_shaded\"]\n",
    ")\n",
    "df_comp.loc[anomaly_mask, [\"ghi_w_m2\", \"ghi_modeled_w_m2\"]] = np.nan\n",
    "print(\n",
    "    \"Step 1: Synchronized NaNs and excluded anomalous data points for GHI comparison.\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2. Aggregate to Daily Values ---\n",
    "def daily_energy_wh(series: pd.Series) -> float:\n",
    "    \"\"\"Integrates a power series (W) to daily energy (Wh) using the trapezoidal rule.\"\"\"\n",
    "    series = series.dropna()\n",
    "    if len(series) < 2:\n",
    "        return np.nan\n",
    "    assert isinstance(series.index, pd.DatetimeIndex)\n",
    "    time_in_hours = series.index.hour + series.index.minute / 60.0\n",
    "    return np.trapezoid(y=series.values, x=time_in_hours.values)  # type: ignore\n",
    "\n",
    "\n",
    "print(\"Step 2: Aggregating 30-min data to daily summaries...\")\n",
    "# Resample GHI columns to get daily energy\n",
    "df_daily_analysis = (\n",
    "    df_comp[[\"ghi_w_m2\", \"ghi_modeled_w_m2\"]].resample(\"D\").apply(daily_energy_wh)  # type: ignore\n",
    ")\n",
    "df_daily_analysis.rename(\n",
    "    columns={\"ghi_w_m2\": \"ghi_ref_wh_m2\", \"ghi_modeled_w_m2\": \"ghi_modeled_wh_m2\"},\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Resample to get mean daily temperature\n",
    "df_daily_analysis[\"temp_air_c_mean\"] = df_comp[\"temp_air_c\"].resample(\"D\").mean()\n",
    "\n",
    "\n",
    "# --- 3. Add Daytime Point Count ---\n",
    "print(\"Step 3: Counting valid daytime data points per day...\")\n",
    "daytime_points = df_comp.loc[df_comp[\"is_day\"]]\n",
    "df_daily_analysis[\"daytime_point_count\"] = (\n",
    "    daytime_points[\"ghi_w_m2\"].resample(\"D\").count()\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Calculate Deltas ---\n",
    "print(\"Step 4: Calculating daily absolute and relative deltas...\")\n",
    "df_daily_analysis[\"delta_abs_wh_m2\"] = (\n",
    "    df_daily_analysis[\"ghi_modeled_wh_m2\"] - df_daily_analysis[\"ghi_ref_wh_m2\"]\n",
    ")\n",
    "df_daily_analysis[\"delta_rel_pct\"] = (\n",
    "    (df_daily_analysis[\"delta_abs_wh_m2\"] / df_daily_analysis[\"ghi_ref_wh_m2\"]) * 100\n",
    ").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "# --- 5. Flag Potential Snow Cover (Energy Deficit Method) ---\n",
    "print(\"Step 5: Flagging potential snow cover days...\")\n",
    "cond_temp = df_daily_analysis[\"temp_air_c_mean\"] <= POTENTIAL_SNOW_TEMP_THRESHOLD_C\n",
    "\n",
    "# Condition 1: Significant relative energy deficit.\n",
    "cond_deficit = df_daily_analysis[\"delta_rel_pct\"] <= ENERGY_DEFICIT_THRESHOLD_PCT\n",
    "\n",
    "# Condition 2 (Aggressive): Modeled energy is zero, which is a strong indicator of\n",
    "# complete panel obstruction (e.g., snow) on a cold day, regardless of reference GHI.\n",
    "cond_zero_model = df_daily_analysis[\"ghi_modeled_wh_m2\"] == 0\n",
    "\n",
    "# A day is flagged if the temperature is low AND (either there's a large energy deficit\n",
    "# OR the modeled energy is exactly zero).\n",
    "# The fillna(False) is crucial for cond_deficit, as NaN relative deltas (e.g., on\n",
    "# days with zero reference energy) should not satisfy the deficit condition on their own.\n",
    "df_daily_analysis[\"is_snow_covered_by_energy\"] = cond_temp & (\n",
    "    cond_deficit.fillna(False) | cond_zero_model\n",
    ")\n",
    "\n",
    "\n",
    "# --- 6. Finalize and Display Daily Analysis ---\n",
    "snow_days_count = df_daily_analysis[\"is_snow_covered_by_energy\"].sum()\n",
    "print(\n",
    "    f\"\\n✅ Daily analysis complete. Flagged {snow_days_count} potential snow cover days.\"\n",
    ")\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(\"\\nDescriptive Statistics for Daily Analysis:\")\n",
    "display(df_daily_analysis.describe())\n",
    "\n",
    "\n",
    "# --- 7. Integrate Snow Flags into Main DataFrame ---\n",
    "print(\"\\n--- Integrating Snow Flags into Main DataFrame ---\")\n",
    "\n",
    "# Get the dates flagged for snow cover\n",
    "snow_covered_dates = df_daily_analysis[\n",
    "    df_daily_analysis[\"is_snow_covered_by_energy\"]\n",
    "].index.normalize() # type: ignore\n",
    "\n",
    "# Create the flag column by checking if the timestamp's date is in the flagged dates\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "df_30min[\"is_snow_covered\"] = df_30min.index.normalize().isin(snow_covered_dates)\n",
    "\n",
    "print(\n",
    "    \"\\n✅ Snow cover analysis integrated. 'df_30min' has been updated with the new flag.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3b1ad",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 6.4 Visualization: Daily Energy and Snow Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization: Daily Energy and Snow Cover ---\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Define the date range for the plot.\n",
    "START_DATE_PLOT = \"2022-11-01\"\n",
    "END_DATE_PLOT = \"2023-02-28\"\n",
    "\n",
    "# Define a new color scheme for better visual distinction.\n",
    "COLOR_REF_NORMAL = \"skyblue\"\n",
    "COLOR_MODELED_NORMAL = \"steelblue\"\n",
    "COLOR_REF_SNOW = \"sandybrown\"\n",
    "COLOR_MODELED_SNOW = \"orangered\"\n",
    "# Colors for the temperature line based on the threshold\n",
    "COLOR_TEMP_NORMAL = \"lightslategray\"\n",
    "COLOR_TEMP_COLD = \"red\"\n",
    "\n",
    "print(f\"--- Generating Daily Energy Plot from {START_DATE_PLOT} to {END_DATE_PLOT} ---\")\n",
    "\n",
    "# --- 2. Data Preparation for Plotting ---\n",
    "# Filter the dataframe for the selected date range\n",
    "df_plot = df_daily_analysis.loc[START_DATE_PLOT:END_DATE_PLOT].copy()\n",
    "\n",
    "# Split into normal and snow-flagged days for bar colors\n",
    "df_plot_normal = df_plot[~df_plot[\"is_snow_covered_by_energy\"]]\n",
    "df_plot_snow = df_plot[df_plot[\"is_snow_covered_by_energy\"]]\n",
    "\n",
    "# Create two separate series for temperature based on the threshold for conditional coloring.\n",
    "# Values that don't meet the condition become NaN, which Plotly uses to create breaks in the line.\n",
    "df_plot[\"temp_above_thresh\"] = df_plot[\"temp_air_c_mean\"].where(\n",
    "    df_plot[\"temp_air_c_mean\"] > POTENTIAL_SNOW_TEMP_THRESHOLD_C\n",
    ")\n",
    "df_plot[\"temp_below_thresh\"] = df_plot[\"temp_air_c_mean\"].where(\n",
    "    df_plot[\"temp_air_c_mean\"] <= POTENTIAL_SNOW_TEMP_THRESHOLD_C\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. Create the Figure with a Secondary Y-Axis ---\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "\n",
    "# --- 4. Add Bar Traces for GHI Energy ---\n",
    "# Offset groups ensure bars are properly aligned side-by-side.\n",
    "\n",
    "# Offset Group 0: All \"Reference GHI\" bars\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_plot_normal.index,\n",
    "        y=df_plot_normal[\"ghi_ref_wh_m2\"],\n",
    "        name=\"Reference GHI (Normal)\",\n",
    "        marker_color=COLOR_REF_NORMAL,\n",
    "        offsetgroup=0,\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_plot_snow.index,\n",
    "        y=df_plot_snow[\"ghi_ref_wh_m2\"],\n",
    "        name=\"Reference GHI (Snow Flag)\",\n",
    "        marker_color=COLOR_REF_SNOW,\n",
    "        offsetgroup=0,\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Offset Group 1: All \"Modeled GHI\" bars\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_plot_normal.index,\n",
    "        y=df_plot_normal[\"ghi_modeled_wh_m2\"],\n",
    "        name=\"Modeled GHI (Normal)\",\n",
    "        marker_color=COLOR_MODELED_NORMAL,\n",
    "        offsetgroup=1,\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_plot_snow.index,\n",
    "        y=df_plot_snow[\"ghi_modeled_wh_m2\"],\n",
    "        name=\"Modeled GHI (Snow Flag)\",\n",
    "        marker_color=COLOR_MODELED_SNOW,\n",
    "        offsetgroup=1,\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# --- 5. Add Conditionally Colored Line Trace for Temperature ---\n",
    "# Plot two separate scatter traces for the temperature. Plotly will not connect\n",
    "# the gaps, effectively creating a single line that changes color.\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_plot.index,\n",
    "        y=df_plot[\"temp_above_thresh\"],\n",
    "        name=\"Mean Air Temp (°C)\",\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(color=COLOR_TEMP_NORMAL, dash=\"dash\"),\n",
    "        marker=dict(size=4),\n",
    "        legendgroup=\"temp\",  # Group both traces under one legend item\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_plot.index,\n",
    "        y=df_plot[\"temp_below_thresh\"],\n",
    "        name=\"Mean Air Temp (°C)\",\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(color=COLOR_TEMP_COLD, dash=\"dash\"),\n",
    "        marker=dict(size=4),\n",
    "        legendgroup=\"temp\",\n",
    "        showlegend=False,  # Hide the second trace from the legend\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "\n",
    "# --- 6. Update Layout and Finalize ---\n",
    "subtitle = f\"Temperature line colored red when at/below the {POTENTIAL_SNOW_TEMP_THRESHOLD_C}°C threshold for potential snow\"\n",
    "fig.update_layout(\n",
    "    barmode=\"group\",\n",
    "    title_text=f\"Daily GHI & Air Temperature ({START_DATE_PLOT} to {END_DATE_PLOT})<br><sup>{subtitle}</sup>\",\n",
    "    xaxis_title=\"Date\",\n",
    "    height=600,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    hovermode=\"x\",\n",
    ")\n",
    "\n",
    "# Configure y-axes titles\n",
    "fig.update_yaxes(title_text=\"Daily Energy (Wh/m²)\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Mean Air Temperature (°C)\", secondary_y=True)\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea3f80",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 7. Advanced GHI Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7d7cc",
   "metadata": {},
   "source": [
    "### 7.1 Golden DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Golden DataFrame Construction ---\n",
    "\n",
    "# --- Imports and Constants ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- Configuration for Golden Day Selection ---\n",
    "\n",
    "# Defines the percentage of daytime data to trim from the edges (morning/evening)\n",
    "# before fitting the parabola. This helps to focus the fit on the peak portion\n",
    "# of the day, where the parabolic approximation is more accurate.\n",
    "# e.g., 0.20 means trimming 10% from the start and 10% from the end of the day.\n",
    "DAY_EDGE_TRIM_PERCENTAGE = 0.40\n",
    "\n",
    "# Defines the tolerance for how far the parabola's vertex can be from solar noon.\n",
    "# A value of 60 minutes is a reasonable starting point.\n",
    "VERTEX_NOON_TOLERANCE_MINUTES = 60\n",
    "\n",
    "# Defines the quantile for R² scores to be considered \"golden\".\n",
    "R2_QUANTILE_THRESHOLD = 0.90\n",
    "\n",
    "# Defines the minimum number of valid data points required within a day\n",
    "# to attempt a parabolic fit. This prevents fitting on sparse data.\n",
    "# A value of 6 corresponds to at least 3 hours of valid data.\n",
    "MIN_DATAPOINTS_FOR_FIT = 6\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "def get_parabolic_fit_details(series: pd.Series) -> dict[str, float | int | None]:\n",
    "    \"\"\"\n",
    "    Fits a parabola to a given time series and returns the fit details.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series with a DatetimeIndex and numeric values.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the R-squared value, polynomial coefficients (a, b, c),\n",
    "        the vertex time in seconds from the start, and the number of data points used.\n",
    "        Returns None for values if fitting is not possible.\n",
    "    \"\"\"\n",
    "    series_to_fit = series.dropna()\n",
    "    data_points = len(series_to_fit)\n",
    "\n",
    "    # A parabola requires at least 3 points to be uniquely defined.\n",
    "    if data_points < 3:\n",
    "        return {\n",
    "            \"r2\": None,\n",
    "            \"a\": None,\n",
    "            \"b\": None,\n",
    "            \"c\": None,\n",
    "            \"vertex_x_seconds\": None,\n",
    "            \"data_points\": data_points,\n",
    "        }\n",
    "\n",
    "    time_seconds = (series_to_fit.index - series_to_fit.index.min()).total_seconds()\n",
    "    values = series_to_fit.to_numpy()\n",
    "\n",
    "    try:\n",
    "        # Fit a 2nd degree polynomial (parabola)\n",
    "        coeffs = np.polyfit(time_seconds, values, 2)\n",
    "        a, b, c = coeffs\n",
    "\n",
    "        values_predicted = np.polyval(coeffs, time_seconds)\n",
    "        r2 = r2_score(values, values_predicted)\n",
    "\n",
    "        # Vertex formula for y = ax^2 + bx + c is x = -b / 2a\n",
    "        # If 'a' is extremely close to zero, the fit is essentially linear,\n",
    "        # and the vertex is at a nonsensical, far-off time. We treat this as an invalid vertex.\n",
    "        if abs(a) < 1e-9:\n",
    "            vertex_x_seconds = np.nan\n",
    "        else:\n",
    "            vertex_x_seconds = -b / (2 * a)\n",
    "\n",
    "    except (np.linalg.LinAlgError, ValueError):\n",
    "        # Handle cases where fitting fails\n",
    "        return {\n",
    "            \"r2\": None,\n",
    "            \"a\": None,\n",
    "            \"b\": None,\n",
    "            \"c\": None,\n",
    "            \"vertex_x_seconds\": None,\n",
    "            \"data_points\": data_points,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"r2\": r2,\n",
    "        \"a\": a,\n",
    "        \"b\": b,\n",
    "        \"c\": c,\n",
    "        \"vertex_x_seconds\": vertex_x_seconds,\n",
    "        \"data_points\": data_points,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Main Analysis ---\n",
    "\n",
    "# --- Step 1: Prepare and Pre-filter the Analysis DataFrame ---\n",
    "print(\"Step 1: Preparing and pre-filtering the main analysis DataFrame...\")\n",
    "df_30min_analysis = df_30min.copy()\n",
    "assert isinstance(df_30min_analysis.index, pd.DatetimeIndex)\n",
    "df_30min_analysis[\"date_only\"] = df_30min_analysis.index.normalize()\n",
    "\n",
    "# --- Step 1a: Flag daytime edges for trimming ---\n",
    "print(\"   - Flagging daytime edges to improve parabolic fit...\")\n",
    "df_daylight = df_30min_analysis.loc[df_30min_analysis[\"is_day\"]].copy()\n",
    "df_daylight[\"day_rank\"] = df_daylight.groupby(\"date_only\").cumcount()\n",
    "df_daylight[\"day_size\"] = df_daylight.groupby(\"date_only\")[\"is_day\"].transform(\"size\")\n",
    "n_trim = np.floor(df_daylight[\"day_size\"] * DAY_EDGE_TRIM_PERCENTAGE / 2).astype(int)\n",
    "is_day_edge = (df_daylight[\"day_rank\"] < n_trim) | (\n",
    "    df_daylight[\"day_rank\"] >= df_daylight[\"day_size\"] - n_trim\n",
    ")\n",
    "df_30min_analysis[\"is_day_edge\"] = is_day_edge.reindex(\n",
    "    df_30min_analysis.index, fill_value=False\n",
    ")\n",
    "print(\n",
    "    f\"   - Flagged {df_30min_analysis['is_day_edge'].sum()} data points as daytime edges.\"\n",
    ")\n",
    "\n",
    "# --- Step 1b: Invalidate data points for the entire analysis ---\n",
    "print(\n",
    "    \"   - Invalidating data points (nighttime, edges, anomalies) by setting to NaN...\"\n",
    ")\n",
    "# Invalidate nighttime values for both AC and GHI\n",
    "night_mask = ~df_30min_analysis[\"is_day\"]\n",
    "df_30min_analysis.loc[night_mask, [\"ac_export_w\", \"ghi_w_m2\", \"ghi_modeled_w_m2\"]] = (\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Define and apply invalidation masks for AC power and GHI\n",
    "ac_invalidation_mask = (\n",
    "    df_30min_analysis[\"is_clipped\"]\n",
    "    | df_30min_analysis[\"is_curtailed\"]\n",
    "    | df_30min_analysis[\"is_shaded\"]\n",
    "    | df_30min_analysis[\"is_snow_covered\"]\n",
    "    | (df_30min_analysis[\"ac_export_w\"] <= 0)\n",
    "    | df_30min_analysis[\"is_day_edge\"]\n",
    ")\n",
    "ghi_invalidation_mask = df_30min_analysis[\"is_day_edge\"]\n",
    "\n",
    "df_30min_analysis.loc[ac_invalidation_mask, \"ac_export_w\"] = np.nan\n",
    "# Modeled GHI comes from the AC so it's logical to invalidate that one too to keep things synced.\n",
    "df_30min_analysis.loc[ac_invalidation_mask, \"ghi_modeled_w_m2\"] = np.nan\n",
    "df_30min_analysis.loc[ghi_invalidation_mask, \"ghi_w_m2\"] = np.nan\n",
    "print(f\"   - Invalidated {ac_invalidation_mask.sum()} AC export data points.\")\n",
    "print(\n",
    "    f\"   - Invalidated {ghi_invalidation_mask.sum()} GHI data points (on top of nighttime).\"\n",
    ")\n",
    "\n",
    "# --- Step 2: Create daily summary DataFrame from pre-filtered data ---\n",
    "print(\"\\nStep 2: Creating daily summary DataFrame with parabolic fit details...\")\n",
    "daily_groups = df_30min_analysis.groupby(\"date_only\")\n",
    "\n",
    "ac_fit_series = daily_groups[\"ac_export_w\"].agg(get_parabolic_fit_details)\n",
    "ghi_fit_series = daily_groups[\"ghi_w_m2\"].agg(get_parabolic_fit_details)\n",
    "\n",
    "ac_fit_details = pd.DataFrame(\n",
    "    ac_fit_series.tolist(), index=ac_fit_series.index\n",
    ").add_prefix(\"ac_\")\n",
    "ghi_fit_details = pd.DataFrame(\n",
    "    ghi_fit_series.tolist(), index=ghi_fit_series.index\n",
    ").add_prefix(\"ghi_\")\n",
    "\n",
    "df_daily_analysis = pd.concat([ac_fit_details, ghi_fit_details], axis=1)\n",
    "df_daily_analysis.index.name = \"date\"\n",
    "\n",
    "df_daily_analysis[\"solar_noon\"] = daily_groups[\"zenith\"].idxmin()\n",
    "# Correctly determine the start time used for fitting for each series.\n",
    "# This is the timestamp of the first non-NaN value in each day's group,\n",
    "# which matches the reference used inside the get_parabolic_fit_details function.\n",
    "df_daily_analysis[\"ac_fit_start_ts\"] = daily_groups[\"ac_export_w\"].apply(\n",
    "    lambda s: s.first_valid_index()\n",
    ")\n",
    "df_daily_analysis[\"ghi_fit_start_ts\"] = daily_groups[\"ghi_w_m2\"].apply(\n",
    "    lambda s: s.first_valid_index()\n",
    ")\n",
    "\n",
    "# --- Step 3: Identify and flag \"Golden Days\" ---\n",
    "print(\"\\nStep 3: Identifying and flagging 'Golden Days' based on fit quality...\")\n",
    "df_daily_analysis[\"ac_vertex_time\"] = df_daily_analysis[\n",
    "    \"ac_fit_start_ts\"\n",
    "] + pd.to_timedelta(df_daily_analysis[\"ac_vertex_x_seconds\"], unit=\"s\")\n",
    "df_daily_analysis[\"ghi_vertex_time\"] = df_daily_analysis[\n",
    "    \"ghi_fit_start_ts\"\n",
    "] + pd.to_timedelta(df_daily_analysis[\"ghi_vertex_x_seconds\"], unit=\"s\")\n",
    "df_daily_analysis[\"ac_vertex_noon_diff_min\"] = abs(\n",
    "    (\n",
    "        df_daily_analysis[\"ac_vertex_time\"] - df_daily_analysis[\"solar_noon\"]\n",
    "    ).dt.total_seconds()\n",
    "    / 60\n",
    ")\n",
    "df_daily_analysis[\"ghi_vertex_noon_diff_min\"] = abs(\n",
    "    (\n",
    "        df_daily_analysis[\"ghi_vertex_time\"] - df_daily_analysis[\"solar_noon\"]\n",
    "    ).dt.total_seconds()\n",
    "    / 60\n",
    ")\n",
    "\n",
    "r2_ac_threshold = df_daily_analysis[\"ac_r2\"].quantile(R2_QUANTILE_THRESHOLD)\n",
    "r2_ghi_threshold = df_daily_analysis[\"ghi_r2\"].quantile(R2_QUANTILE_THRESHOLD)\n",
    "top_percent = int(np.round((1 - R2_QUANTILE_THRESHOLD) * 100))\n",
    "print(\n",
    "    f\"   - Using AC Power R² threshold: {r2_ac_threshold:.4f} (Top {top_percent}% of fits)\"\n",
    ")\n",
    "print(\n",
    "    f\"   - Using Reference GHI R² threshold: {r2_ghi_threshold:.4f} (Top {top_percent}% of fits)\"\n",
    ")\n",
    "\n",
    "is_valid_ac_fit = (\n",
    "    (df_daily_analysis[\"ac_data_points\"] >= MIN_DATAPOINTS_FOR_FIT)\n",
    "    & (df_daily_analysis[\"ac_a\"] < 0)\n",
    "    & (df_daily_analysis[\"ac_vertex_noon_diff_min\"] <= VERTEX_NOON_TOLERANCE_MINUTES)\n",
    "    & (df_daily_analysis[\"ac_r2\"] > r2_ac_threshold)\n",
    ")\n",
    "is_valid_ghi_fit = (\n",
    "    (df_daily_analysis[\"ghi_data_points\"] >= MIN_DATAPOINTS_FOR_FIT)\n",
    "    & (df_daily_analysis[\"ghi_a\"] < 0)\n",
    "    & (df_daily_analysis[\"ghi_vertex_noon_diff_min\"] <= VERTEX_NOON_TOLERANCE_MINUTES)\n",
    "    & (df_daily_analysis[\"ghi_r2\"] > r2_ghi_threshold)\n",
    ")\n",
    "df_daily_analysis[\"is_golden\"] = is_valid_ac_fit & is_valid_ghi_fit\n",
    "\n",
    "# --- Step 4: Merge daily analysis back and calculate parabolic curves ---\n",
    "print(\"\\nStep 4: Merging daily stats and calculating parabolic curves...\")\n",
    "columns_to_merge = [\n",
    "    \"is_golden\",\n",
    "    \"ac_fit_start_ts\",\n",
    "    \"ghi_fit_start_ts\",\n",
    "    \"ac_a\",\n",
    "    \"ac_b\",\n",
    "    \"ac_c\",\n",
    "    \"ghi_a\",\n",
    "    \"ghi_b\",\n",
    "    \"ghi_c\",\n",
    "]\n",
    "df_30min_analysis = df_30min_analysis.merge(\n",
    "    df_daily_analysis[columns_to_merge],\n",
    "    left_on=\"date_only\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Calculate time deltas relative to the correct start time for each series\n",
    "time_seconds_ac = (\n",
    "    df_30min_analysis.index - df_30min_analysis[\"ac_fit_start_ts\"]\n",
    ").dt.total_seconds()  # type: ignore\n",
    "time_seconds_ghi = (\n",
    "    df_30min_analysis.index - df_30min_analysis[\"ghi_fit_start_ts\"]\n",
    ").dt.total_seconds()  # type: ignore\n",
    "\n",
    "ac_valid_fit_mask = df_30min_analysis[\"ac_a\"].notna() & df_30min_analysis[\"is_day\"]\n",
    "ghi_valid_fit_mask = df_30min_analysis[\"ghi_a\"].notna() & df_30min_analysis[\"is_day\"]\n",
    "\n",
    "ac_coeffs = (\n",
    "    df_30min_analysis.loc[ac_valid_fit_mask, [\"ac_a\", \"ac_b\", \"ac_c\"]].to_numpy().T\n",
    ")\n",
    "ghi_coeffs = (\n",
    "    df_30min_analysis.loc[ghi_valid_fit_mask, [\"ghi_a\", \"ghi_b\", \"ghi_c\"]].to_numpy().T\n",
    ")\n",
    "\n",
    "df_30min_analysis[\"ac_export_parabolic_fit_w\"] = np.nan\n",
    "df_30min_analysis.loc[ac_valid_fit_mask, \"ac_export_parabolic_fit_w\"] = np.polyval(\n",
    "    ac_coeffs, time_seconds_ac[ac_valid_fit_mask]\n",
    ")\n",
    "df_30min_analysis[\"ghi_parabolic_fit_w_m2\"] = np.nan\n",
    "df_30min_analysis.loc[ghi_valid_fit_mask, \"ghi_parabolic_fit_w_m2\"] = np.polyval(\n",
    "    ghi_coeffs, time_seconds_ghi[ghi_valid_fit_mask]\n",
    ")\n",
    "\n",
    "# --- Masking: Set fitted curve values to NaN where the original data was trimmed ---\n",
    "# The fitted curves should only exist where the underlying data was used for fitting.\n",
    "mask_edges_or_night = df_30min_analysis[\"is_day_edge\"] | (~df_30min_analysis[\"is_day\"])\n",
    "\n",
    "df_30min_analysis.loc[\n",
    "    mask_edges_or_night, [\"ac_export_parabolic_fit_w\", \"ghi_parabolic_fit_w_m2\"]\n",
    "] = np.nan\n",
    "\n",
    "df_30min_analysis[\"is_contaminated\"] = (\n",
    "    df_30min_analysis[\"is_clipped\"]\n",
    "    | df_30min_analysis[\"is_curtailed\"]\n",
    "    | df_30min_analysis[\"is_shaded\"]\n",
    "    | df_30min_analysis[\"is_snow_covered\"]\n",
    ")\n",
    "\n",
    "# --- Step 5: Finalize and clean DataFrames ---\n",
    "print(\"\\nStep 5: Finalizing DataFrames for analysis...\")\n",
    "df_daily_analysis.drop(columns=[\"ac_fit_start_ts\", \"ghi_fit_start_ts\"], inplace=True)\n",
    "\n",
    "final_columns = [\n",
    "    \"ac_export_w\",\n",
    "    \"ghi_w_m2\",\n",
    "    \"ghi_modeled_w_m2\",\n",
    "    \"ac_export_parabolic_fit_w\",\n",
    "    \"ghi_parabolic_fit_w_m2\",\n",
    "    \"temp_cell_estimated_c\",\n",
    "    \"is_day\",\n",
    "    \"is_clipped\",\n",
    "    \"is_curtailed\",\n",
    "    \"is_shaded\",\n",
    "    \"is_day_edge\",\n",
    "    \"is_contaminated\",\n",
    "    \"is_snow_covered\",\n",
    "    \"is_golden\",\n",
    "]\n",
    "df_30min_analysis = df_30min_analysis[final_columns]\n",
    "\n",
    "# Fix for FutureWarning: Explicitly convert to nullable boolean dtype before filling.\n",
    "# This ensures that NaN values are handled correctly before converting to non-nullable bool.\n",
    "df_30min_analysis[\"is_golden\"] = (\n",
    "    df_30min_analysis[\"is_golden\"].astype(\"boolean\").fillna(False).astype(bool)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nConstructed daily summary 'df_daily_analysis' with {df_daily_analysis['is_golden'].sum()} golden days.\"\n",
    ")\n",
    "print(\"Sample of the final Daily DataFrame:\")\n",
    "print(df_daily_analysis.sample(n=5))\n",
    "print(\n",
    "    f\"\\nConstructed final 30-minute analysis 'df_30min_analysis' with shape {df_30min_analysis.shape}.\"\n",
    ")\n",
    "print(\"Sample of the final 30-minute DataFrame:\")\n",
    "print(df_30min_analysis.sample(n=5))\n",
    "print(\"\\nDataFrames are now ready for advanced validation and visualization.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3f585",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 7.2 Verify The Fitting Visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d08b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports for Visualization ---\n",
    "import calendar\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- User Configuration ---\n",
    "# Set to a specific year and month for focused analysis, or leave as None for a random selection.\n",
    "YEAR_TO_ANALYZE: int | None = 2022\n",
    "MONTH_TO_ANALYZE: int | None = 6\n",
    "\n",
    "# --- Data Preparation ---\n",
    "assert isinstance(df_daily_analysis.index, pd.DatetimeIndex)\n",
    "if YEAR_TO_ANALYZE is None or MONTH_TO_ANALYZE is None:\n",
    "    # Select a random month from the available data if not specified\n",
    "    available_periods = df_daily_analysis.index.to_period(\"M\").unique()\n",
    "    if not available_periods.empty:\n",
    "        random_period = random.choice(available_periods)\n",
    "        year = random_period.year\n",
    "        month = random_period.month\n",
    "        print(\n",
    "            f\"No year/month specified. Randomly selected: {calendar.month_name[month]} {year}\"\n",
    "        )\n",
    "    else:\n",
    "        year, month = df_daily_analysis.index.year.min(), 1  # Fallback\n",
    "        print(\"Warning: No data available to make a random selection.\")\n",
    "else:\n",
    "    year, month = YEAR_TO_ANALYZE, MONTH_TO_ANALYZE\n",
    "    print(f\"Displaying data for: {calendar.month_name[month]} {year}\")\n",
    "\n",
    "# Filter both DataFrames to the selected month\n",
    "month_mask_daily = (df_daily_analysis.index.year == year) & (\n",
    "    df_daily_analysis.index.month == month\n",
    ")\n",
    "df_daily_month = df_daily_analysis[month_mask_daily]\n",
    "\n",
    "assert isinstance(df_30min_analysis.index, pd.DatetimeIndex)\n",
    "month_mask_30min = (df_30min_analysis.index.year == year) & (\n",
    "    df_30min_analysis.index.month == month\n",
    ")\n",
    "df_30min_month = df_30min_analysis[month_mask_30min]\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "if df_30min_month.empty:\n",
    "    print(\"\\nNo 30-minute data available for the selected period.\")\n",
    "else:\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.05,\n",
    "        row_heights=[0.4, 0.4, 0.2],\n",
    "    )\n",
    "\n",
    "    # --- Row 1: Irradiance ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_30min_month.index,\n",
    "            y=df_30min_month[\"ghi_w_m2\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"GHI (Reference)\",\n",
    "            line={\"color\": \"skyblue\"},\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_30min_month.index,\n",
    "            y=df_30min_month[\"ghi_parabolic_fit_w_m2\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"GHI (Fit)\",\n",
    "            line={\"dash\": \"dash\", \"color\": \"cyan\"},\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_30min_month.index,\n",
    "            y=df_30min_month[\"ghi_modeled_w_m2\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"GHI (Modeled)\",\n",
    "            line={\"color\": \"orange\"},\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # --- Row 2: AC Power ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_30min_month.index,\n",
    "            y=df_30min_month[\"ac_export_w\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"AC Power\",\n",
    "            line={\"color\": \"lightgreen\"},\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_30min_month.index,\n",
    "            y=df_30min_month[\"ac_export_parabolic_fit_w\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"AC Power (Fit)\",\n",
    "            line={\"dash\": \"dash\", \"color\": \"lime\"},\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # --- Row 3: Cell Temperature ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_30min_month.index,\n",
    "            y=df_30min_month[\"temp_cell_estimated_c\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Cell Temp.\",\n",
    "            line={\"color\": \"red\"},\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # --- Add vertical lines for golden days ---\n",
    "    # FIXME: add_vline has a known bug where the x coord needs to be defined in a hacky way\n",
    "    # golden_days_in_month = df_daily_month[df_daily_month[\"is_golden\"]]\n",
    "    # for ts in golden_days_in_month[\"solar_noon\"]:\n",
    "    #     fig.add_vline(\n",
    "    #         x=ts,\n",
    "    #         line_width=1,\n",
    "    #         line_dash=\"dash\",\n",
    "    #         line_color=\"gold\",\n",
    "    #         annotation_text=\"Golden Day\",\n",
    "    #         annotation_position=\"top left\",\n",
    "    #     )\n",
    "\n",
    "    # --- Layout ---\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Monthly GHI and AC Power Analysis for {calendar.month_name[month]} {year}\",\n",
    "        height=800,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Irradiance (W/m²)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"AC Power (W)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Cell Temp (°C)\", row=3, col=1)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# --- Daily Summary Display ---\n",
    "print(f\"\\nDaily Summary for {calendar.month_name[month]} {year}:\")\n",
    "display(df_daily_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be7db6",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 7.3 Visual Comparison on Golden Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d034fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual Comparison on Golden Days ---\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if df_30min_analysis.empty:\n",
    "    print(\"The 'golden' dataset is empty. Cannot generate the time series plot.\")\n",
    "else:\n",
    "    # Create a copy and mask non-golden days with NaN to ensure Plotly renders gaps.\n",
    "    df_golden_only = df_30min_analysis.copy()\n",
    "\n",
    "    # Mask to select data points that are NOT on a golden day OR are contaminated\n",
    "    # (even if on a golden day, contamination should be excluded from validation view)\n",
    "    non_golden_mask = (\n",
    "        ~df_golden_only[\"is_golden\"]\n",
    "        | df_golden_only[\"is_contaminated\"]\n",
    "        | ~df_golden_only[\"is_day\"]\n",
    "    )\n",
    "\n",
    "    # Apply the mask to the GHI columns\n",
    "    df_golden_only.loc[non_golden_mask, [\"ghi_modeled_w_m2\", \"ghi_w_m2\"]] = np.nan\n",
    "\n",
    "    # Reindex for gapped plotting (this ensures all time points exist, even if NaN)\n",
    "    full_range_index = pd.date_range(\n",
    "        start=df_golden_only.index.min(),\n",
    "        end=df_golden_only.index.max(),\n",
    "        freq=\"30min\",\n",
    "    )\n",
    "    df_plot = df_golden_only.reindex(full_range_index)\n",
    "\n",
    "    # --- Create the plot using graph_objects ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add Modeled GHI trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.index,\n",
    "            y=df_plot[\"ghi_modeled_w_m2\"],\n",
    "            name=\"Modeled GHI (AC-derived)\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"#1f77b4\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add Reference GHI trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.index,\n",
    "            y=df_plot[\"ghi_w_m2\"],\n",
    "            name=\"Reference GHI (Satellite or Station)\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"#2ca02c\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Update layout and axis titles ---\n",
    "    fig.update_layout(\n",
    "        title=f\"Modeled vs. Reference GHI on Clear-Sky Golden Days for {TARGET_PARK_NAME}\",\n",
    "        legend_title_text=\"GHI Source\",\n",
    "        yaxis_title=\"GHI (W/m²)\",\n",
    "        xaxis_title=\"Date / Time (UTC)\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f421638",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 7.4 Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e25f5",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pcolors\n",
    "import statsmodels.api as sm\n",
    "import colorsys\n",
    "from statsmodels.regression.linear_model import RegressionResultsWrapper\n",
    "\n",
    "\n",
    "def plot_regression_scatter(\n",
    "    df_reg: pd.DataFrame,\n",
    "    x_col: str,\n",
    "    y_col: str,\n",
    "    ols_model: RegressionResultsWrapper,\n",
    "    title: str,\n",
    "    x_label: str,\n",
    "    y_label: str,\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Generates a standardized regression scatter plot with a Year-Quarter color scheme.\n",
    "    \"\"\"\n",
    "    # --- Color Preparation ---\n",
    "    df_plot = df_reg.copy()\n",
    "    assert isinstance(df_plot.index, pd.DatetimeIndex)\n",
    "    df_plot[\"Timestamp\"] = df_plot.index\n",
    "    df_plot[\"Year\"] = df_plot.index.year\n",
    "    df_plot[\"Quarter\"] = df_plot.index.quarter\n",
    "\n",
    "    unique_years = sorted(df_plot[\"Year\"].unique())\n",
    "    base_colors_hex = pcolors.qualitative.Plotly\n",
    "    year_colors = {\n",
    "        year: base_colors_hex[i % len(base_colors_hex)]\n",
    "        for i, year in enumerate(unique_years)\n",
    "    }\n",
    "    quarter_lightness = {1: 0.80, 2: 0.65, 3: 0.50, 4: 0.35}\n",
    "\n",
    "    color_map = {}\n",
    "    for year, hex_color in year_colors.items():\n",
    "        color_map[year] = {}\n",
    "        rgb_float = tuple(c / 255.0 for c in pcolors.hex_to_rgb(hex_color))\n",
    "        h, l, s = colorsys.rgb_to_hls(*rgb_float)\n",
    "        for quarter, lightness_mod in quarter_lightness.items():\n",
    "            new_rgb_float = colorsys.hls_to_rgb(h, lightness_mod, s)\n",
    "            new_rgb_int = tuple(int(c * 255) for c in new_rgb_float)\n",
    "            color_map[year][quarter] = f\"rgb{new_rgb_int}\"\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter traces\n",
    "    for (year, quarter), group in df_plot.groupby([\"Year\", \"Quarter\"]):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=group[x_col],\n",
    "                y=group[y_col],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=color_map[year][quarter], opacity=0.6, size=5),\n",
    "                name=f\"{year}-Q{quarter}\",\n",
    "                customdata=group[[\"ghi_modeled_w_m2\", \"ghi_w_m2\", \"Timestamp\"]],\n",
    "                hovertemplate=(\n",
    "                    \"<b>%{customdata[2]|%Y-%m-%d %H:%M}</b><br><br>\"\n",
    "                    f\"{x_label}: %{{x:.1f}}<br>\"\n",
    "                    f\"{y_label}: %{{y:.2f}}<br>\"\n",
    "                    \"Modeled GHI: %{customdata[0]:.1f}<br>\"\n",
    "                    \"Reference GHI: %{customdata[1]:.1f}\"\n",
    "                    \"<extra></extra>\"\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add OLS trendline\n",
    "    x_range = [df_plot[x_col].min(), df_plot[x_col].max()]\n",
    "    y_range = [ols_model.params[\"const\"] + ols_model.params[x_col] * x for x in x_range]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=y_range,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"red\", width=2),\n",
    "            name=\"OLS Trendline\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Layout ---\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=x_label,\n",
    "        yaxis_title=y_label,\n",
    "        legend_title=\"Year-Quarter\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "print(\"Helper function plot_regression_scatter defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9459238",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 7.4.1 Regression for Absolute Delta\n",
    "\n",
    "Here we analyze the absolute error `(Modeled - Reference)` in W/m² to identify any temperature-dependent additive bias in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regression for Absolute Delta ---\n",
    "\n",
    "# --- 0. Configuration ---\n",
    "# Define the minimum estimated cell temperature (°C) required for data points\n",
    "# to be included in the regression analysis. This helps filter out cold\n",
    "# periods (potential snow cover) that skew results.\n",
    "MIN_CELL_TEMP_FOR_REGRESSION_C = 0.0\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "# Filter data to exclude low cell temperatures\n",
    "df_abs_reg = df_golden_only[\n",
    "    df_golden_only[\"temp_cell_estimated_c\"] >= MIN_CELL_TEMP_FOR_REGRESSION_C\n",
    "].copy()\n",
    "df_abs_reg[\"delta_ghi_w_m2\"] = df_abs_reg[\"ghi_modeled_w_m2\"] - df_abs_reg[\"ghi_w_m2\"]\n",
    "\n",
    "# --- 2. OLS Regression ---\n",
    "X = sm.add_constant(df_abs_reg[\"temp_cell_estimated_c\"])\n",
    "y = df_abs_reg[\"delta_ghi_w_m2\"]\n",
    "ols_model_abs = sm.OLS(y, X, missing=\"drop\").fit()\n",
    "\n",
    "# --- 3. Plotting ---\n",
    "fig_abs = plot_regression_scatter(\n",
    "    df_reg=df_abs_reg,\n",
    "    x_col=\"temp_cell_estimated_c\",\n",
    "    y_col=\"delta_ghi_w_m2\",\n",
    "    ols_model=ols_model_abs,\n",
    "    title=(\n",
    "        f\"Absolute Model Error vs. Cell Temperature for {TARGET_PARK_NAME}\\n\"\n",
    "        f\"(Filtered: Cell Temp >= {MIN_CELL_TEMP_FOR_REGRESSION_C}°C)\"\n",
    "    ),\n",
    "    x_label=\"Estimated Cell Temperature (°C)\",\n",
    "    y_label=\"Delta (W/m²)\",\n",
    ")\n",
    "fig_abs.show()\n",
    "\n",
    "# --- 4. Summary and Analysis ---\n",
    "print(\"\\n--- OLS Regression Results (Absolute Delta) ---\")\n",
    "print(ols_model_abs.summary())\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Calculate the predicted model error at the standard test condition (STC)\n",
    "# cell temperature of 25°C. This value indicates the model's bias when\n",
    "# the temperature correction term is zero.\n",
    "TEMP_REF = 25.0\n",
    "predicted_delta_at_25c_abs = ols_model_abs.predict([1, TEMP_REF])[0]\n",
    "print(\n",
    "    f\"📈 Predicted Absolute Delta at {TEMP_REF}°C: {predicted_delta_at_25c_abs:+.2f} W/m²\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c461ee",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 7.4.2 Regression for Relative Delta\n",
    "\n",
    "Next, we analyze the relative error as a percentage. This helps identify multiplicative biases that scale with irradiance. We filter out low-irradiance data to avoid numerical instability where small absolute errors can lead to huge relative errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66811c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regression for Relative Delta ---\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "# Filter out low irradiance values and low cell temperatures (using the threshold defined in 7.4.1)\n",
    "df_rel_reg = df_golden_only[\n",
    "    (df_golden_only[\"ghi_w_m2\"] > 100)\n",
    "    & (df_golden_only[\"temp_cell_estimated_c\"] >= MIN_CELL_TEMP_FOR_REGRESSION_C)\n",
    "].copy()\n",
    "\n",
    "# Calculate relative delta as a percentage\n",
    "df_rel_reg[\"delta_ghi_relative\"] = (\n",
    "    (df_rel_reg[\"ghi_modeled_w_m2\"] - df_rel_reg[\"ghi_w_m2\"])\n",
    "    / df_rel_reg[\"ghi_w_m2\"]\n",
    "    * 100\n",
    ")\n",
    "\n",
    "# --- 2. OLS Regression ---\n",
    "X = sm.add_constant(df_rel_reg[\"temp_cell_estimated_c\"])\n",
    "y = df_rel_reg[\"delta_ghi_relative\"]\n",
    "ols_model_rel = sm.OLS(y, X, missing=\"drop\").fit()\n",
    "\n",
    "# --- 3. Plotting ---\n",
    "fig_rel = plot_regression_scatter(\n",
    "    df_reg=df_rel_reg,\n",
    "    x_col=\"temp_cell_estimated_c\",\n",
    "    y_col=\"delta_ghi_relative\",\n",
    "    ols_model=ols_model_rel,\n",
    "    title=(\n",
    "        f\"Relative Model Error vs. Cell Temperature for {TARGET_PARK_NAME}\\n\"\n",
    "        f\"(Filtered: GHI > 100 W/m² and Cell Temp >= {MIN_CELL_TEMP_FOR_REGRESSION_C}°C)\"\n",
    "    ),\n",
    "    x_label=\"Estimated Cell Temperature (°C)\",\n",
    "    y_label=\"Relative Delta (%)\",\n",
    ")\n",
    "fig_rel.show()\n",
    "\n",
    "# --- 4. Summary and Analysis ---\n",
    "print(\"\\n--- OLS Regression Results (Relative Delta) ---\")\n",
    "print(ols_model_rel.summary())\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Calculate the predicted model error at the standard test condition (STC)\n",
    "# cell temperature of 25°C. This provides a direct estimate for the\n",
    "# necessary adjustment to the dc_derate_factor.\n",
    "TEMP_REF = 25.0\n",
    "predicted_delta_at_25c_rel = ols_model_rel.predict([1, TEMP_REF])[0]\n",
    "print(\n",
    "    f\"📈 Predicted Relative Delta at {TEMP_REF}°C: {predicted_delta_at_25c_rel:+.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d85aa",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 8. Final Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ccaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "try:\n",
    "    # Ensure necessary configuration variables are defined from running previous cells\n",
    "    if \"TARGET_PARK_NAME\" not in globals() or \"OUTPUT_DIR\" not in globals():\n",
    "        raise NameError(\n",
    "            \"Required configuration variables (TARGET_PARK_NAME, OUTPUT_DIR) are not defined. \"\n",
    "            \"Please run the '1.2 Configuration' cell first.\"\n",
    "        )\n",
    "\n",
    "    print(\"Preparing data for final export...\")\n",
    "\n",
    "    # 1. Select relevant columns for the export file\n",
    "    columns_to_export = [\n",
    "        \"ac_export_w\",\n",
    "        \"temp_air_c\",\n",
    "        \"wind_speed_m_s\",\n",
    "        \"pressure_hpa\",\n",
    "        \"is_day\",\n",
    "        \"is_clipped\",\n",
    "        \"is_curtailed\",\n",
    "        \"is_shaded\",\n",
    "        \"is_snow_covered\",\n",
    "        \"pdc_effective_w\",\n",
    "        \"poa_estimated_w_m2\",\n",
    "        \"temp_cell_estimated_c\",\n",
    "        \"dni_extra_w_m2\",\n",
    "        \"ghi_modeled_w_m2\",\n",
    "        \"dni_modeled_w_m2\",\n",
    "        \"dhi_modeled_w_m2\",\n",
    "        \"kt_modeled\",\n",
    "    ]\n",
    "    df_export = df_30min[columns_to_export].copy()\n",
    "\n",
    "    # 2. Invalidate data points affected by anomalies\n",
    "    contamination_flags = [\n",
    "        \"is_clipped\",\n",
    "        \"is_curtailed\",\n",
    "        \"is_shaded\",\n",
    "        \"is_snow_covered\",\n",
    "    ]\n",
    "    columns_to_invalidate = [\n",
    "        \"poa_estimated_w_m2\",\n",
    "        \"temp_cell_estimated_c\",\n",
    "        \"ghi_modeled_w_m2\",\n",
    "        \"dni_modeled_w_m2\",\n",
    "        \"dhi_modeled_w_m2\",\n",
    "        \"kt_modeled\",\n",
    "    ]\n",
    "\n",
    "    contamination_mask = df_export[contamination_flags].any(axis=1)\n",
    "    df_export.loc[contamination_mask, columns_to_invalidate] = np.nan\n",
    "    print(f\"Invalidated {contamination_mask.sum()} data points due to anomaly flags.\")\n",
    "\n",
    "    # 3. Handle nighttime values\n",
    "    night_mask = ~df_export[\"is_day\"]\n",
    "    irradiance_columns = [\n",
    "        \"poa_estimated_w_m2\",\n",
    "        \"ghi_modeled_w_m2\",\n",
    "        \"dni_modeled_w_m2\",\n",
    "        \"dhi_modeled_w_m2\",\n",
    "    ]\n",
    "    df_export.loc[night_mask, irradiance_columns] = 0\n",
    "    df_export.loc[night_mask, \"temp_cell_estimated_c\"] = df_export.loc[\n",
    "        night_mask, \"temp_air_c\"\n",
    "    ]\n",
    "    print(f\"Corrected nighttime values for {night_mask.sum()} data points.\")\n",
    "\n",
    "    # 4. Define output paths and save the CSV file\n",
    "    base_filename = f\"{TARGET_PARK_NAME}_irradiance_data\"\n",
    "    output_path_csv = OUTPUT_DIR / f\"{base_filename}.csv\"\n",
    "    output_path_md = OUTPUT_DIR / f\"{base_filename}.md\"\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df_export.to_csv(output_path_csv, index=True, index_label=\"timestamp_utc\")\n",
    "\n",
    "    # 5. Gather file stats and generate Markdown metadata\n",
    "    csv_size_bytes = output_path_csv.stat().st_size\n",
    "    if csv_size_bytes < 1024**2:\n",
    "        csv_size_str = f\"{csv_size_bytes / 1024:.2f} KB\"\n",
    "    else:\n",
    "        csv_size_str = f\"{csv_size_bytes / 1024**2:.2f} MB\"\n",
    "\n",
    "    metadata_md = f\"\"\"\n",
    "# Metadata for {TARGET_PARK_NAME} Irradiance Data\n",
    "\n",
    "## File Information\n",
    "- **Data File:** `{output_path_csv.name}`\n",
    "- **Generated On (UTC):** `{dt.datetime.now(dt.timezone.utc).isoformat()}`\n",
    "- **File Size:** {csv_size_str}\n",
    "\n",
    "## Data Coverage\n",
    "- **Park Name:** `{TARGET_PARK_NAME}`\n",
    "- **Time Period Start (UTC):** `{df_export.index.min()}`\n",
    "- **Time Period End (UTC):** `{df_export.index.max()}`\n",
    "- **Number of Rows:** `{len(df_export):,}`\n",
    "- **Time Frequency:** 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Column Descriptions\n",
    "\n",
    "- `timestamp_utc`: Timestamp in UTC for the start of the 30-minute interval.\n",
    "\n",
    "### Input & Measured Data\n",
    "- `ac_export_w`: AC power exported to the grid, in **Watts**. _Note: This is derived from hourly kWh energy by converting to average power and linearly interpolating to a 30-minute frequency._\n",
    "- `temp_air_c`: Ambient air temperature, in **degrees Celsius**. _Note: Based on hourly measurements, linearly interpolated to a 30-minute frequency._\n",
    "- `wind_speed_m_s`: Wind speed, in **meters per second**. _Note: Based on hourly measurements, linearly interpolated to a 30-minute frequency._\n",
    "- `pressure_hpa`: Atmospheric pressure, in **hectopascals**. _Note: Based on hourly measurements, linearly interpolated to a 30-minute frequency._\n",
    "\n",
    "### Anomaly Flags\n",
    "- `is_day`: Boolean flag, `True` if the sun is above the horizon.\n",
    "- `is_clipped`: Boolean flag, `True` if production is likely limited by inverter clipping.\n",
    "- `is_curtailed`: Boolean flag, `True` if production is likely limited by external curtailment.\n",
    "- `is_shaded`: Boolean flag, `True` if production is likely affected by inter-row shading.\n",
    "- `is_snow_covered`: Boolean flag, `True` if production is likely affected by snow cover on panels.\n",
    "\n",
    "### Modeled & Estimated Data\n",
    "- `pdc_effective_w`: Estimated effective DC capacity of the plant, accounting for degradation and outages, in **Watts**.\n",
    "- `poa_estimated_w_m2`: Estimated Plane of Array (POA) irradiance, derived from AC power, in **W/m²**. _Note: Values are invalidated (`NaN`) during periods with active anomaly flags._\n",
    "- `temp_cell_estimated_c`: Estimated solar cell temperature, in **degrees Celsius**. _Note: Values are invalidated (`NaN`) during periods with active anomaly flags._\n",
    "- `dni_extra_w_m2`: Extraterrestrial direct normal irradiance on a plane normal to the sun's rays, in **W/m²**.\n",
    "- `ghi_modeled_w_m2`: Modeled Global Horizontal Irradiance (GHI), derived from estimated POA, in **W/m²**. _Note: Values are invalidated (`NaN`) during periods with active anomaly flags._\n",
    "- `dni_modeled_w_m2`: Modeled Direct Normal Irradiance (DNI), derived from estimated POA, in **W/m²**. _Note: Values are invalidated (`NaN`) during periods with active anomaly flags._\n",
    "- `dhi_modeled_w_m2`: Modeled Diffuse Horizontal Irradiance (DHI), derived from estimated POA, in **W/m²**. _Note: Values are invalidated (`NaN`) during periods with active anomaly flags._\n",
    "- `kt_modeled`: Modeled clearness index (`ghi_modeled_w_m2` / `ghi_clearsky_w_m2`). _Note: Values are invalidated (`NaN`) during periods with active anomaly flags._\n",
    "\"\"\"\n",
    "\n",
    "    # 6. Save the metadata to a Markdown file\n",
    "    with open(output_path_md, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Use strip() to remove leading/trailing whitespace from the multiline string\n",
    "        f.write(metadata_md.strip())\n",
    "\n",
    "    print(f\"✅ Successfully exported data to: {output_path_csv}\")\n",
    "    print(f\"✅ Successfully exported metadata to: {output_path_md}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"❌ EXPORT FAILED: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred during export: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
