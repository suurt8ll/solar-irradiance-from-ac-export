{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b4f091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Solar Irradiance From AC Export\n",
    "\n",
    "A Jupyter Notebook that does it's best to model and construct a historical solar irradiance time series from solar panel park's historical AC export data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb98ad",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5f3b9",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "# Standard Library Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd074bad",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 1.2 Configuration\n",
    "\n",
    "This project uses a two-step configuration process:\n",
    "\n",
    "1.  **Path Definition (`.env`):** This file defines the project's physical location (`PROJECT_ROOT`) and the name of the configuration file. This separation ensures the notebook is portable across different machines and environments.\n",
    "2.  **Parameter Definition (`config.yml`):** This file contains the physical and electrical parameters of your solar park(s), including sensitive information like GPS coordinates and detailed system specifications.\n",
    "\n",
    "**To get started:**\n",
    "\n",
    "1.  **Configure Paths:** Copy the template file `.env.example` to a new file named `.env`. Open `.env` and set the absolute path for the `PROJECT_ROOT` variable.\n",
    "2.  **Configure Parks:** Copy the example configuration file `config.example.yml` to `config.yml`. Open `config.yml` and replace the placeholder values with the details of your solar installation.\n",
    "\n",
    "The cell below loads the environment variables, resolves the final configuration path, and sets up the plotting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define paths using environment variables\n",
    "PROJECT_ROOT_STR = os.getenv(\"PROJECT_ROOT\")\n",
    "CONFIG_FILENAME = os.getenv(\"CONFIG_FILENAME\", \"config.yml\")  # Fallback to config.yml\n",
    "PRODUCTION_AND_PRICE_FILE_PATH = os.getenv(\n",
    "    \"PRODUCTION_AND_PRICE_FILE_PATH\",\n",
    "    \"/home/user/solar-irradiance-from-ac-export/production.csv\",\n",
    ")\n",
    "WEATHER_FILE_PATH = os.getenv(\n",
    "    \"WEATHER_FILE_PATH\", \"/home/user/solar-irradiance-from-ac-export/weather.csv\"\n",
    ")\n",
    "\n",
    "if not PROJECT_ROOT_STR:\n",
    "    # If PROJECT_ROOT is not set in .env, assume the current working directory\n",
    "    PROJECT_ROOT_STR = os.getcwd()\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è WARNING: PROJECT_ROOT not set in .env. Using current directory: {PROJECT_ROOT_STR}\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = Path(PROJECT_ROOT_STR)\n",
    "CONFIG_PATH = PROJECT_ROOT / CONFIG_FILENAME\n",
    "\n",
    "print(f\"Project Root defined as: {PROJECT_ROOT}\")\n",
    "print(f\"Configuration file path: {CONFIG_PATH}\")\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Extract park configurations\n",
    "    PARK_CONFIGS = config.get(\"parks\", {})\n",
    "\n",
    "    if not PARK_CONFIGS:\n",
    "        raise ValueError(\n",
    "            \"No parks defined under the 'parks' key in the configuration file.\"\n",
    "        )\n",
    "\n",
    "    # Create a list of park names for easy iteration later\n",
    "    PARK_NAMES = list(PARK_CONFIGS.keys())\n",
    "\n",
    "    # --- Load and Validate Target Park for Analysis ---\n",
    "    TARGET_PARK_NAME = os.getenv(\"TARGET_PARK_NAME\")\n",
    "\n",
    "    if not TARGET_PARK_NAME:\n",
    "        raise ValueError(\"TARGET_PARK_NAME is not set in the .env file. Please specify which park to analyze.\")\n",
    "\n",
    "    if TARGET_PARK_NAME not in PARK_NAMES:\n",
    "        raise ValueError(\n",
    "            f\"The target park '{TARGET_PARK_NAME}' defined in .env is not found in 'config.yml'.\\n\"\n",
    "            f\"Available parks in config: {PARK_NAMES}\"\n",
    "        )\n",
    "\n",
    "    print(f\"üéØ Analysis will be performed for target park: '{TARGET_PARK_NAME}'\")\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Configuration loaded successfully from '{CONFIG_PATH}' for {len(PARK_NAMES)} park(s): {', '.join(PARK_NAMES)}.\"\n",
    "    )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå CONFIGURATION ERROR: The '{CONFIG_PATH}' file was not found.\")\n",
    "    print(\n",
    "        \"Please check your .env file's PROJECT_ROOT setting, and ensure 'config.yml' exists at that location.\"\n",
    "    )\n",
    "    print(\n",
    "        \"If 'config.yml' is missing, copy 'config.example.yml' to 'config.yml' and fill in your park's details.\"\n",
    "    )\n",
    "except (yaml.YAMLError, ValueError) as e:\n",
    "    print(\n",
    "        f\"‚ùå CONFIGURATION ERROR: Could not parse '{CONFIG_PATH}'. Please check its format. Details: {e}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Plotting and Display Configuration ---\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# Set display options for better viewing in Jupyter\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "print(\"Plotting and display options set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522e0e5",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6183609",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Helper Function ---\n",
    "\n",
    "\n",
    "def load_park_specific_data(\n",
    "    file_path: str,\n",
    "    timestamp_col: str,\n",
    "    park_name_col: str,\n",
    "    required_data_cols: list[str],\n",
    "    target_park_name: str,\n",
    "    data_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads, validates, and filters data for a single specified park from a long-format CSV.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Absolute path to the CSV file.\n",
    "        timestamp_col (str): Name of the timestamp column.\n",
    "        park_name_col (str): Name of the park identifier column.\n",
    "        required_data_cols (list): List of required data column names.\n",
    "        target_park_name (str): The specific park to extract data for.\n",
    "        data_name (str): A descriptive name for the data (e.g., \"Production\").\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing only the data for the target park,\n",
    "                          with the park_name column removed. Returns an empty\n",
    "                          DataFrame on failure.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading {data_name} Data for '{target_park_name}' ---\")\n",
    "    print(f\"Attempting to load from: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load the full CSV\n",
    "        df = pd.read_csv(\n",
    "            file_path, parse_dates=[timestamp_col], index_col=timestamp_col\n",
    "        )\n",
    "\n",
    "        # 2. Basic Column Check\n",
    "        all_required_cols = required_data_cols + [park_name_col]\n",
    "        if not all(col in df.columns for col in all_required_cols):\n",
    "            missing = [col for col in all_required_cols if col not in df.columns]\n",
    "            raise ValueError(f\"Missing required columns in {data_name} CSV: {missing}\")\n",
    "\n",
    "        # 3. Data Cleaning and Validation\n",
    "        df.index = pd.to_datetime(df.index, utc=True)\n",
    "        df = df.dropna(subset=[park_name_col])\n",
    "        df[park_name_col] = df[park_name_col].astype(str)\n",
    "\n",
    "        # 4. Check if Target Park Exists in Data\n",
    "        if target_park_name not in df[park_name_col].unique():\n",
    "            raise ValueError(\n",
    "                f\"Target park '{target_park_name}' not found in the {data_name} file.\"\n",
    "            )\n",
    "\n",
    "        # 5. Filter for Target Park and Finalize\n",
    "        df_park = df[df[park_name_col] == target_park_name].copy()\n",
    "\n",
    "        # Drop the now-redundant park name column\n",
    "        df_park = df_park.drop(columns=[park_name_col])\n",
    "\n",
    "        df_park = df_park.sort_index()\n",
    "        print(f\"‚úÖ {data_name} data for '{target_park_name}' loaded successfully.\")\n",
    "        print(f\"   Shape of final DataFrame: {df_park.shape}\")\n",
    "        print(f\"   Time range: {df_park.index.min()} to {df_park.index.max()}\")\n",
    "        print(\"Sample:\")\n",
    "        print(df_park.sample(n=5))\n",
    "        return df_park\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå DATA ERROR: The {data_name} file was not found at: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå AN UNEXPECTED ERROR OCCURRED during {data_name} data loading: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"‚úÖ Helper function load_park_specific_data defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084d559",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.1 Hourly Production And Spot Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a38d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Production and Price Data ---\n",
    "\n",
    "# Define required column names for production data\n",
    "COL_TIMESTAMP = \"timestamp_utc\"\n",
    "COL_PARK_NAME = \"park_name\"\n",
    "PRODUCTION_DATA_COLS = [\"ac_export_kwh\", \"spot_price_eur_mwh\"]\n",
    "\n",
    "# Load the data for the target park using the helper function\n",
    "df_production = load_park_specific_data(\n",
    "    file_path=PRODUCTION_AND_PRICE_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=PRODUCTION_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Production & Price\",\n",
    ")\n",
    "assert isinstance(df_production.index, pd.DatetimeIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45339e8",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.2 Load Hourly Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Crop Weather Data ---\n",
    "\n",
    "# Define required column names for weather data\n",
    "WEATHER_DATA_COLS = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\", \"ghi_w_m2\"]\n",
    "\n",
    "# Load the weather data for the target park using the helper function\n",
    "df_weather = load_park_specific_data(\n",
    "    file_path=WEATHER_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=WEATHER_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Weather\",\n",
    ")\n",
    "assert isinstance(df_weather.index, pd.DatetimeIndex)\n",
    "\n",
    "# Post-processing: Crop the weather data to the production time range\n",
    "if not df_production.empty and not df_weather.empty:\n",
    "    start_time = df_production.index.min()\n",
    "    end_time = df_production.index.max()\n",
    "\n",
    "    original_rows = len(df_weather)\n",
    "    df_weather = df_weather.loc[start_time:end_time].copy()\n",
    "\n",
    "    print(f\"\\nWeather data cropped to production time range.\")\n",
    "    print(f\"   Original rows: {original_rows}, Cropped rows: {len(df_weather)}\")\n",
    "    print(f\"   New time range: {df_weather.index.min()} to {df_weather.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdd204",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 3. Data Upsampling and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f165ee8",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interpolation Helper Function ---\n",
    "\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def interpolate_by_gap_size(\n",
    "    data: pd.Series | pd.DataFrame,\n",
    "    max_gap_size: int = 1,\n",
    "    method: Literal[\n",
    "        \"linear\",\n",
    "        \"time\",\n",
    "        \"index\",\n",
    "        \"values\",\n",
    "        \"pad\",\n",
    "        \"nearest\",\n",
    "        \"zero\",\n",
    "        \"slinear\",\n",
    "        \"quadratic\",\n",
    "        \"cubic\",\n",
    "        \"barycentric\",\n",
    "        \"polynomial\",\n",
    "        \"spline\",\n",
    "        \"krogh\",\n",
    "        \"piecewise_polynomial\",\n",
    "        \"pchip\",\n",
    "        \"akima\",\n",
    "        \"cubicspline\",\n",
    "        \"from_derivatives\",\n",
    "    ] = \"linear\",\n",
    "    **kwargs\n",
    ") -> pd.Series | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpolates NaN values in a Series or DataFrame, but only for gaps\n",
    "    that are less than or equal to a specified maximum size.\n",
    "\n",
    "    For a DataFrame, the operation is applied column-wise.\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series | pd.DataFrame): Input data with potential NaNs.\n",
    "        max_gap_size (int): Max consecutive NaNs to interpolate. Gaps larger\n",
    "                            than this are ignored. Defaults to 1.\n",
    "        method (str): Interpolation technique (see pandas.Series.interpolate).\n",
    "                      Defaults to \"linear\".\n",
    "        **kwargs: Additional keyword arguments for the interpolate() method.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series | pd.DataFrame: New object with specified gaps filled.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, (pd.Series, pd.DataFrame)):\n",
    "        raise TypeError(\"Input `data` must be a pandas Series or DataFrame.\")\n",
    "    if not isinstance(max_gap_size, int) or max_gap_size <= 0:\n",
    "        raise ValueError(\"`max_gap_size` must be a positive integer.\")\n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        if data.empty or data.isna().sum() == 0:\n",
    "            return data.copy()\n",
    "\n",
    "        # Identify gaps by creating groups based on non-NaN values\n",
    "        grouper = data.notna().cumsum()\n",
    "        # Calculate the size of each NaN gap\n",
    "        gap_sizes = data.isna().groupby(grouper).transform(\"sum\")\n",
    "        # Create a boolean mask for NaNs that are part of small-enough gaps\n",
    "        mask_to_fill = data.isna() & (gap_sizes <= max_gap_size)\n",
    "\n",
    "        # Interpolate the entire series to get the fill values\n",
    "        fully_interpolated = data.interpolate(method=method, **kwargs) # type: ignore\n",
    "\n",
    "        # Apply the fill values only where the mask is True\n",
    "        return data.where(~mask_to_fill, fully_interpolated)\n",
    "\n",
    "    # If it's a DataFrame, apply this function to each column\n",
    "    return data.apply(\n",
    "        lambda col: interpolate_by_gap_size(\n",
    "            col, max_gap_size=max_gap_size, method=method, **kwargs\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper function interpolate_by_gap_size defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a52d2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.1 Upsample Production Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b71531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Production and Price Data to 30-Minute Frequency ---\n",
    "\n",
    "# Check if the production dataframe exists and is not empty before proceeding\n",
    "if \"df_production\" in locals() and not df_production.empty:\n",
    "    print(f\"--- Upsampling data for park: '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "    # 1. Determine the actual production period to avoid creating a massive index\n",
    "    first_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.min()\n",
    "    last_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.max()\n",
    "\n",
    "    if pd.isna(first_prod_time) or pd.isna(last_prod_time):\n",
    "        print(\"‚ö†Ô∏è No valid production data (> 0 kWh) found. Cannot proceed.\")\n",
    "        df_production_30min = pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"   - Production data range: {first_prod_time} to {last_prod_time}\")\n",
    "\n",
    "        # Crop the hourly data to the actual production period\n",
    "        df_prod_cropped = df_production.loc[first_prod_time:last_prod_time]\n",
    "\n",
    "        # Create a full 30-minute time range for this period\n",
    "        full_30min_range = pd.date_range(\n",
    "            start=first_prod_time, end=last_prod_time, freq=\"30min\", tz=\"UTC\"\n",
    "        )\n",
    "\n",
    "        # 2. Upsample spot price using forward-fill\n",
    "        # The price at HH:30 is the same as the price at HH:00\n",
    "        price_30min = (\n",
    "            df_prod_cropped[\"spot_price_eur_mwh\"]\n",
    "            .reindex(full_30min_range)\n",
    "            .ffill()\n",
    "            .to_frame()\n",
    "        )\n",
    "\n",
    "        # 3. Upsample production data from hourly kWh to 30-min average Power (W)\n",
    "        # Convert kWh (energy) to W (power) for the hourly interval\n",
    "        power_w_hourly = df_prod_cropped[\"ac_export_kwh\"] * 1000\n",
    "\n",
    "        # Shift the index 30 mins forward. The power calculated from the interval\n",
    "        # [HH:00, HH+1:00] is best represented at the midpoint, HH:30.\n",
    "        power_w_hourly.index = power_w_hourly.index + pd.Timedelta(minutes=30)\n",
    "\n",
    "        # Reindex to the full 30-min range. This introduces NaNs at HH:00.\n",
    "        # Use interpolate_by_gap_size(max_gap_size=1) to linearly fill only the\n",
    "        # points at HH:00, which are surrounded by two HH:30 points.\n",
    "        power_w_30min = interpolate_by_gap_size(\n",
    "            power_w_hourly.reindex(full_30min_range), max_gap_size=1, method=\"linear\"\n",
    "        ).to_frame(\"ac_export_w\") # type: ignore\n",
    "\n",
    "        # 4. Combine the upsampled series into a single DataFrame\n",
    "        df_production_30min = price_30min.join(power_w_30min)\n",
    "\n",
    "        print(\"\\n‚úÖ Production and price data upsampled to 30-minute frequency.\")\n",
    "        print(f\"   - Shape of new DataFrame: {df_production_30min.shape}\")\n",
    "        print(\n",
    "            f\"   - New time range: {df_production_30min.index.min()} to {df_production_30min.index.max()}\"\n",
    "        )\n",
    "        print(\"   - Sample data:\")\n",
    "        print(df_production_30min.sample(n=5))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping upsampling: `df_production` is not defined or is empty.\")\n",
    "    df_production_30min = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e916ff",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.2 Upsample Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Weather Data to 30-Minute Frequency ---\n",
    "\n",
    "# Check if both required dataframes are available before proceeding\n",
    "if (\n",
    "    \"df_weather\" in locals()\n",
    "    and not df_weather.empty\n",
    "    and \"df_production_30min\" in locals()\n",
    "    and not df_production_30min.empty\n",
    "):\n",
    "\n",
    "    print(\"--- Upsampling weather data ---\")\n",
    "\n",
    "    # 1. Use the index from the upsampled production data for perfect alignment\n",
    "    target_30min_range = df_production_30min.index\n",
    "    print(\n",
    "        f\"   - Aligning to target time range: {target_30min_range.min()} to {target_30min_range.max()}\"\n",
    "    )\n",
    "\n",
    "    # 2. Handle standard weather variables with linear interpolation\n",
    "    standard_weather_cols = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\"]\n",
    "    df_weather_standard_30min = interpolate_by_gap_size(\n",
    "        df_weather[standard_weather_cols].reindex(target_30min_range),\n",
    "        max_gap_size=1,\n",
    "        method=\"linear\",\n",
    "    )\n",
    "\n",
    "    # 3. Handle GHI with a time shift before interpolation\n",
    "    # The hourly GHI value at HH:00 represents the average over the interval [HH-1:00, HH:00].\n",
    "    # We shift the timestamp back 30 mins to place this value at the interval's midpoint (HH-1:30).\n",
    "    ghi_hourly = df_weather[[\"ghi_w_m2\"]].copy()\n",
    "    assert isinstance(ghi_hourly.index, pd.DatetimeIndex)\n",
    "    ghi_hourly.index = ghi_hourly.index - pd.Timedelta(minutes=30)\n",
    "\n",
    "    # Reindex and interpolate. This will correctly fill the values at the top of the hour (HH:00).\n",
    "    df_ghi_30min = interpolate_by_gap_size(\n",
    "        ghi_hourly.reindex(target_30min_range), max_gap_size=1, method=\"linear\"\n",
    "    )\n",
    "\n",
    "    # 4. Combine all upsampled weather series\n",
    "    df_weather_30min = df_weather_standard_30min.join(df_ghi_30min)\n",
    "\n",
    "    print(\"\\n‚úÖ Weather data upsampled to 30-minute frequency.\")\n",
    "    print(f\"   - Shape of new DataFrame: {df_weather_30min.shape}\")\n",
    "    print(\"   - Sample data:\")\n",
    "    print(df_weather_30min.sample(n=5))\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"‚ö†Ô∏è Skipping weather upsampling: `df_weather` or `df_production_30min` is not available.\"\n",
    "    )\n",
    "    df_weather_30min = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856009b2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.3 Model PVLIB Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fe609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Solar Geometry and Clear-Sky Irradiance with PVLIB ---\n",
    "\n",
    "import pvlib\n",
    "\n",
    "# Check if pre-requisite dataframes are available\n",
    "if (\n",
    "    \"df_weather_30min\" in locals()\n",
    "    and not df_weather_30min.empty\n",
    "    and \"PARK_CONFIGS\" in locals()\n",
    "    and TARGET_PARK_NAME in PARK_CONFIGS\n",
    "):\n",
    "\n",
    "    print(f\"--- Modeling PVLIB features for '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "    # 1. Extract location parameters from the main config\n",
    "    park_params = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "    location = pvlib.location.Location(\n",
    "        latitude=park_params[\"location\"][\"latitude\"],\n",
    "        longitude=park_params[\"location\"][\"longitude\"],\n",
    "        tz=\"UTC\",\n",
    "    )\n",
    "\n",
    "    # 2. Prepare inputs for pvlib calculations\n",
    "    times = df_weather_30min.index\n",
    "    pressure_pa = df_weather_30min[\"pressure_hpa\"] * 100  # Convert hPa to Pa\n",
    "    temperature_c = df_weather_30min[\"temp_air_c\"]\n",
    "\n",
    "    # 3. Perform pvlib calculations\n",
    "    print(\"   - Calculating solar position...\")\n",
    "    solar_position = location.get_solarposition(\n",
    "        times, pressure=pressure_pa, temperature=temperature_c  # type: ignore\n",
    "    )\n",
    "    assert isinstance(solar_position, pd.DataFrame)\n",
    "\n",
    "    print(\"   - Calculating airmass...\")\n",
    "    airmass_relative = pvlib.atmosphere.get_relative_airmass(\n",
    "        zenith=solar_position[\"apparent_zenith\"]\n",
    "    )\n",
    "    airmass_absolute = pvlib.atmosphere.get_absolute_airmass(\n",
    "        airmass_relative=airmass_relative, pressure=pressure_pa  # type: ignore\n",
    "    )\n",
    "\n",
    "    print(\"   - Calculating extraterrestrial radiation...\")\n",
    "    dni_extra = pvlib.irradiance.get_extra_radiation(times)\n",
    "\n",
    "    print(\"   - Calculating clear-sky irradiance (Ineichen model)...\")\n",
    "    clearsky_irrad = location.get_clearsky(\n",
    "        times,\n",
    "        model=\"ineichen\",\n",
    "        solar_position=solar_position,\n",
    "        airmass_absolute=airmass_absolute,\n",
    "        dni_extra=dni_extra,\n",
    "    )\n",
    "    assert isinstance(clearsky_irrad, pd.DataFrame)\n",
    "    clearsky_irrad = clearsky_irrad.rename(\n",
    "        columns={\n",
    "            \"ghi\": \"ghi_clearsky_w_m2\",\n",
    "            \"dni\": \"dni_clearsky_w_m2\",\n",
    "            \"dhi\": \"dhi_clearsky_w_m2\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"   - Adding 'is_day' flag...\")\n",
    "    is_day = solar_position[\"apparent_zenith\"] < 90.0\n",
    "\n",
    "    # 4. Assemble the final pvlib DataFrame\n",
    "    df_pvlib_30min = pd.concat(\n",
    "        [\n",
    "            solar_position[[\"apparent_zenith\", \"zenith\", \"azimuth\"]],\n",
    "            pd.Series(airmass_relative, name=\"airmass_relative\"),\n",
    "            pd.Series(airmass_absolute, name=\"airmass_absolute\"),\n",
    "            pd.Series(dni_extra, name=\"dni_extra_w_m2\"),\n",
    "            clearsky_irrad,\n",
    "            pd.Series(is_day, name=\"is_day\"),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ PVLIB features modeled successfully.\")\n",
    "    print(f\"   - Shape of new DataFrame: {df_pvlib_30min.shape}\")\n",
    "    print(\"   - Sample data:\")\n",
    "    print(df_pvlib_30min.sample(n=5))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping PVLIB modeling: Prerequisite data is not available.\")\n",
    "    df_pvlib_30min = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4a5aa",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.4 Merge DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge All 30-Minute DataFrames ---\n",
    "\n",
    "# Check if all three dataframes to be merged are available\n",
    "if (\n",
    "    \"df_production_30min\" in locals()\n",
    "    and not df_production_30min.empty\n",
    "    and \"df_weather_30min\" in locals()\n",
    "    and not df_weather_30min.empty\n",
    "    and \"df_pvlib_30min\" in locals()\n",
    "    and not df_pvlib_30min.empty\n",
    "):\n",
    "\n",
    "    print(\"--- Merging all 30-minute data sources ---\")\n",
    "\n",
    "    # Join the three dataframes on their common DatetimeIndex\n",
    "    # This preserves all rows and fills with NaNs where data is missing in one source\n",
    "    df_30min = df_production_30min.join([df_weather_30min, df_pvlib_30min])\n",
    "\n",
    "    print(\"\\n‚úÖ All data sources successfully merged into a single DataFrame.\")\n",
    "    print(f\"   - Final shape: {df_30min.shape}\")\n",
    "    print(f\"   - Final time range: {df_30min.index.min()} to {df_30min.index.max()}\")\n",
    "    print(\"   - Sample of merged data:\")\n",
    "    print(df_30min.sample(n=5))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping merge operation: One or more source DataFrames are missing.\")\n",
    "    df_30min = pd.DataFrame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
