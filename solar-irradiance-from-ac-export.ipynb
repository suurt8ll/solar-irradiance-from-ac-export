{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b4f091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Solar Irradiance From AC Export\n",
    "\n",
    "A Jupyter Notebook that does it's best to model and construct a historical solar irradiance time series from solar panel park's historical AC export data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb98ad",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5f3b9",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "# Standard Library Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd074bad",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 1.2 Configuration\n",
    "\n",
    "This project uses a two-step configuration process:\n",
    "\n",
    "1.  **Path Definition (`.env`):** This file defines the project's physical location (`PROJECT_ROOT`) and the name of the configuration file. This separation ensures the notebook is portable across different machines and environments.\n",
    "2.  **Parameter Definition (`config.yml`):** This file contains the physical and electrical parameters of your solar park(s), including sensitive information like GPS coordinates and detailed system specifications.\n",
    "\n",
    "**To get started:**\n",
    "\n",
    "1.  **Configure Paths:** Copy the template file `.env.example` to a new file named `.env`. Open `.env` and set the absolute path for the `PROJECT_ROOT` variable.\n",
    "2.  **Configure Parks:** Copy the example configuration file `config.example.yml` to `config.yml`. Open `config.yml` and replace the placeholder values with the details of your solar installation.\n",
    "\n",
    "The cell below loads the environment variables, resolves the final configuration path, and sets up the plotting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define paths using environment variables\n",
    "PROJECT_ROOT_STR = os.getenv(\"PROJECT_ROOT\")\n",
    "CONFIG_FILENAME = os.getenv(\"CONFIG_FILENAME\", \"config.yml\")  # Fallback to config.yml\n",
    "PRODUCTION_AND_PRICE_FILE_PATH = os.getenv(\n",
    "    \"PRODUCTION_AND_PRICE_FILE_PATH\",\n",
    "    \"/home/user/solar-irradiance-from-ac-export/production.csv\",\n",
    ")\n",
    "WEATHER_FILE_PATH = os.getenv(\n",
    "    \"WEATHER_FILE_PATH\", \"/home/user/solar-irradiance-from-ac-export/weather.csv\"\n",
    ")\n",
    "\n",
    "if not PROJECT_ROOT_STR:\n",
    "    # If PROJECT_ROOT is not set in .env, assume the current working directory\n",
    "    PROJECT_ROOT_STR = os.getcwd()\n",
    "    print(\n",
    "        f\"âš ï¸ WARNING: PROJECT_ROOT not set in .env. Using current directory: {PROJECT_ROOT_STR}\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = Path(PROJECT_ROOT_STR)\n",
    "CONFIG_PATH = PROJECT_ROOT / CONFIG_FILENAME\n",
    "\n",
    "print(f\"Project Root defined as: {PROJECT_ROOT}\")\n",
    "print(f\"Configuration file path: {CONFIG_PATH}\")\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Extract park configurations\n",
    "    PARK_CONFIGS = config.get(\"parks\", {})\n",
    "\n",
    "    if not PARK_CONFIGS:\n",
    "        raise ValueError(\n",
    "            \"No parks defined under the 'parks' key in the configuration file.\"\n",
    "        )\n",
    "\n",
    "    # Create a list of park names for easy iteration later\n",
    "    PARK_NAMES = list(PARK_CONFIGS.keys())\n",
    "\n",
    "    # --- Load and Validate Target Park for Analysis ---\n",
    "    TARGET_PARK_NAME = os.getenv(\"TARGET_PARK_NAME\")\n",
    "\n",
    "    if not TARGET_PARK_NAME:\n",
    "        raise ValueError(\"TARGET_PARK_NAME is not set in the .env file. Please specify which park to analyze.\")\n",
    "\n",
    "    if TARGET_PARK_NAME not in PARK_NAMES:\n",
    "        raise ValueError(\n",
    "            f\"The target park '{TARGET_PARK_NAME}' defined in .env is not found in 'config.yml'.\\n\"\n",
    "            f\"Available parks in config: {PARK_NAMES}\"\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸŽ¯ Analysis will be performed for target park: '{TARGET_PARK_NAME}'\")\n",
    "\n",
    "    print(\n",
    "        f\"âœ… Configuration loaded successfully from '{CONFIG_PATH}' for {len(PARK_NAMES)} park(s): {', '.join(PARK_NAMES)}.\"\n",
    "    )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ CONFIGURATION ERROR: The '{CONFIG_PATH}' file was not found.\")\n",
    "    print(\n",
    "        \"Please check your .env file's PROJECT_ROOT setting, and ensure 'config.yml' exists at that location.\"\n",
    "    )\n",
    "    print(\n",
    "        \"If 'config.yml' is missing, copy 'config.example.yml' to 'config.yml' and fill in your park's details.\"\n",
    "    )\n",
    "except (yaml.YAMLError, ValueError) as e:\n",
    "    print(\n",
    "        f\"âŒ CONFIGURATION ERROR: Could not parse '{CONFIG_PATH}'. Please check its format. Details: {e}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Plotting and Display Configuration ---\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# Set display options for better viewing in Jupyter\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "print(\"Plotting and display options set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522e0e5",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6183609",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Helper Function ---\n",
    "\n",
    "\n",
    "def load_park_specific_data(\n",
    "    file_path: str,\n",
    "    timestamp_col: str,\n",
    "    park_name_col: str,\n",
    "    required_data_cols: list[str],\n",
    "    target_park_name: str,\n",
    "    data_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads, validates, and filters data for a single specified park from a long-format CSV.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Absolute path to the CSV file.\n",
    "        timestamp_col (str): Name of the timestamp column.\n",
    "        park_name_col (str): Name of the park identifier column.\n",
    "        required_data_cols (list): List of required data column names.\n",
    "        target_park_name (str): The specific park to extract data for.\n",
    "        data_name (str): A descriptive name for the data (e.g., \"Production\").\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing only the data for the target park,\n",
    "                          with the park_name column removed. Returns an empty\n",
    "                          DataFrame on failure.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading {data_name} Data for '{target_park_name}' ---\")\n",
    "    print(f\"Attempting to load from: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Load the full CSV\n",
    "        df = pd.read_csv(\n",
    "            file_path, parse_dates=[timestamp_col], index_col=timestamp_col\n",
    "        )\n",
    "\n",
    "        # 2. Basic Column Check\n",
    "        all_required_cols = required_data_cols + [park_name_col]\n",
    "        if not all(col in df.columns for col in all_required_cols):\n",
    "            missing = [col for col in all_required_cols if col not in df.columns]\n",
    "            raise ValueError(f\"Missing required columns in {data_name} CSV: {missing}\")\n",
    "\n",
    "        # 3. Data Cleaning and Validation\n",
    "        df.index = pd.to_datetime(df.index, utc=True)\n",
    "        df = df.dropna(subset=[park_name_col])\n",
    "        df[park_name_col] = df[park_name_col].astype(str)\n",
    "\n",
    "        # 4. Check if Target Park Exists in Data\n",
    "        if target_park_name not in df[park_name_col].unique():\n",
    "            raise ValueError(\n",
    "                f\"Target park '{target_park_name}' not found in the {data_name} file.\"\n",
    "            )\n",
    "\n",
    "        # 5. Filter for Target Park and Finalize\n",
    "        df_park = df[df[park_name_col] == target_park_name].copy()\n",
    "\n",
    "        # Drop the now-redundant park name column\n",
    "        df_park = df_park.drop(columns=[park_name_col])\n",
    "\n",
    "        df_park = df_park.sort_index()\n",
    "        print(f\"âœ… {data_name} data for '{target_park_name}' loaded successfully.\")\n",
    "        print(f\"   Shape of final DataFrame: {df_park.shape}\")\n",
    "        print(f\"   Time range: {df_park.index.min()} to {df_park.index.max()}\")\n",
    "        print(\"Sample:\")\n",
    "        print(df_park.sample(n=5))\n",
    "        return df_park\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ DATA ERROR: The {data_name} file was not found at: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ AN UNEXPECTED ERROR OCCURRED during {data_name} data loading: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"âœ… Helper function load_park_specific_data defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084d559",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.1 Hourly Production And Spot Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a38d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Production and Price Data ---\n",
    "\n",
    "# Define required column names for production data\n",
    "COL_TIMESTAMP = \"timestamp_utc\"\n",
    "COL_PARK_NAME = \"park_name\"\n",
    "PRODUCTION_DATA_COLS = [\"ac_export_kwh\", \"spot_price_eur_mwh\"]\n",
    "\n",
    "# Load the data for the target park using the helper function\n",
    "df_production = load_park_specific_data(\n",
    "    file_path=PRODUCTION_AND_PRICE_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=PRODUCTION_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Production & Price\",\n",
    ")\n",
    "assert isinstance(df_production.index, pd.DatetimeIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45339e8",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2.2 Load Hourly Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Crop Weather Data ---\n",
    "\n",
    "# Define required column names for weather data\n",
    "WEATHER_DATA_COLS = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\", \"ghi_w_m2\"]\n",
    "\n",
    "# Load the weather data for the target park using the helper function\n",
    "df_weather = load_park_specific_data(\n",
    "    file_path=WEATHER_FILE_PATH,\n",
    "    timestamp_col=COL_TIMESTAMP,\n",
    "    park_name_col=COL_PARK_NAME,\n",
    "    required_data_cols=WEATHER_DATA_COLS,\n",
    "    target_park_name=TARGET_PARK_NAME,  # type: ignore\n",
    "    data_name=\"Weather\",\n",
    ")\n",
    "assert isinstance(df_weather.index, pd.DatetimeIndex)\n",
    "\n",
    "# Post-processing: Crop the weather data to the production time range\n",
    "if not df_production.empty and not df_weather.empty:\n",
    "    start_time = df_production.index.min()\n",
    "    end_time = df_production.index.max()\n",
    "\n",
    "    original_rows = len(df_weather)\n",
    "    df_weather = df_weather.loc[start_time:end_time].copy()\n",
    "\n",
    "    print(f\"\\nWeather data cropped to production time range.\")\n",
    "    print(f\"   Original rows: {original_rows}, Cropped rows: {len(df_weather)}\")\n",
    "    print(f\"   New time range: {df_weather.index.min()} to {df_weather.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdd204",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 3. Data Upsampling and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f165ee8",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interpolation Helper Function ---\n",
    "\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def interpolate_by_gap_size(\n",
    "    data: pd.Series | pd.DataFrame,\n",
    "    max_gap_size: int = 1,\n",
    "    method: Literal[\n",
    "        \"linear\",\n",
    "        \"time\",\n",
    "        \"index\",\n",
    "        \"values\",\n",
    "        \"pad\",\n",
    "        \"nearest\",\n",
    "        \"zero\",\n",
    "        \"slinear\",\n",
    "        \"quadratic\",\n",
    "        \"cubic\",\n",
    "        \"barycentric\",\n",
    "        \"polynomial\",\n",
    "        \"spline\",\n",
    "        \"krogh\",\n",
    "        \"piecewise_polynomial\",\n",
    "        \"pchip\",\n",
    "        \"akima\",\n",
    "        \"cubicspline\",\n",
    "        \"from_derivatives\",\n",
    "    ] = \"linear\",\n",
    "    **kwargs\n",
    ") -> pd.Series | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpolates NaN values in a Series or DataFrame, but only for gaps\n",
    "    that are less than or equal to a specified maximum size.\n",
    "\n",
    "    For a DataFrame, the operation is applied column-wise.\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series | pd.DataFrame): Input data with potential NaNs.\n",
    "        max_gap_size (int): Max consecutive NaNs to interpolate. Gaps larger\n",
    "                            than this are ignored. Defaults to 1.\n",
    "        method (str): Interpolation technique (see pandas.Series.interpolate).\n",
    "                      Defaults to \"linear\".\n",
    "        **kwargs: Additional keyword arguments for the interpolate() method.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series | pd.DataFrame: New object with specified gaps filled.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, (pd.Series, pd.DataFrame)):\n",
    "        raise TypeError(\"Input `data` must be a pandas Series or DataFrame.\")\n",
    "    if not isinstance(max_gap_size, int) or max_gap_size <= 0:\n",
    "        raise ValueError(\"`max_gap_size` must be a positive integer.\")\n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        if data.empty or data.isna().sum() == 0:\n",
    "            return data.copy()\n",
    "\n",
    "        # Identify gaps by creating groups based on non-NaN values\n",
    "        grouper = data.notna().cumsum()\n",
    "        # Calculate the size of each NaN gap\n",
    "        gap_sizes = data.isna().groupby(grouper).transform(\"sum\")\n",
    "        # Create a boolean mask for NaNs that are part of small-enough gaps\n",
    "        mask_to_fill = data.isna() & (gap_sizes <= max_gap_size)\n",
    "\n",
    "        # Interpolate the entire series to get the fill values\n",
    "        fully_interpolated = data.interpolate(method=method, **kwargs) # type: ignore\n",
    "\n",
    "        # Apply the fill values only where the mask is True\n",
    "        return data.where(~mask_to_fill, fully_interpolated)\n",
    "\n",
    "    # If it's a DataFrame, apply this function to each column\n",
    "    return data.apply(\n",
    "        lambda col: interpolate_by_gap_size(\n",
    "            col, max_gap_size=max_gap_size, method=method, **kwargs\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"âœ… Helper function interpolate_by_gap_size defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a52d2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.1 Upsample Production Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b71531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Production and Price Data to 30-Minute Frequency ---\n",
    "\n",
    "print(f\"--- Upsampling data for park: '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# 1. Determine the actual production period to avoid creating a massive index\n",
    "first_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.min()\n",
    "last_prod_time: pd.Timestamp = df_production[df_production[\"ac_export_kwh\"] > 0].index.max()\n",
    "\n",
    "if pd.isna(first_prod_time) or pd.isna(last_prod_time):\n",
    "    print(\"âš ï¸ No valid production data (> 0 kWh) found. Cannot proceed.\")\n",
    "    df_production_30min = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"   - Production data range: {first_prod_time} to {last_prod_time}\")\n",
    "\n",
    "    # Crop the hourly data to the actual production period\n",
    "    df_prod_cropped = df_production.loc[first_prod_time:last_prod_time]\n",
    "\n",
    "    # Create a full 30-minute time range for this period\n",
    "    full_30min_range = pd.date_range(\n",
    "        start=first_prod_time, end=last_prod_time, freq=\"30min\", tz=\"UTC\"\n",
    "    )\n",
    "\n",
    "    # 2. Upsample spot price using forward-fill\n",
    "    # The price at HH:30 is the same as the price at HH:00\n",
    "    price_30min = (\n",
    "        df_prod_cropped[\"spot_price_eur_mwh\"]\n",
    "        .reindex(full_30min_range)\n",
    "        .ffill()\n",
    "        .to_frame()\n",
    "    )\n",
    "\n",
    "    # 3. Upsample production data from hourly kWh to 30-min average Power (W)\n",
    "    # Convert kWh (energy) to W (power) for the hourly interval\n",
    "    power_w_hourly = df_prod_cropped[\"ac_export_kwh\"] * 1000\n",
    "\n",
    "    # Shift the index 30 mins forward. The power calculated from the interval\n",
    "    # [HH:00, HH+1:00] is best represented at the midpoint, HH:30.\n",
    "    power_w_hourly.index = power_w_hourly.index + pd.Timedelta(minutes=30)\n",
    "\n",
    "    # Reindex to the full 30-min range. This introduces NaNs at HH:00.\n",
    "    # Use interpolate_by_gap_size(max_gap_size=1) to linearly fill only the\n",
    "    # points at HH:00, which are surrounded by two HH:30 points.\n",
    "    power_w_30min = interpolate_by_gap_size(\n",
    "        power_w_hourly.reindex(full_30min_range), max_gap_size=1, method=\"linear\"\n",
    "    ).to_frame(\"ac_export_w\") # type: ignore\n",
    "\n",
    "    # 4. Combine the upsampled series into a single DataFrame\n",
    "    df_production_30min = price_30min.join(power_w_30min)\n",
    "\n",
    "    print(\"\\nâœ… Production and price data upsampled to 30-minute frequency.\")\n",
    "    print(f\"   - Shape of new DataFrame: {df_production_30min.shape}\")\n",
    "    print(\n",
    "        f\"   - New time range: {df_production_30min.index.min()} to {df_production_30min.index.max()}\"\n",
    "    )\n",
    "    print(\"   - Sample data:\")\n",
    "    print(df_production_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e916ff",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.2 Upsample Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upsample Weather Data to 30-Minute Frequency ---\n",
    "\n",
    "print(\"--- Upsampling weather data ---\")\n",
    "\n",
    "# 1. Use the index from the upsampled production data for perfect alignment\n",
    "target_30min_range = df_production_30min.index\n",
    "print(\n",
    "    f\"   - Aligning to target time range: {target_30min_range.min()} to {target_30min_range.max()}\"\n",
    ")\n",
    "\n",
    "# 2. Handle standard weather variables with linear interpolation\n",
    "standard_weather_cols = [\"temp_air_c\", \"wind_speed_m_s\", \"pressure_hpa\"]\n",
    "df_weather_standard_30min = interpolate_by_gap_size(\n",
    "    df_weather[standard_weather_cols].reindex(target_30min_range),\n",
    "    max_gap_size=1,\n",
    "    method=\"linear\",\n",
    ")\n",
    "\n",
    "# 3. Handle GHI with a time shift before interpolation\n",
    "# The hourly GHI value at HH:00 represents the average over the interval [HH-1:00, HH:00].\n",
    "# We shift the timestamp back 30 mins to place this value at the interval's midpoint (HH-1:30).\n",
    "ghi_hourly = df_weather[[\"ghi_w_m2\"]].copy()\n",
    "assert isinstance(ghi_hourly.index, pd.DatetimeIndex)\n",
    "ghi_hourly.index = ghi_hourly.index - pd.Timedelta(minutes=30)\n",
    "\n",
    "# Reindex and interpolate. This will correctly fill the values at the top of the hour (HH:00).\n",
    "df_ghi_30min = interpolate_by_gap_size(\n",
    "    ghi_hourly.reindex(target_30min_range), max_gap_size=1, method=\"linear\"\n",
    ")\n",
    "\n",
    "# 4. Combine all upsampled weather series\n",
    "df_weather_30min = df_weather_standard_30min.join(df_ghi_30min)\n",
    "\n",
    "print(\"\\nâœ… Weather data upsampled to 30-minute frequency.\")\n",
    "print(f\"   - Shape of new DataFrame: {df_weather_30min.shape}\")\n",
    "print(\"   - Sample data:\")\n",
    "print(df_weather_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856009b2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.3 Model PVLIB Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fe609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Solar Geometry and Clear-Sky Irradiance with PVLIB ---\n",
    "\n",
    "import pvlib\n",
    "\n",
    "print(f\"--- Modeling PVLIB features for '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# 1. Extract location parameters from the main config\n",
    "park_params = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "location = pvlib.location.Location(\n",
    "    latitude=park_params[\"location\"][\"latitude\"],\n",
    "    longitude=park_params[\"location\"][\"longitude\"],\n",
    "    tz=\"UTC\",\n",
    ")\n",
    "\n",
    "# 2. Prepare inputs for pvlib calculations\n",
    "times = df_weather_30min.index\n",
    "pressure_pa = df_weather_30min[\"pressure_hpa\"] * 100  # Convert hPa to Pa\n",
    "temperature_c = df_weather_30min[\"temp_air_c\"]\n",
    "\n",
    "# 3. Perform pvlib calculations\n",
    "print(\"   - Calculating solar position...\")\n",
    "solar_position = location.get_solarposition(\n",
    "    times, pressure=pressure_pa, temperature=temperature_c  # type: ignore\n",
    ")\n",
    "assert isinstance(solar_position, pd.DataFrame)\n",
    "\n",
    "print(\"   - Calculating airmass...\")\n",
    "airmass_relative = pvlib.atmosphere.get_relative_airmass(\n",
    "    zenith=solar_position[\"apparent_zenith\"]\n",
    ")\n",
    "airmass_absolute = pvlib.atmosphere.get_absolute_airmass(\n",
    "    airmass_relative=airmass_relative, pressure=pressure_pa  # type: ignore\n",
    ")\n",
    "\n",
    "print(\"   - Calculating extraterrestrial radiation...\")\n",
    "dni_extra = pvlib.irradiance.get_extra_radiation(times)\n",
    "\n",
    "print(\"   - Calculating clear-sky irradiance (Ineichen model)...\")\n",
    "clearsky_irrad = location.get_clearsky(\n",
    "    times,\n",
    "    model=\"ineichen\",\n",
    "    solar_position=solar_position,\n",
    "    airmass_absolute=airmass_absolute,\n",
    "    dni_extra=dni_extra,\n",
    ")\n",
    "assert isinstance(clearsky_irrad, pd.DataFrame)\n",
    "clearsky_irrad = clearsky_irrad.rename(\n",
    "    columns={\n",
    "        \"ghi\": \"ghi_clearsky_w_m2\",\n",
    "        \"dni\": \"dni_clearsky_w_m2\",\n",
    "        \"dhi\": \"dhi_clearsky_w_m2\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"   - Adding 'is_day' flag...\")\n",
    "is_day = solar_position[\"apparent_zenith\"] < 90.0\n",
    "\n",
    "# 4. Assemble the final pvlib DataFrame\n",
    "df_pvlib_30min = pd.concat(\n",
    "    [\n",
    "        solar_position[[\"apparent_zenith\", \"zenith\", \"azimuth\"]],\n",
    "        pd.Series(airmass_relative, name=\"airmass_relative\"),\n",
    "        pd.Series(airmass_absolute, name=\"airmass_absolute\"),\n",
    "        pd.Series(dni_extra, name=\"dni_extra_w_m2\"),\n",
    "        clearsky_irrad,\n",
    "        pd.Series(is_day, name=\"is_day\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… PVLIB features modeled successfully.\")\n",
    "print(f\"   - Shape of new DataFrame: {df_pvlib_30min.shape}\")\n",
    "print(\"   - Sample data:\")\n",
    "print(df_pvlib_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4a5aa",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3.4 Merge DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge All 30-Minute DataFrames ---\n",
    "\n",
    "print(\"--- Merging all 30-minute data sources ---\")\n",
    "\n",
    "# Join the three dataframes on their common DatetimeIndex\n",
    "# This preserves all rows and fills with NaNs where data is missing in one source\n",
    "df_30min = df_production_30min.join([df_weather_30min, df_pvlib_30min])\n",
    "\n",
    "print(\"\\nâœ… All data sources successfully merged into a single DataFrame.\")\n",
    "print(f\"   - Final shape: {df_30min.shape}\")\n",
    "print(f\"   - Final time range: {df_30min.index.min()} to {df_30min.index.max()}\")\n",
    "print(\"   - Sample of merged data:\")\n",
    "print(df_30min.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d3e84",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 4. Anomaly Flagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d961c",
   "metadata": {},
   "source": [
    "### 4.1 Flag Clipping and Curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Flag Clipping and Curtailment Events ---\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "print(f\"--- Flagging anomalies for '{TARGET_PARK_NAME}' ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Fetch park-specific parameters from the config\n",
    "park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "export_limit_w = park_config[\"export_limit_kw\"] * 1000\n",
    "pdc0_w = park_config[\"system\"][\"pdc0\"]\n",
    "\n",
    "# Define thresholds for flagging logic\n",
    "# Flag as clipped if power is within this percentage of the export limit\n",
    "CLIPPING_TOLERANCE_PCT = 0.01\n",
    "# Flag as curtailed if power is below this percentage of DC capacity during negative prices\n",
    "CURTAILMENT_POWER_THRESHOLD_PCT = 0.01\n",
    "\n",
    "clipping_threshold_w = export_limit_w * (1 - CLIPPING_TOLERANCE_PCT)\n",
    "curtailment_threshold_w = pdc0_w * CURTAILMENT_POWER_THRESHOLD_PCT\n",
    "\n",
    "print(f\"   - Clipping threshold: >= {clipping_threshold_w:.2f} W\")\n",
    "print(\n",
    "    f\"   - Curtailment threshold: < {curtailment_threshold_w:.2f} W (during negative prices)\"\n",
    ")\n",
    "\n",
    "# Initialize flag columns to False\n",
    "df_30min[\"is_clipped\"] = False\n",
    "df_30min[\"is_curtailed\"] = False\n",
    "\n",
    "# --- Clipping Detection Logic ---\n",
    "print(\"\\n1. Applying Clipping Detection Logic...\")\n",
    "\n",
    "# Step 1: Directly flag HH:30 points at or near the export limit\n",
    "is_hh30 = df_30min.index.minute == 30\n",
    "direct_clip_mask = is_hh30 & (df_30min[\"ac_export_w\"] >= clipping_threshold_w)\n",
    "df_30min.loc[direct_clip_mask, \"is_clipped\"] = True\n",
    "print(f\"   - Found {direct_clip_mask.sum()} HH:30 points with direct clipping.\")\n",
    "\n",
    "# Step 2: Propagate flag to adjacent HH:30 points (Temporal Contamination)\n",
    "clipped_at_hh30 = df_30min[\"is_clipped\"][is_hh30]\n",
    "neighbor_clip_mask = clipped_at_hh30.shift(\n",
    "    1, fill_value=False\n",
    ") | clipped_at_hh30.shift(-1, fill_value=False)\n",
    "\n",
    "# --- FIX ---\n",
    "# The original code incorrectly used `neighbor_clip_mask.index`, which flagged all HH:30 points.\n",
    "# The corrected code uses boolean alignment to flag only the HH:30 points where the neighbor_clip_mask is True.\n",
    "df_30min.loc[is_hh30, \"is_clipped\"] |= neighbor_clip_mask\n",
    "print(\n",
    "    f\"   - Propagated clipping flag to {neighbor_clip_mask.sum()} neighboring HH:30 points.\"\n",
    ")\n",
    "\n",
    "# Step 3: Propagate flag to interpolated HH:00 points (Interpolation Contamination)\n",
    "is_hh00 = df_30min.index.minute == 0\n",
    "interpolated_clip_mask = df_30min[\"is_clipped\"].shift(\n",
    "    1, fill_value=False\n",
    ") | df_30min[\"is_clipped\"].shift(-1, fill_value=False)\n",
    "df_30min.loc[is_hh00, \"is_clipped\"] |= interpolated_clip_mask[is_hh00]\n",
    "print(f\"   - Propagated clipping flag to interpolated HH:00 points.\")\n",
    "\n",
    "# --- Curtailment Detection Logic ---\n",
    "print(\"\\n2. Applying Curtailment Detection Logic...\")\n",
    "\n",
    "# Step 1: Directly flag HH:30 points with negative prices and low production during the day\n",
    "# The 'is_day' check prevents flagging nighttime hours where production is naturally zero.\n",
    "direct_curtail_mask = (\n",
    "    is_hh30\n",
    "    & df_30min[\"is_day\"]\n",
    "    & (df_30min[\"spot_price_eur_mwh\"] < 0)\n",
    "    & (df_30min[\"ac_export_w\"] < curtailment_threshold_w)\n",
    ")\n",
    "df_30min.loc[direct_curtail_mask, \"is_curtailed\"] = True\n",
    "print(\n",
    "    f\"   - Found {direct_curtail_mask.sum()} HH:30 points with direct curtailment.\"\n",
    ")\n",
    "\n",
    "# Step 2: Propagate flag to interpolated HH:00 points\n",
    "interpolated_curtail_mask = df_30min[\"is_curtailed\"].shift(\n",
    "    1, fill_value=False\n",
    ") | df_30min[\"is_curtailed\"].shift(-1, fill_value=False)\n",
    "df_30min.loc[is_hh00, \"is_curtailed\"] |= interpolated_curtail_mask[is_hh00]\n",
    "print(f\"   - Propagated curtailment flag to interpolated HH:00 points.\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "total_clipped = df_30min[\"is_clipped\"].sum()\n",
    "total_curtailed = df_30min[\"is_curtailed\"].sum()\n",
    "print(\"\\nâœ… Anomaly flagging complete.\")\n",
    "print(\n",
    "    f\"   - Total points flagged as clipped: {total_clipped} ({total_clipped / len(df_30min):.2%})\"\n",
    ")\n",
    "print(\n",
    "    f\"   - Total points flagged as curtailed: {total_curtailed} ({total_curtailed / len(df_30min):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e78923",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 4.2 Visually Verify Flags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visually Verify Anomaly Flags with Interactive Plots ---\n",
    "\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def create_flag_verification_plot(\n",
    "    df_day: pd.DataFrame,\n",
    "    flag_col: str,\n",
    "    title: str,\n",
    "    flag_color: str,\n",
    "    normal_color: str = \"royalblue\",\n",
    ") -> go.Figure:\n",
    "    \"\"\"Creates an interactive bar plot to visualize a specific anomaly flag for a single day.\"\"\"\n",
    "\n",
    "    df_plot = df_day.copy()\n",
    "    df_plot[\"color\"] = df_plot[flag_col].map({True: flag_color, False: normal_color})\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_plot.index,\n",
    "            y=df_plot[\"ac_export_w\"],\n",
    "            marker_color=df_plot[\"color\"],\n",
    "            customdata=df_plot[flag_col],\n",
    "            hovertemplate=\"<b>Time</b>: %{x|%H:%M}<br><b>Power</b>: %{y:.0f} W<br><b>Flagged</b>: %{customdata}<extra></extra>\",\n",
    "            # Set period to 30 minutes (in milliseconds) and align bars to the start of the period\n",
    "            xperiod=30 * 60 * 1000,\n",
    "            xperiodalignment=\"middle\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        xaxis_title=\"Time [UTC]\",\n",
    "        yaxis_title=\"AC Export [W]\",\n",
    "        bargap=0.05,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "assert isinstance(df_30min.index, pd.DatetimeIndex)\n",
    "\n",
    "# --- Plot 1: Clipping Verification ---\n",
    "clipped_days = df_30min[df_30min[\"is_clipped\"]].index.normalize().unique() # type: ignore\n",
    "\n",
    "if not clipped_days.empty:\n",
    "    random_clipped_day = random.choice(clipped_days)\n",
    "    df_plot_clip = df_30min[df_30min.index.date == random_clipped_day.date()]\n",
    "\n",
    "    fig_clip = create_flag_verification_plot(\n",
    "        df_day=df_plot_clip,\n",
    "        flag_col=\"is_clipped\",\n",
    "        title=f\"Clipping Verification for {random_clipped_day.strftime('%Y-%m-%d')}\",\n",
    "        flag_color=\"crimson\",\n",
    "    )\n",
    "    fig_clip.show()\n",
    "else:\n",
    "    print(\"â„¹ï¸ No days with clipping were found to plot.\")\n",
    "\n",
    "# --- Plot 2: Curtailment Verification ---\n",
    "curtailed_days = df_30min[df_30min[\"is_curtailed\"]].index.normalize().unique() # type: ignore\n",
    "\n",
    "if not curtailed_days.empty:\n",
    "    random_curtailed_day = random.choice(curtailed_days)\n",
    "    df_plot_curtail = df_30min[df_30min.index.date == random_curtailed_day.date()]\n",
    "\n",
    "    fig_curtail = create_flag_verification_plot(\n",
    "        df_day=df_plot_curtail,\n",
    "        flag_col=\"is_curtailed\",\n",
    "        title=f\"Curtailment Verification for {random_curtailed_day.strftime('%Y-%m-%d')}\",\n",
    "        flag_color=\"crimson\",\n",
    "    )\n",
    "    fig_curtail.show()\n",
    "else:\n",
    "    print(\"â„¹ï¸ No days with curtailment were found to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6817726",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 5. AC Export -> Plane of Array (POA) Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c471d",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff33105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "# This section defines the core function for modeling POA irradiance from AC power.\n",
    "# The process is iterative:\n",
    "# 1. Guess the cell temperature (`T_cell`).\n",
    "# 2. Calculate the expected DC power from the measured AC power.\n",
    "# 3. Use the PVWatts DC power model to estimate the POA irradiance required to produce that DC power, given the current `T_cell` guess.\n",
    "# 4. Recalculate `T_cell` using the estimated POA irradiance and the SAPM temperature model.\n",
    "# 5. Compare the new `T_cell` with the previous guess. If they are close enough, the process has converged. If not, repeat from step 3 with the new `T_cell`.\n",
    "#\n",
    "# This approach effectively reverses the standard power simulation chain (Irradiance -> DC Power -> AC Power) to derive the initial irradiance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pvlib\n",
    "\n",
    "\n",
    "def estimate_poa_and_temp_cell(\n",
    "    p_ac: float,\n",
    "    temp_air: float,\n",
    "    wind_speed: float,\n",
    "    pdc0: float,\n",
    "    gamma_pmp: float,\n",
    "    inverter_efficiency: float,\n",
    "    temp_model_params: dict[str, float],\n",
    "    commissioning_date: pd.Timestamp,\n",
    "    current_timestamp: pd.Timestamp,\n",
    "    degradation_rate: float,\n",
    ") -> tuple[float, float, float, float]:  # Return type for iterations changed to float\n",
    "    \"\"\"\n",
    "    Iteratively estimates POA irradiance and cell temperature from AC power,\n",
    "    accounting for system degradation over time.\n",
    "\n",
    "    Args:\n",
    "        p_ac: AC power output in Watts.\n",
    "        temp_air: Ambient air temperature in Celsius.\n",
    "        wind_speed: Wind speed in m/s.\n",
    "        pdc0: Nameplate DC power of the system at STC in Watts.\n",
    "        gamma_pmp: Power temperature coefficient (e.g., -0.004 for -0.4%/Â°C).\n",
    "        inverter_efficiency: Nominal inverter efficiency (e.g., 0.985).\n",
    "        temp_model_params: Parameters for the SAPM cell temperature model.\n",
    "        commissioning_date: The timestamp of the system's start of operation.\n",
    "        current_timestamp: The timestamp of the data point being modeled.\n",
    "        degradation_rate: The annual degradation rate (e.g., 0.005 for 0.5%).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - Estimated POA irradiance (W/m^2).\n",
    "        - Estimated cell temperature (Â°C).\n",
    "        - Final temperature difference upon convergence or max iterations (Â°C).\n",
    "        - Number of iterations performed (can be np.nan).\n",
    "    \"\"\"\n",
    "    if any(\n",
    "        pd.isna(val)\n",
    "        for val in [p_ac, temp_air, wind_speed, pdc0, gamma_pmp, inverter_efficiency]\n",
    "    ):\n",
    "        # Return np.nan for all values, including iterations, if inputs are invalid.\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    if p_ac <= 0:\n",
    "        return 0.0, temp_air, 0.0, 0\n",
    "\n",
    "    # Calculate effective DC capacity for the current timestamp considering degradation\n",
    "    time_delta = current_timestamp - commissioning_date\n",
    "    years_passed = time_delta.total_seconds() / (365.25 * 24 * 3600)\n",
    "\n",
    "    if years_passed < 0:\n",
    "        years_passed = 0.0\n",
    "\n",
    "    pdc_effective = pdc0 * (1 - degradation_rate) ** years_passed\n",
    "\n",
    "    # Constants and initial guess\n",
    "    TEMP_REF = 25.0\n",
    "    IRRAD_REF = 1000.0\n",
    "    MAX_ITER = 10\n",
    "    TOLERANCE = 0.1\n",
    "    temp_cell_guess = temp_air + 20.0\n",
    "    p_dc = p_ac / inverter_efficiency\n",
    "\n",
    "    # Address the unbound linter warning\n",
    "    irrad_estimate = np.nan\n",
    "    temp_cell_new = np.nan\n",
    "    temp_diff = np.nan\n",
    "\n",
    "    for i in range(1, MAX_ITER + 1):\n",
    "        temp_factor = 1 + gamma_pmp * (temp_cell_guess - TEMP_REF)\n",
    "\n",
    "        if temp_factor <= 0:\n",
    "            return 0.0, temp_air, np.nan, i\n",
    "\n",
    "        irrad_estimate = (p_dc / (pdc_effective * temp_factor)) * IRRAD_REF\n",
    "        irrad_estimate = max(0, irrad_estimate)\n",
    "\n",
    "        temp_cell_new = pvlib.temperature.sapm_cell(\n",
    "            poa_global=irrad_estimate,\n",
    "            temp_air=temp_air,\n",
    "            wind_speed=wind_speed,\n",
    "            **temp_model_params,\n",
    "        )\n",
    "        temp_diff = abs(temp_cell_new - temp_cell_guess)\n",
    "\n",
    "        if temp_diff < TOLERANCE:\n",
    "            return irrad_estimate, temp_cell_new, temp_diff, i\n",
    "\n",
    "        temp_cell_guess = temp_cell_new\n",
    "\n",
    "    return irrad_estimate, temp_cell_new, temp_diff, MAX_ITER\n",
    "\n",
    "\n",
    "print(\"âœ… Helper function estimate_poa_and_temp_cell defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b4c65",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 5.1 Main Processing and Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing and Data Integration ---\n",
    "\n",
    "# --- Retrieve Park-Specific Configuration ---\n",
    "try:\n",
    "    park_config = PARK_CONFIGS[TARGET_PARK_NAME]\n",
    "    system_config = park_config[\"system\"]\n",
    "    temp_model_config = park_config[\"temperature_model\"]\n",
    "\n",
    "    pdc0: float = system_config[\"pdc0\"]\n",
    "    gamma_pmp: float = system_config[\"gamma_pmp\"]\n",
    "    inverter_efficiency: float = system_config[\"inverter_efficiency\"]\n",
    "    degradation_rate: float = system_config[\"degradation_rate\"]\n",
    "    commissioning_date_str: str = system_config[\"commissioning_date\"]\n",
    "    commissioning_ts: pd.Timestamp = pd.to_datetime(commissioning_date_str, utc=True)\n",
    "\n",
    "    print(f\"âš™ï¸ Using parameters for '{TARGET_PARK_NAME}':\")\n",
    "    print(f\"  - Commissioning Date: {commissioning_ts.date()}\")\n",
    "    print(f\"  - Nameplate DC Power (pdc0): {pdc0 / 1e3:,.1f} kW\")\n",
    "    print(f\"  - Annual Degradation: {degradation_rate:.1%}\")\n",
    "    print(f\"  - Temp. Coefficient (gamma_pmp): {gamma_pmp * 100:.3f} %/Â°C\")\n",
    "    print(f\"  - Inverter Efficiency: {inverter_efficiency:.1%}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(\n",
    "        f\"âŒ CONFIGURATION ERROR: Missing key {e} for park '{TARGET_PARK_NAME}' in config.yml.\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# --- Define Temperature Model from Config ---\n",
    "try:\n",
    "    model_type: str = temp_model_config[\"model_type\"]\n",
    "    model_name: str = temp_model_config[\"model_name\"]\n",
    "    temp_model_parameters = pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS[model_type][\n",
    "        model_name\n",
    "    ]\n",
    "    print(f\"\\nðŸŒ¡ï¸ Using {model_type.upper()} temperature model: '{model_name}'\")\n",
    "except KeyError:\n",
    "    print(\n",
    "        f\"âŒ CONFIGURATION ERROR: Invalid temperature model '{model_type}/{model_name}' specified in config.yml.\" # type: ignore\n",
    "    )\n",
    "    print(\n",
    "        \"Please check available models in pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS.\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# --- Prepare DataFrame for Re-running ---\n",
    "# Drop columns from previous runs to avoid conflicts during the join operation.\n",
    "cols_to_drop = [\n",
    "    \"poa_estimated_w_m2\",\n",
    "    \"temp_cell_estimated_c\",\n",
    "    \"temp_convergence_diff\",\n",
    "    \"iterations\",\n",
    "]\n",
    "df_30min = df_30min.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# --- Run POA Estimation ---\n",
    "estimation_mask = (\n",
    "    (df_30min[\"ac_export_w\"] > 0) &\n",
    "    (~df_30min[\"is_clipped\"]) &\n",
    "    (~df_30min[\"is_curtailed\"]) &\n",
    "    (df_30min[\"is_day\"])\n",
    ")\n",
    "df_analysis = df_30min.loc[estimation_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ”¬ Running POA estimation for {len(df_analysis):,} data points (daytime, non-clipped, non-curtailed, positive power).\")\n",
    "\n",
    "results = df_analysis.apply(\n",
    "    lambda row: estimate_poa_and_temp_cell(\n",
    "        p_ac=row[\"ac_export_w\"],\n",
    "        temp_air=row[\"temp_air_c\"],\n",
    "        wind_speed=row[\"wind_speed_m_s\"],\n",
    "        pdc0=pdc0,\n",
    "        gamma_pmp=gamma_pmp,\n",
    "        inverter_efficiency=inverter_efficiency,\n",
    "        temp_model_params=temp_model_parameters,\n",
    "        commissioning_date=commissioning_ts,\n",
    "        current_timestamp=row.name,\n",
    "        degradation_rate=degradation_rate,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "if not results.empty:\n",
    "    results_df = pd.DataFrame(\n",
    "        results.tolist(),\n",
    "        index=df_analysis.index,\n",
    "        columns=cols_to_drop, # Use the list defined earlier\n",
    "    )\n",
    "    df_30min = df_30min.join(results_df)\n",
    "\n",
    "print(\"âœ… Estimation complete.\")\n",
    "\n",
    "# --- Final Data Cleanup ---\n",
    "print(\"\\nðŸ§¹ Cleaning up and filling non-estimated periods...\")\n",
    "\n",
    "# For nighttime, irradiance is 0 and cell temperature equals ambient.\n",
    "night_mask = ~df_30min[\"is_day\"]\n",
    "df_30min.loc[night_mask, \"poa_estimated_w_m2\"] = 0.0\n",
    "df_30min.loc[night_mask, \"temp_cell_estimated_c\"] = df_30min.loc[night_mask, \"temp_air_c\"]\n",
    "\n",
    "# For all remaining NaNs in 'iterations' (night, clipped, curtailed, etc.), fill with 0.\n",
    "df_30min[\"iterations\"] = df_30min[\"iterations\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"âœ… Cleanup complete. Clipped/curtailed daytime periods remain as NaN for irradiance and temperature.\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nðŸ“Š Sample of dataframe with new estimated POA and cell temperature columns:\")\n",
    "display(df_30min.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f091f1a",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 6. Modeling GHI from Estimated POA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0340f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pvlib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"--- Starting GHI Modeling from Estimated POA ---\")\n",
    "\n",
    "# --- 1. Preparation ---\n",
    "GHI_MODEL_COLS = [\n",
    "    \"ghi_modeled_w_m2\",\n",
    "    \"dni_modeled_w_m2\",\n",
    "    \"dhi_modeled_w_m2\",\n",
    "    \"kt_modeled\",\n",
    "    \"ghi_model_converged\",\n",
    "    \"ghi_model_iterations\",\n",
    "]\n",
    "IRRADIANCE_COLS = GHI_MODEL_COLS[:4]\n",
    "\n",
    "df_30min.drop(\n",
    "    columns=[col for col in GHI_MODEL_COLS if col in df_30min.columns],\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "\n",
    "df_30min[\"ghi_model_converged\"] = pd.Series(\n",
    "    pd.NA, index=df_30min.index, dtype=\"boolean\"\n",
    ")\n",
    "df_30min[\"ghi_model_iterations\"] = np.nan\n",
    "for col in IRRADIANCE_COLS:\n",
    "    df_30min[col] = np.nan\n",
    "\n",
    "location_params = PARK_CONFIGS[TARGET_PARK_NAME][\"location\"]\n",
    "\n",
    "# --- 2. GHI Reverse Transposition with Progress Bar ---\n",
    "print(\"Step 1: Calculating GHI for daytime points with valid POA...\")\n",
    "\n",
    "# A more robust mask: only process daytime points with a positive POA estimate.\n",
    "# The > 0 condition implicitly handles both NaNs and zeros.\n",
    "calc_mask = df_30min[\"is_day\"] & (df_30min[\"poa_estimated_w_m2\"] > 0)\n",
    "df_to_process = df_30min.loc[calc_mask]\n",
    "assert isinstance(df_to_process.index, pd.DatetimeIndex)\n",
    "\n",
    "print(f\"   - Found {len(df_to_process)} points to process.\")\n",
    "\n",
    "if not df_to_process.empty:\n",
    "    monthly_groups = df_to_process.groupby(\n",
    "        [df_to_process.index.year, df_to_process.index.month]\n",
    "    )\n",
    "    results_list = []\n",
    "\n",
    "    pbar = tqdm(monthly_groups, desc=\"Modeling GHI (monthly chunks)\")\n",
    "    for (year, month), group in pbar:\n",
    "        pbar.set_postfix_str(f\"{year}-{month:02d}\")\n",
    "\n",
    "        ghi_results_chunk = pvlib.irradiance.ghi_from_poa_driesse_2023(\n",
    "            surface_tilt=location_params[\"surface_tilt\"],\n",
    "            surface_azimuth=location_params[\"surface_azimuth\"],\n",
    "            solar_zenith=group[\"zenith\"],\n",
    "            solar_azimuth=group[\"azimuth\"],\n",
    "            poa_global=group[\"poa_estimated_w_m2\"],\n",
    "            dni_extra=group[\"dni_extra_w_m2\"],\n",
    "            albedo=location_params[\"albedo\"],\n",
    "            full_output=True,\n",
    "        )\n",
    "\n",
    "        results_list.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"ghi_modeled_w_m2\": ghi_results_chunk[0],\n",
    "                    \"ghi_model_converged\": ghi_results_chunk[1],\n",
    "                    \"ghi_model_iterations\": ghi_results_chunk[2],\n",
    "                },\n",
    "                index=group.index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if results_list:\n",
    "        all_results_df = pd.concat(results_list)\n",
    "        df_30min.update(all_results_df)\n",
    "\n",
    "# --- 3. GHI Decomposition (ERBS Model) ---\n",
    "# Decompose only where GHI was successfully modeled and converged.\n",
    "decomp_mask = df_30min[\"ghi_modeled_w_m2\"].notna() & (\n",
    "    df_30min[\"ghi_model_converged\"] == True\n",
    ")\n",
    "print(f\"Step 2: Decomposing GHI for {decomp_mask.sum()} valid points...\")\n",
    "\n",
    "if decomp_mask.any():\n",
    "    decomposed = pvlib.irradiance.erbs_driesse(\n",
    "        ghi=df_30min.loc[decomp_mask, \"ghi_modeled_w_m2\"],\n",
    "        zenith=df_30min.loc[decomp_mask, \"zenith\"],\n",
    "        datetime_or_doy=df_30min.loc[decomp_mask].index,\n",
    "    ).rename( # type: ignore\n",
    "        columns={\n",
    "            \"dni\": \"dni_modeled_w_m2\",\n",
    "            \"dhi\": \"dhi_modeled_w_m2\",\n",
    "            \"kt\": \"kt_modeled\",\n",
    "        }\n",
    "    )\n",
    "    df_30min.update(decomposed)\n",
    "\n",
    "# --- 4. Data Cleaning and Finalization ---\n",
    "print(\"Step 3: Cleaning and finalizing modeled irradiance data...\")\n",
    "\n",
    "failed_convergence_mask = df_30min[\"ghi_model_converged\"] == False\n",
    "df_30min.loc[failed_convergence_mask, IRRADIANCE_COLS] = np.nan\n",
    "print(\n",
    "    f\"   - Invalidated {failed_convergence_mask.sum()} points due to GHI model non-convergence.\"\n",
    ")\n",
    "\n",
    "unrealistic_dni_mask = df_30min[\"dni_modeled_w_m2\"] > (\n",
    "    df_30min[\"dni_clearsky_w_m2\"] * 1.05\n",
    ")\n",
    "df_30min.loc[unrealistic_dni_mask, IRRADIANCE_COLS] = np.nan\n",
    "print(\n",
    "    f\"   - Invalidated {unrealistic_dni_mask.sum()} points exceeding the clear-sky DNI limit.\"\n",
    ")\n",
    "\n",
    "night_mask = ~df_30min[\"is_day\"]\n",
    "df_30min.loc[night_mask, IRRADIANCE_COLS] = df_30min.loc[\n",
    "    night_mask, IRRADIANCE_COLS\n",
    "].fillna(0)\n",
    "df_30min[\"ghi_model_converged\"] = df_30min[\"ghi_model_converged\"].fillna(False)\n",
    "df_30min.loc[night_mask, \"ghi_model_iterations\"] = df_30min.loc[\n",
    "    night_mask, \"ghi_model_iterations\"\n",
    "].fillna(0)\n",
    "\n",
    "for col in IRRADIANCE_COLS:\n",
    "    if col in df_30min.columns:\n",
    "        df_30min[col] = df_30min[col].clip(lower=0)\n",
    "\n",
    "print(\"\\nâœ… GHI Modeling and Decomposition complete.\")\n",
    "\n",
    "display_cols = [\n",
    "    \"poa_estimated_w_m2\",\n",
    "    \"ghi_modeled_w_m2\",\n",
    "    \"dni_modeled_w_m2\",\n",
    "    \"dhi_modeled_w_m2\",\n",
    "    \"kt_modeled\",\n",
    "    \"ghi_clearsky_w_m2\",\n",
    "    \"dni_clearsky_w_m2\",\n",
    "    \"ghi_model_converged\",\n",
    "    \"ghi_model_iterations\",\n",
    "]\n",
    "display(df_30min[display_cols].sample(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
